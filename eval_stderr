[33m01/23 18:28:22 GPU ID: 1 | available memory: 12207M[0m
[33m01/23 18:28:48 Starting from the best saved model[0m
[33m01/23 18:28:48 Starting from save/word_coco_tword015_a07_showtell152/model-best.pth[0m
[36m01/23 18:28:51 Setting CNN weigths from save/word_coco_tword015_a07_showtell152/model-cnn-best.pth[0m
[33m01/23 18:29:09 Finetuning up from 0 modules in the cnn[0m
[33m01/23 18:29:10 Show & Tell : OrderedDict([('img_embed', Linear (2048 -> 512)), ('core', LSTM(512, 512, bias=False, dropout=0.5)), ('embed', Embedding(9488, 512)), ('drop_x_lm', Dropout (p = 0.5)), ('logit', Linear (512 -> 9488))])[0m
[33m01/23 18:29:10 Loading the model dict (last checkpoint) ['img_embed.weight', 'img_embed.bias', 'core.weight_ih_l0', 'core.weight_hh_l0', 'embed.weight', 'logit.weight', 'logit.bias'][0m
[33m01/23 18:29:14 DataLoader loading json file: data/coco/cocotalk.json[0m
[33m01/23 18:29:15 vocab size is 9487 [0m
[33m01/23 18:29:15 DataLoader loading h5 file: data/coco/cocotalk.h5[0m
[33m01/23 18:29:15 read 123287 images of size 3x256x256[0m
[33m01/23 18:29:15 max sequence length in data is 16[0m
[33m01/23 18:29:15 assigned 82783 images to split train[0m
[33m01/23 18:29:15 assigned 5000 images to split val[0m
[33m01/23 18:29:15 assigned 5000 images to split test[0m
## OAR [2018-01-23 18:29:27] Job 137964 KILLED ##
[33m01/23 18:29:43 GPU ID: 1 | available memory: 12189M[0m
[33m01/23 18:29:48 Starting from the best saved model[0m
[33m01/23 18:29:48 Starting from save/word_coco_tword015_a07_showtell152/model-best.pth[0m
[36m01/23 18:29:52 Setting CNN weigths from save/word_coco_tword015_a07_showtell152/model-cnn-best.pth[0m
odel-cnn-best.pth[0m
[33m01/23 18:30:00 GPU ID: 5 | available memory: 16276M[0m
[33m01/23 18:30:13 Finetuning up from 0 modules in the cnn[0m
[33m01/23 18:30:14 Show & Tell : OrderedDict([('img_embed', Linear (2048 -> 512)), ('core', LSTM(512, 512, bias=False, dropout=0.5)), ('embed', Embedding(9488, 512)), ('drop_x_lm', Dropout (p = 0.5)), ('logit', Linear (512 -> 9488))])[0m
[33m01/23 18:30:17 Loading the model dict (last checkpoint) ['img_embed.weight', 'img_embed.bias', 'core.weight_ih_l0', 'core.weight_hh_l0', 'embed.weight', 'logit.weight', 'logit.bias'][0m
[33m01/23 18:30:18 DataLoader loading json file: data/coco/cocotalk.json[0m
[33m01/23 18:30:18 vocab size is 9487 [0m
[33m01/23 18:30:18 DataLoader loading h5 file: data/coco/cocotalk.h5[0m
[33m01/23 18:30:18 read 123287 images of size 3x256x256[0m
[33m01/23 18:30:18 max sequence length in data is 16[0m
[33m01/23 18:30:18 assigned 82783 images to split train[0m
[33m01/23 18:30:18 assigned 5000 images to split val[0m
[33m01/23 18:30:18 assigned 5000 images to split test[0m
[32m01/23 18:30:18 RewardSampler (stratified sampling), r=Hamming (Vpool=1, tau=0.17)[0m
[33m01/23 18:30:18 Evaluating the test split (-1)[0m
assigned 82783 images to split train[0m
[33m01/23 18:30:20 assigned 5000 images to split val[0m
[33m01/23 18:30:20 assigned 5000 images to split test[0m
[32m01/23 18:30:20 RewardSampler (stratified sampling), r=Hamming (Vpool=2, tau=0.30)[0m
[33m01/23 18:30:20 Evaluating the test split (-1)[0m
[32m01/23 18:30:51 Initialized Word2 loss tau=0.150, alpha=0.7[0m
[33m01/23 18:30:51 Evaluating the test split (-1)[0m
[33m01/23 18:35:58 Loading reference captions..[0m
[33m01/23 18:36:00 Reference captions loaded![0m
[33m01/23 18:36:02 using 5000/5000 predictions[0m
[33m01/23 18:36:02 Loading model captions[0m
[33m01/23 18:36:02 Model captions loaded[0m
[33m01/23 18:36:02 Starting evaluation...[0m
PTBTokenizer tokenized 307085 tokens at 1150313.53 tokens per second.
PTBTokenizer tokenized 51263 tokens at 360003.18 tokens per second.
[33m01/23 18:36:20 Loading reference captions..[0m
[33m01/23 18:36:22 Reference captions loaded![0m
[33m01/23 18:36:24 using 5000/5000 predictions[0m
[33m01/23 18:36:24 Loading model captions[0m
[33m01/23 18:36:24 Model captions loaded[0m
[33m01/23 18:36:24 Starting evaluation...[0m
PTBTokenizer tokenized 307085 tokens at 923650.88 tokens per second.

PTBTokenizer tokenized 49687 tokens at 425840.69 tokens per second.
PTBTokenizer tokenized 51758 tokens at 432839.72 tokens per second.
[33m01/23 18:37:26 GPU ID: 1 | available memory: 12207M[0m
[33m01/23 18:37:30 Starting from the best saved model[0m
[33m01/23 18:37:30 Starting from save/importance_lazy_qhamming_limited1_tsent017_rcider_tsent05_a04_showtell152/model-best.pth[0m
[36m01/23 18:37:34 Setting CNN weigths from save/importance_lazy_qhamming_limited1_tsent017_rcider_tsent05_a04_showtell152/model-cnn-best.pth[0m
[33m01/23 18:37:45 Finetuning up from 0 modules in the cnn[0m
[33m01/23 18:37:46 Show & Tell : OrderedDict([('img_embed', Linear (2048 -> 512)), ('core', LSTM(512, 512, bias=False, dropout=0.5)), ('embed', Embedding(9488, 512)), ('drop_x_lm', Dropout (p = 0.5)), ('logit', Linear (512 -> 9488))])[0m
[33m01/23 18:37:47 Loading the model dict (last checkpoint) ['img_embed.weight', 'img_embed.bias', 'core.weight_ih_l0', 'core.weight_hh_l0', 'embed.weight', 'logit.weight', 'logit.bias'][0m
[33m01/23 18:37:47 DataLoader loading json file: data/coco/cocotalk.json[0m
[33m01/23 18:37:47 vocab size is 9487 [0m
[33m01/23 18:37:47 DataLoader loading h5 file: data/coco/cocotalk.h5[0m
[33m01/23 18:37:47 read 123287 images of size 3x256x256[0m
[33m01/23 18:37:47 max sequence length in data is 16[0m
[33m01/23 18:37:47 assigned 82783 images to split train[0m
[33m01/23 18:37:47 assigned 5000 images to split val[0m
[33m01/23 18:37:47 assigned 5000 images to split test[0m
[32m01/23 18:37:53 using importance sampling r=CIDEr (tau=0.50) and q=Hamming (Vpool=1, tau=0.17)[0m
[33m01/23 18:37:53 Evaluating the test split (-1)[0m
[33m01/23 18:37:59 GPU ID: 0 | available memory: 12205M[0m
[33m01/23 18:38:05 Starting from the best saved model[0m
[33m01/23 18:38:05 Starting from save/importance_qhamming_limited1_tsent017_rcider_tsent05_a04_showtell152/model-best.pth[0m
[36m01/23 18:38:09 Setting CNN weigths from save/importance_qhamming_limited1_tsent017_rcider_tsent05_a04_showtell152/model-cnn-best.pth[0m
[33m01/23 18:38:16 Finetuning up from 0 modules in the cnn[0m
[33m01/23 18:38:17 Show & Tell : OrderedDict([('img_embed', Linear (2048 -> 512)), ('core', LSTM(512, 512, bias=False, dropout=0.5)), ('embed', Embedding(9488, 512)), ('drop_x_lm', Dropout (p = 0.5)), ('logit', Linear (512 -> 9488))])[0m
[33m01/23 18:38:18 Loading the model dict (last checkpoint) ['img_embed.weight', 'img_embed.bias', 'core.weight_ih_l0', 'core.weight_hh_l0', 'embed.weight', 'logit.weight', 'logit.bias'][0m
[33m01/23 18:38:19 DataLoader loading json file: data/coco/cocotalk.json[0m
[33m01/23 18:38:19 vocab size is 9487 [0m
[33m01/23 18:38:19 DataLoader loading h5 file: data/coco/cocotalk.h5[0m
[33m01/23 18:38:19 read 123287 images of size 3x256x256[0m
[33m01/23 18:38:19 max sequence length in data is 16[0m
[33m01/23 18:38:19 assigned 82783 images to split train[0m
[33m01/23 18:38:19 assigned 5000 images to split val[0m
[33m01/23 18:38:19 assigned 5000 images to split test[0m
[32m01/23 18:38:22 using importance sampling r=CIDEr (tau=0.50) and q=Hamming (Vpool=1, tau=0.17)[0m
[33m01/23 18:38:22 Evaluating the test split (-1)[0m
[33m01/23 18:42:32 Loading reference captions..[0m
[33m01/23 18:42:33 Reference captions loaded![0m
[33m01/23 18:42:34 using 5000/5000 predictions[0m
[33m01/23 18:42:35 Loading model captions[0m
[33m01/23 18:42:35 Model captions loaded[0m
[33m01/23 18:42:35 Starting evaluation...[0m
PTBTokenizer tokenized 307085 tokens at 1392237.86 tokens per second.
PTBTokenizer tokenized 51403 tokens at 443307.31 tokens per second.
[33m01/23 18:43:57 Loading reference captions..[0m
[33m01/23 18:43:58 Reference captions loaded![0m
[33m01/23 18:44:00 using 5000/5000 predictions[0m
[33m01/23 18:44:00 Loading model captions[0m
[33m01/23 18:44:00 Model captions loaded[0m
[33m01/23 18:44:00 Starting evaluation...[0m
PTBTokenizer tokenized 307085 tokens at 806774.17 tokens per second.
PTBTokenizer tokenized 51784 tokens at 153366.24 tokens per second.
[33m01/23 19:54:40 GPU ID: 1 | available memory: 12189M[0m
[33m01/23 19:54:52 Starting from the best saved model[0m
[33m01/23 19:54:52 Starting from save/topdown_resnet152_msc/model-best.pth[0m
Traceback (most recent call last):
  File "eval.py", line 76, in <module>
    str(vars(opt)[k]) + ' vs. ' + str(vars(infos['opt'])[k]))
AssertionError: region_size option not consistent 14 vs. 7
[33m01/23 19:55:54 GPU ID: 1 | available memory: 12189M[0m
[33m01/23 19:55:58 Starting from the best saved model[0m
[33m01/23 19:55:58 Starting from save/fncnn6_topdown_resnet152_msc/model-best.pth[0m
Traceback (most recent call last):
  File "eval.py", line 76, in <module>
    str(vars(opt)[k]) + ' vs. ' + str(vars(infos['opt'])[k]))
AssertionError: region_size option not consistent 14 vs. 7
[33m01/23 19:57:24 GPU ID: 1 | available memory: 12189M[0m
[33m01/23 19:57:28 Starting from the best saved model[0m
[33m01/23 19:57:28 Starting from save/fncnn6_topdown_resnet152_msc/model-best.pth[0m
Traceback (most recent call last):
  File "eval.py", line 76, in <module>
    str(vars(opt)[k]) + ' vs. ' + str(vars(infos['opt'])[k]))
AssertionError: use_adaptive_pooling option not consistent 1 vs. 0
[33m01/23 19:57:31 GPU ID: 0 | available memory: 12205M[0m
[33m01/23 19:57:36 Starting from the best saved model[0m
[33m01/23 19:57:36 Starting from save/topdown_resnet152_msc/model-best.pth[0m
Traceback (most recent call last):
  File "eval.py", line 76, in <module>
    str(vars(opt)[k]) + ' vs. ' + str(vars(infos['opt'])[k]))
AssertionError: use_adaptive_pooling option not consistent 1 vs. 0
[33m01/23 19:58:04 GPU ID: 1 | available memory: 12189M[0m
[33m01/23 19:58:08 Starting from the best saved model[0m
[33m01/23 19:58:08 Starting from save/topdown_resnet152_msc/model-best.pth[0m
[36m01/23 19:58:12 Setting CNN weigths from save/topdown_resnet152_msc/model-cnn-best.pth[0m
[33m01/23 19:58:16 Finetuning up from 0 modules in the cnn[0m
[33m01/23 19:58:17 Top Down : OrderedDict([('embed_1', Embedding(9488, 512)), ('embed', Sequential (
  (0): Embedding(9488, 512)
  (1): ReLU ()
  (2): Dropout (p = 0.5)
)), ('att_embed', Sequential (
  (0): Linear (2048 -> 512)
  (1): ReLU ()
  (2): Dropout (p = 0.0)
)), ('logit', Linear (512 -> 9488)), ('ctx2att', Linear (512 -> 512)), ('fc_embed', Sequential (
  (0): Linear (2048 -> 512)
  (1): ReLU ()
  (2): Dropout (p = 0.0)
)), ('core', TopDownCore (
  (att_lstm): LSTMCell(1536, 512)
  (lang_lstm): LSTMCell(1024, 512)
  (attention): Attention (
    (h2att): Linear (512 -> 512)
    (alpha_net): Linear (512 -> 1)
  )
))])[0m
[33m01/23 19:58:17 Loading the model dict (last checkpoint) ['embed_1.weight', 'embed.0.weight', 'att_embed.0.weight', 'att_embed.0.bias', 'logit.weight', 'logit.bias', 'ctx2att.weight', 'ctx2att.bias', 'fc_embed.0.weight', 'fc_embed.0.bias', 'core.att_lstm.weight_ih', 'core.att_lstm.weight_hh', 'core.att_lstm.bias_ih', 'core.att_lstm.bias_hh', 'core.lang_lstm.weight_ih', 'core.lang_lstm.weight_hh', 'core.lang_lstm.bias_ih', 'core.lang_lstm.bias_hh', 'core.attention.h2att.weight', 'core.attention.h2att.bias', 'core.attention.alpha_net.weight', 'core.attention.alpha_net.bias'][0m
[33m01/23 19:58:17 DataLoader loading json file: data/coco/cocotalk.json[0m
[33m01/23 19:58:17 vocab size is 9487 [0m
[33m01/23 19:58:17 DataLoader loading h5 file: data/coco/cocotalk.h5[0m
[33m01/23 19:58:17 read 123287 images of size 3x256x256[0m
[33m01/23 19:58:17 max sequence length in data is 16[0m
[33m01/23 19:58:17 assigned 82783 images to split train[0m
[33m01/23 19:58:17 assigned 5000 images to split val[0m
[33m01/23 19:58:17 assigned 5000 images to split test[0m
[32m01/23 19:58:17 Default ML loss[0m
[33m01/23 19:58:17 Evaluating the test split (-1)[0m
[33m01/23 19:58:45 GPU ID: 0 | available memory: 12205M[0m
[33m01/23 19:58:50 Starting from the best saved model[0m
[33m01/23 19:58:50 Starting from save/fncnn6_topdown_resnet152_msc/model-best.pth[0m
[36m01/23 19:58:55 Setting CNN weigths from save/fncnn6_topdown_resnet152_msc/model-cnn-best.pth[0m
[33m01/23 19:59:02 Finetuning up from 6 modules in the cnn[0m
[33m01/23 19:59:02 Top Down : OrderedDict([('embed_1', Embedding(9488, 512)), ('embed', Sequential (
  (0): Embedding(9488, 512)
  (1): ReLU ()
  (2): Dropout (p = 0.5)
)), ('att_embed', Sequential (
  (0): Linear (2048 -> 512)
  (1): ReLU ()
  (2): Dropout (p = 0.0)
)), ('logit', Linear (512 -> 9488)), ('ctx2att', Linear (512 -> 512)), ('fc_embed', Sequential (
  (0): Linear (2048 -> 512)
  (1): ReLU ()
  (2): Dropout (p = 0.0)
)), ('core', TopDownCore (
  (att_lstm): LSTMCell(1536, 512)
  (lang_lstm): LSTMCell(1024, 512)
  (attention): Attention (
    (h2att): Linear (512 -> 512)
    (alpha_net): Linear (512 -> 1)
  )
))])[0m
[33m01/23 19:59:04 Loading the model dict (last checkpoint) ['embed_1.weight', 'embed.0.weight', 'att_embed.0.weight', 'att_embed.0.bias', 'logit.weight', 'logit.bias', 'ctx2att.weight', 'ctx2att.bias', 'fc_embed.0.weight', 'fc_embed.0.bias', 'core.att_lstm.weight_ih', 'core.att_lstm.weight_hh', 'core.att_lstm.bias_ih', 'core.att_lstm.bias_hh', 'core.lang_lstm.weight_ih', 'core.lang_lstm.weight_hh', 'core.lang_lstm.bias_ih', 'core.lang_lstm.bias_hh', 'core.attention.h2att.weight', 'core.attention.h2att.bias', 'core.attention.alpha_net.weight', 'core.attention.alpha_net.bias'][0m
[33m01/23 19:59:04 DataLoader loading json file: data/coco/cocotalk.json[0m
[33m01/23 19:59:04 vocab size is 9487 [0m
[33m01/23 19:59:04 DataLoader loading h5 file: data/coco/cocotalk.h5[0m
[33m01/23 19:59:04 read 123287 images of size 3x256x256[0m
[33m01/23 19:59:04 max sequence length in data is 16[0m
[33m01/23 19:59:04 assigned 82783 images to split train[0m
[33m01/23 19:59:04 assigned 5000 images to split val[0m
[33m01/23 19:59:04 assigned 5000 images to split test[0m
[32m01/23 19:59:04 Default ML loss[0m
[33m01/23 19:59:04 Evaluating the test split (-1)[0m
[33m01/23 20:02:25 Loading reference captions..[0m
[33m01/23 20:02:26 Reference captions loaded![0m
[33m01/23 20:02:27 using 5000/5000 predictions[0m
[33m01/23 20:02:28 Loading model captions[0m
[33m01/23 20:02:28 Model captions loaded[0m
[33m01/23 20:02:28 Starting evaluation...[0m
PTBTokenizer tokenized 307085 tokens at 1256736,39 tokens per second.
PTBTokenizer tokenized 51828 tokens at 413418,32 tokens per second.
[33m01/23 20:04:14 Loading reference captions..[0m
[33m01/23 20:04:15 Reference captions loaded![0m
[33m01/23 20:04:17 using 5000/5000 predictions[0m
[33m01/23 20:04:17 Loading model captions[0m
[33m01/23 20:04:17 Model captions loaded[0m
[33m01/23 20:04:17 Starting evaluation...[0m
PTBTokenizer tokenized 307085 tokens at 525931.13 tokens per second.
PTBTokenizer tokenized 51957 tokens at 320091.68 tokens per second.
[33m01/23 20:11:38 GPU ID: 3 | available memory: 16276M[0m
[33m01/23 20:11:46 Starting from the best saved model[0m
[33m01/23 20:11:46 Starting from save/strat_lazy_rhamming_pool1_tsent017_a04_topdown/model-best.pth[0m
[36m01/23 20:11:50 Setting CNN weigths from save/strat_lazy_rhamming_pool1_tsent017_a04_topdown/model-cnn-best.pth[0m
[33m01/23 20:11:53 Finetuning up from 0 modules in the cnn[0m
[33m01/23 20:11:54 Top Down : OrderedDict([('embed_1', Embedding(9488, 512)), ('embed', Sequential (
  (0): Embedding(9488, 512)
  (1): ReLU ()
  (2): Dropout (p = 0.5)
)), ('att_embed', Sequential (
  (0): Linear (2048 -> 512)
  (1): ReLU ()
  (2): Dropout (p = 0.0)
)), ('logit', Linear (512 -> 9488)), ('ctx2att', Linear (512 -> 512)), ('fc_embed', Sequential (
  (0): Linear (2048 -> 512)
  (1): ReLU ()
  (2): Dropout (p = 0.0)
)), ('core', TopDownCore (
  (att_lstm): LSTMCell(1536, 512)
  (lang_lstm): LSTMCell(1024, 512)
  (attention): Attention (
    (h2att): Linear (512 -> 512)
    (alpha_net): Linear (512 -> 1)
  )
))])[0m
[33m01/23 20:11:54 Loading the model dict (last checkpoint) ['embed_1.weight', 'embed.0.weight', 'att_embed.0.weight', 'att_embed.0.bias', 'logit.weight', 'logit.bias', 'ctx2att.weight', 'ctx2att.bias', 'fc_embed.0.weight', 'fc_embed.0.bias', 'core.att_lstm.weight_ih', 'core.att_lstm.weight_hh', 'core.att_lstm.bias_ih', 'core.att_lstm.bias_hh', 'core.lang_lstm.weight_ih', 'core.lang_lstm.weight_hh', 'core.lang_lstm.bias_ih', 'core.lang_lstm.bias_hh', 'core.attention.h2att.weight', 'core.attention.h2att.bias', 'core.attention.alpha_net.weight', 'core.attention.alpha_net.bias'][0m
[33m01/23 20:11:54 DataLoader loading json file: data/coco/cocotalk.json[0m
[33m01/23 20:11:54 vocab size is 9487 [0m
[33m01/23 20:11:54 DataLoader loading h5 file: data/coco/cocotalk.h5[0m
[33m01/23 20:11:54 read 123287 images of size 3x256x256[0m
[33m01/23 20:11:54 max sequence length in data is 16[0m
[33m01/23 20:11:54 assigned 82783 images to split train[0m
[33m01/23 20:11:54 assigned 5000 images to split val[0m
[33m01/23 20:11:54 assigned 5000 images to split test[0m
[32m01/23 20:11:54 RewardSampler (stratified sampling), r=Hamming (Vpool=1, tau=0.17)[0m
[33m01/23 20:11:54 Evaluating the test split (-1)[0m
[33m01/23 20:17:15 Loading reference captions..[0m
[33m01/23 20:17:15 Reference captions loaded![0m
[33m01/23 20:17:18 using 5000/5000 predictions[0m
[33m01/23 20:17:18 Loading model captions[0m
[33m01/23 20:17:18 Model captions loaded[0m
[33m01/23 20:17:18 Starting evaluation...[0m
PTBTokenizer tokenized 307085 tokens at 1184015.35 tokens per second.
PTBTokenizer tokenized 51523 tokens at 355437.63 tokens per second.
[33m01/23 20:17:43 GPU ID: 3 | available memory: 16276M[0m
[33m01/23 20:17:51 Starting from the best saved model[0m
[33m01/23 20:17:51 Starting from save/strat_rhamming_pool1_tsent017_a04_topdown/model-best.pth[0m
[36m01/23 20:17:56 Setting CNN weigths from save/strat_rhamming_pool1_tsent017_a04_topdown/model-cnn-best.pth[0m
[33m01/23 20:17:59 Finetuning up from 0 modules in the cnn[0m
[33m01/23 20:18:00 Top Down : OrderedDict([('embed_1', Embedding(9488, 512)), ('embed', Sequential (
  (0): Embedding(9488, 512)
  (1): ReLU ()
  (2): Dropout (p = 0.5)
)), ('att_embed', Sequential (
  (0): Linear (2048 -> 512)
  (1): ReLU ()
  (2): Dropout (p = 0.0)
)), ('logit', Linear (512 -> 9488)), ('ctx2att', Linear (512 -> 512)), ('fc_embed', Sequential (
  (0): Linear (2048 -> 512)
  (1): ReLU ()
  (2): Dropout (p = 0.0)
)), ('core', TopDownCore (
  (att_lstm): LSTMCell(1536, 512)
  (lang_lstm): LSTMCell(1024, 512)
  (attention): Attention (
    (h2att): Linear (512 -> 512)
    (alpha_net): Linear (512 -> 1)
  )
))])[0m
[33m01/23 20:18:00 Loading the model dict (last checkpoint) ['embed_1.weight', 'embed.0.weight', 'att_embed.0.weight', 'att_embed.0.bias', 'logit.weight', 'logit.bias', 'ctx2att.weight', 'ctx2att.bias', 'fc_embed.0.weight', 'fc_embed.0.bias', 'core.att_lstm.weight_ih', 'core.att_lstm.weight_hh', 'core.att_lstm.bias_ih', 'core.att_lstm.bias_hh', 'core.lang_lstm.weight_ih', 'core.lang_lstm.weight_hh', 'core.lang_lstm.bias_ih', 'core.lang_lstm.bias_hh', 'core.attention.h2att.weight', 'core.attention.h2att.bias', 'core.attention.alpha_net.weight', 'core.attention.alpha_net.bias'][0m
[33m01/23 20:18:00 DataLoader loading json file: data/coco/cocotalk.json[0m
[33m01/23 20:18:00 vocab size is 9487 [0m
[33m01/23 20:18:00 DataLoader loading h5 file: data/coco/cocotalk.h5[0m
[33m01/23 20:18:00 read 123287 images of size 3x256x256[0m
[33m01/23 20:18:00 max sequence length in data is 16[0m
[33m01/23 20:18:00 assigned 82783 images to split train[0m
[33m01/23 20:18:00 assigned 5000 images to split val[0m
[33m01/23 20:18:00 assigned 5000 images to split test[0m
[32m01/23 20:18:00 RewardSampler (stratified sampling), r=Hamming (Vpool=1, tau=0.17)[0m
[33m01/23 20:18:00 Evaluating the test split (-1)[0m
[33m01/23 20:23:37 Loading reference captions..[0m
[33m01/23 20:23:38 Reference captions loaded![0m
[33m01/23 20:23:40 using 5000/5000 predictions[0m
[33m01/23 20:23:40 Loading model captions[0m
[33m01/23 20:23:40 Model captions loaded[0m
[33m01/23 20:23:40 Starting evaluation...[0m
PTBTokenizer tokenized 307085 tokens at 1333179.83 tokens per second.
PTBTokenizer tokenized 51726 tokens at 372999.92 tokens per second.
[33m01/23 20:24:08 GPU ID: 3 | available memory: 16276M[0m
[33m01/23 20:24:15 Starting from the best saved model[0m
[33m01/23 20:24:15 Starting from save/word2_coco_tword013_a03_topdown_resnet152_msc/model-best.pth[0m
[36m01/23 20:24:20 Setting CNN weigths from save/word2_coco_tword013_a03_topdown_resnet152_msc/model-cnn-best.pth[0m
[33m01/23 20:24:23 Finetuning up from 0 modules in the cnn[0m
[33m01/23 20:24:23 Top Down : OrderedDict([('embed_1', Embedding(9488, 512)), ('embed', Sequential (
  (0): Embedding(9488, 512)
  (1): ReLU ()
  (2): Dropout (p = 0.5)
)), ('att_embed', Sequential (
  (0): Linear (2048 -> 512)
  (1): ReLU ()
  (2): Dropout (p = 0.0)
)), ('logit', Linear (512 -> 9488)), ('ctx2att', Linear (512 -> 512)), ('fc_embed', Sequential (
  (0): Linear (2048 -> 512)
  (1): ReLU ()
  (2): Dropout (p = 0.0)
)), ('core', TopDownCore (
  (att_lstm): LSTMCell(1536, 512)
  (lang_lstm): LSTMCell(1024, 512)
  (attention): Attention (
    (h2att): Linear (512 -> 512)
    (alpha_net): Linear (512 -> 1)
  )
))])[0m
[33m01/23 20:24:24 Loading the model dict (last checkpoint) ['embed_1.weight', 'embed.0.weight', 'att_embed.0.weight', 'att_embed.0.bias', 'logit.weight', 'logit.bias', 'ctx2att.weight', 'ctx2att.bias', 'fc_embed.0.weight', 'fc_embed.0.bias', 'core.att_lstm.weight_ih', 'core.att_lstm.weight_hh', 'core.att_lstm.bias_ih', 'core.att_lstm.bias_hh', 'core.lang_lstm.weight_ih', 'core.lang_lstm.weight_hh', 'core.lang_lstm.bias_ih', 'core.lang_lstm.bias_hh', 'core.attention.h2att.weight', 'core.attention.h2att.bias', 'core.attention.alpha_net.weight', 'core.attention.alpha_net.bias'][0m
[33m01/23 20:24:24 DataLoader loading json file: data/coco/cocotalk.json[0m
[33m01/23 20:24:24 vocab size is 9487 [0m
[33m01/23 20:24:24 DataLoader loading h5 file: data/coco/cocotalk.h5[0m
[33m01/23 20:24:24 read 123287 images of size 3x256x256[0m
[33m01/23 20:24:24 max sequence length in data is 16[0m
[33m01/23 20:24:24 assigned 82783 images to split train[0m
[33m01/23 20:24:24 assigned 5000 images to split val[0m
[33m01/23 20:24:24 assigned 5000 images to split test[0m
[32m01/23 20:24:26 Initialized Word2 loss tau=0.130, alpha=0.3[0m
[33m01/23 20:24:26 Evaluating the test split (-1)[0m
[33m01/23 20:30:00 Loading reference captions..[0m
[33m01/23 20:30:01 Reference captions loaded![0m
[33m01/23 20:30:03 using 5000/5000 predictions[0m
[33m01/23 20:30:04 Loading model captions[0m
[33m01/23 20:30:04 Model captions loaded[0m
[33m01/23 20:30:04 Starting evaluation...[0m
PTBTokenizer tokenized 307085 tokens at 1260668.94 tokens per second.
PTBTokenizer tokenized 51076 tokens at 388268.45 tokens per second.
[33m01/23 20:30:31 GPU ID: 3 | available memory: 16276M[0m
[33m01/23 20:30:39 Starting from the best saved model[0m
[33m01/23 20:30:39 Starting from save/fncnn6_word2_coco_tword013_a03_topdown_resnet152_msc/model-best.pth[0m
[36m01/23 20:30:44 Setting CNN weigths from save/fncnn6_word2_coco_tword013_a03_topdown_resnet152_msc/model-cnn-best.pth[0m
[33m01/23 20:30:48 Finetuning up from 6 modules in the cnn[0m
[33m01/23 20:30:48 Top Down : OrderedDict([('embed_1', Embedding(9488, 512)), ('embed', Sequential (
  (0): Embedding(9488, 512)
  (1): ReLU ()
  (2): Dropout (p = 0.5)
)), ('att_embed', Sequential (
  (0): Linear (2048 -> 512)
  (1): ReLU ()
  (2): Dropout (p = 0.0)
)), ('logit', Linear (512 -> 9488)), ('ctx2att', Linear (512 -> 512)), ('fc_embed', Sequential (
  (0): Linear (2048 -> 512)
  (1): ReLU ()
  (2): Dropout (p = 0.0)
)), ('core', TopDownCore (
  (att_lstm): LSTMCell(1536, 512)
  (lang_lstm): LSTMCell(1024, 512)
  (attention): Attention (
    (h2att): Linear (512 -> 512)
    (alpha_net): Linear (512 -> 1)
  )
))])[0m
[33m01/23 20:30:49 Loading the model dict (last checkpoint) ['embed_1.weight', 'embed.0.weight', 'att_embed.0.weight', 'att_embed.0.bias', 'logit.weight', 'logit.bias', 'ctx2att.weight', 'ctx2att.bias', 'fc_embed.0.weight', 'fc_embed.0.bias', 'core.att_lstm.weight_ih', 'core.att_lstm.weight_hh', 'core.att_lstm.bias_ih', 'core.att_lstm.bias_hh', 'core.lang_lstm.weight_ih', 'core.lang_lstm.weight_hh', 'core.lang_lstm.bias_ih', 'core.lang_lstm.bias_hh', 'core.attention.h2att.weight', 'core.attention.h2att.bias', 'core.attention.alpha_net.weight', 'core.attention.alpha_net.bias'][0m
[33m01/23 20:30:49 DataLoader loading json file: data/coco/cocotalk.json[0m
[33m01/23 20:30:49 vocab size is 9487 [0m
[33m01/23 20:30:49 DataLoader loading h5 file: data/coco/cocotalk.h5[0m
[33m01/23 20:30:49 read 123287 images of size 3x256x256[0m
[33m01/23 20:30:49 max sequence length in data is 16[0m
[33m01/23 20:30:49 assigned 82783 images to split train[0m
[33m01/23 20:30:49 assigned 5000 images to split val[0m
[33m01/23 20:30:49 assigned 5000 images to split test[0m
[32m01/23 20:30:50 Initialized Word2 loss tau=0.130, alpha=0.3[0m
[33m01/23 20:30:50 Evaluating the test split (-1)[0m
[33m01/23 20:36:25 Loading reference captions..[0m
[33m01/23 20:36:26 Reference captions loaded![0m
[33m01/23 20:36:28 using 5000/5000 predictions[0m
[33m01/23 20:36:28 Loading model captions[0m
[33m01/23 20:36:28 Model captions loaded[0m
[33m01/23 20:36:28 Starting evaluation...[0m
PTBTokenizer tokenized 307085 tokens at 1168434.10 tokens per second.
PTBTokenizer tokenized 50931 tokens at 379725.54 tokens per second.
[33m01/23 20:36:54 GPU ID: 3 | available memory: 16276M[0m
[33m01/23 20:37:02 Starting from the best saved model[0m
[33m01/23 20:37:02 Starting from save/importance_qhamming_limited1_tsent017_rcider_tsent05_a04_topdown/model-best.pth[0m
Traceback (most recent call last):
  File "eval.py", line 76, in <module>
    str(vars(opt)[k]) + ' vs. ' + str(vars(infos['opt'])[k]))
AssertionError: clip_reward option not consistent 1 vs. 0
[33m01/23 20:37:17 GPU ID: 3 | available memory: 16276M[0m
[33m01/23 20:37:24 Starting from the best saved model[0m
[33m01/23 20:37:24 Starting from save/importance_lazy_qhamming_limited1_tsent017_rcider_tsent05_a04_topdown/model-best.pth[0m
Traceback (most recent call last):
  File "eval.py", line 76, in <module>
    str(vars(opt)[k]) + ' vs. ' + str(vars(infos['opt'])[k]))
AssertionError: clip_reward option not consistent 1 vs. 0
[33m01/23 20:37:39 GPU ID: 3 | available memory: 16276M[0m
[33m01/23 20:37:46 Starting from the best saved model[0m
[33m01/23 20:37:46 Starting from save/fncnn6_importance_qhamming_limited1_tsent017_rcider_tsent05_a04_topdown/model-best.pth[0m
[36m01/23 20:37:50 Setting CNN weigths from save/fncnn6_importance_qhamming_limited1_tsent017_rcider_tsent05_a04_topdown/model-cnn-best.pth[0m
[33m01/23 20:37:55 Finetuning up from 6 modules in the cnn[0m
[33m01/23 20:37:56 Top Down : OrderedDict([('embed_1', Embedding(9488, 512)), ('embed', Sequential (
  (0): Embedding(9488, 512)
  (1): ReLU ()
  (2): Dropout (p = 0.5)
)), ('att_embed', Sequential (
  (0): Linear (2048 -> 512)
  (1): ReLU ()
  (2): Dropout (p = 0.0)
)), ('logit', Linear (512 -> 9488)), ('ctx2att', Linear (512 -> 512)), ('fc_embed', Sequential (
  (0): Linear (2048 -> 512)
  (1): ReLU ()
  (2): Dropout (p = 0.0)
)), ('core', TopDownCore (
  (att_lstm): LSTMCell(1536, 512)
  (lang_lstm): LSTMCell(1024, 512)
  (attention): Attention (
    (h2att): Linear (512 -> 512)
    (alpha_net): Linear (512 -> 1)
  )
))])[0m
[33m01/23 20:37:56 Loading the model dict (last checkpoint) ['embed_1.weight', 'embed.0.weight', 'att_embed.0.weight', 'att_embed.0.bias', 'logit.weight', 'logit.bias', 'ctx2att.weight', 'ctx2att.bias', 'fc_embed.0.weight', 'fc_embed.0.bias', 'core.att_lstm.weight_ih', 'core.att_lstm.weight_hh', 'core.att_lstm.bias_ih', 'core.att_lstm.bias_hh', 'core.lang_lstm.weight_ih', 'core.lang_lstm.weight_hh', 'core.lang_lstm.bias_ih', 'core.lang_lstm.bias_hh', 'core.attention.h2att.weight', 'core.attention.h2att.bias', 'core.attention.alpha_net.weight', 'core.attention.alpha_net.bias'][0m
[33m01/23 20:37:56 DataLoader loading json file: data/coco/cocotalk.json[0m
[33m01/23 20:37:57 vocab size is 9487 [0m
[33m01/23 20:37:57 DataLoader loading h5 file: data/coco/cocotalk.h5[0m
[33m01/23 20:37:57 read 123287 images of size 3x256x256[0m
[33m01/23 20:37:57 max sequence length in data is 16[0m
[33m01/23 20:37:57 assigned 82783 images to split train[0m
[33m01/23 20:37:57 assigned 5000 images to split val[0m
[33m01/23 20:37:57 assigned 5000 images to split test[0m
[32m01/23 20:37:59 using importance sampling r=CIDEr (tau=0.50) and q=Hamming (Vpool=1, tau=0.17)[0m
[33m01/23 20:37:59 Evaluating the test split (-1)[0m
[33m01/23 20:44:46 Loading reference captions..[0m
[33m01/23 20:44:48 Reference captions loaded![0m
[33m01/23 20:44:50 using 5000/5000 predictions[0m
[33m01/23 20:44:50 Loading model captions[0m
[33m01/23 20:44:50 Model captions loaded[0m
[33m01/23 20:44:50 Starting evaluation...[0m
PTBTokenizer tokenized 307085 tokens at 1314263.51 tokens per second.
PTBTokenizer tokenized 51938 tokens at 394509.39 tokens per second.
[33m01/23 20:45:17 GPU ID: 3 | available memory: 16276M[0m
[33m01/23 20:45:25 Starting from the best saved model[0m
[33m01/23 20:45:25 Starting from save/fncnn6_importance_lazy_qhamming_limited1_tsent017_rcider_tsent05_a04_topdown/model-best.pth[0m
[36m01/23 20:45:29 Setting CNN weigths from save/fncnn6_importance_lazy_qhamming_limited1_tsent017_rcider_tsent05_a04_topdown/model-cnn-best.pth[0m
[33m01/23 20:45:33 Finetuning up from 6 modules in the cnn[0m
[33m01/23 20:45:33 Top Down : OrderedDict([('embed_1', Embedding(9488, 512)), ('embed', Sequential (
  (0): Embedding(9488, 512)
  (1): ReLU ()
  (2): Dropout (p = 0.5)
)), ('att_embed', Sequential (
  (0): Linear (2048 -> 512)
  (1): ReLU ()
  (2): Dropout (p = 0.0)
)), ('logit', Linear (512 -> 9488)), ('ctx2att', Linear (512 -> 512)), ('fc_embed', Sequential (
  (0): Linear (2048 -> 512)
  (1): ReLU ()
  (2): Dropout (p = 0.0)
)), ('core', TopDownCore (
  (att_lstm): LSTMCell(1536, 512)
  (lang_lstm): LSTMCell(1024, 512)
  (attention): Attention (
    (h2att): Linear (512 -> 512)
    (alpha_net): Linear (512 -> 1)
  )
))])[0m
[33m01/23 20:45:34 Loading the model dict (last checkpoint) ['embed_1.weight', 'embed.0.weight', 'att_embed.0.weight', 'att_embed.0.bias', 'logit.weight', 'logit.bias', 'ctx2att.weight', 'ctx2att.bias', 'fc_embed.0.weight', 'fc_embed.0.bias', 'core.att_lstm.weight_ih', 'core.att_lstm.weight_hh', 'core.att_lstm.bias_ih', 'core.att_lstm.bias_hh', 'core.lang_lstm.weight_ih', 'core.lang_lstm.weight_hh', 'core.lang_lstm.bias_ih', 'core.lang_lstm.bias_hh', 'core.attention.h2att.weight', 'core.attention.h2att.bias', 'core.attention.alpha_net.weight', 'core.attention.alpha_net.bias'][0m
[33m01/23 20:45:34 DataLoader loading json file: data/coco/cocotalk.json[0m
[33m01/23 20:45:34 vocab size is 9487 [0m
[33m01/23 20:45:34 DataLoader loading h5 file: data/coco/cocotalk.h5[0m
[33m01/23 20:45:34 read 123287 images of size 3x256x256[0m
[33m01/23 20:45:34 max sequence length in data is 16[0m
[33m01/23 20:45:34 assigned 82783 images to split train[0m
[33m01/23 20:45:34 assigned 5000 images to split val[0m
[33m01/23 20:45:34 assigned 5000 images to split test[0m
[32m01/23 20:45:37 using importance sampling r=CIDEr (tau=0.50) and q=Hamming (Vpool=1, tau=0.17)[0m
[33m01/23 20:45:37 Evaluating the test split (-1)[0m
[33m01/23 20:51:51 Loading reference captions..[0m
[33m01/23 20:51:53 Reference captions loaded![0m
[33m01/23 20:51:55 using 5000/5000 predictions[0m
[33m01/23 20:51:55 Loading model captions[0m
[33m01/23 20:51:55 Model captions loaded[0m
[33m01/23 20:51:55 Starting evaluation...[0m
PTBTokenizer tokenized 307085 tokens at 1200000.01 tokens per second.
PTBTokenizer tokenized 51629 tokens at 427947.76 tokens per second.
[33m01/23 21:49:14 GPU ID: 0 | available memory: 16276M[0m
[33m01/23 21:49:15 Starting from the best saved model[0m
[33m01/23 21:49:15 Starting from save/importance_lazy_qhamming_limited1_tsent017_rcider_tsent05_a04_topdown/model-best.pth[0m
[36m01/23 21:49:19 Setting CNN weigths from save/importance_lazy_qhamming_limited1_tsent017_rcider_tsent05_a04_topdown/model-cnn-best.pth[0m
[33m01/23 21:49:21 Starting from the best saved model[0m
[33m01/23 21:49:21 Starting from save/importance_qhamming_limited1_tsent017_rcider_tsent05_a04_topdown/model-best.pth[0m
[36m01/23 21:49:25 Setting CNN weigths from save/importance_qhamming_limited1_tsent017_rcider_tsent05_a04_topdown/model-cnn-best.pth[0m
[33m01/23 21:49:35 Finetuning up from 0 modules in the cnn[0m
[33m01/23 21:49:36 Top Down : OrderedDict([('embed_1', Embedding(9488, 512)), ('embed', Sequential (
  (0): Embedding(9488, 512)
  (1): ReLU ()
  (2): Dropout (p = 0.5)
)), ('att_embed', Sequential (
  (0): Linear (2048 -> 512)
  (1): ReLU ()
  (2): Dropout (p = 0.0)
)), ('logit', Linear (512 -> 9488)), ('ctx2att', Linear (512 -> 512)), ('fc_embed', Sequential (
  (0): Linear (2048 -> 512)
  (1): ReLU ()
  (2): Dropout (p = 0.0)
)), ('core', TopDownCore (
  (att_lstm): LSTMCell(1536, 512)
  (lang_lstm): LSTMCell(1024, 512)
  (attention): Attention (
    (h2att): Linear (512 -> 512)
    (alpha_net): Linear (512 -> 1)
  )
))])[0m
[33m01/23 21:49:37 Loading the model dict (last checkpoint) ['embed_1.weight', 'embed.0.weight', 'att_embed.0.weight', 'att_embed.0.bias', 'logit.weight', 'logit.bias', 'ctx2att.weight', 'ctx2att.bias', 'fc_embed.0.weight', 'fc_embed.0.bias', 'core.att_lstm.weight_ih', 'core.att_lstm.weight_hh', 'core.att_lstm.bias_ih', 'core.att_lstm.bias_hh', 'core.lang_lstm.weight_ih', 'core.lang_lstm.weight_hh', 'core.lang_lstm.bias_ih', 'core.lang_lstm.bias_hh', 'core.attention.h2att.weight', 'core.attention.h2att.bias', 'core.attention.alpha_net.weight', 'core.attention.alpha_net.bias'][0m
[33m01/23 21:49:37 DataLoader loading json file: data/coco/cocotalk.json[0m
[33m01/23 21:49:38 vocab size is 9487 [0m
[33m01/23 21:49:38 DataLoader loading h5 file: data/coco/cocotalk.h5[0m
[33m01/23 21:49:38 read 123287 images of size 3x256x256[0m
[33m01/23 21:49:38 max sequence length in data is 16[0m
[33m01/23 21:49:38 assigned 82783 images to split train[0m
[33m01/23 21:49:38 assigned 5000 images to split val[0m
[33m01/23 21:49:38 assigned 5000 images to split test[0m
[32m01/23 21:49:31 using importance sampling r=CIDEr (tau=0.50) and q=Hamming (Vpool=1, tau=0.17)[0m
[33m01/23 21:49:31 Evaluating the test split (-1)[0m
[32m01/23 21:49:43 using importance sampling r=CIDEr (tau=0.50) and q=Hamming (Vpool=1, tau=0.17)[0m
[33m01/23 21:49:43 Evaluating the test split (-1)[0m
[33m01/23 21:54:50 Loading reference captions..[0m
[33m01/23 21:54:51 Reference captions loaded![0m
[33m01/23 21:54:53 using 5000/5000 predictions[0m
[33m01/23 21:54:53 Loading model captions[0m
[33m01/23 21:54:53 Model captions loaded[0m
[33m01/23 21:54:53 Starting evaluation...[0m
PTBTokenizer tokenized 307085 tokens at 508378.12 tokens per second.
PTBTokenizer tokenized 51579 tokens at 332065.26 tokens per second.
## OAR [2018-01-23 21:55:03] Job 138037 KILLED ##
[33m01/23 22:00:44 GPU ID: 3 | available memory: 16276M[0m
[33m01/23 22:00:51 Starting from the best saved model[0m
[33m01/23 22:00:51 Starting from save/importance_qhamming_limited1_tsent017_rcider_tsent05_a04_topdown/model-best.pth[0m
[36m01/23 22:00:56 Setting CNN weigths from save/importance_qhamming_limited1_tsent017_rcider_tsent05_a04_topdown/model-cnn-best.pth[0m
[33m01/23 22:00:59 Finetuning up from 0 modules in the cnn[0m
[33m01/23 22:00:59 Top Down : OrderedDict([('embed_1', Embedding(9488, 512)), ('embed', Sequential (
  (0): Embedding(9488, 512)
  (1): ReLU ()
  (2): Dropout (p = 0.5)
)), ('att_embed', Sequential (
  (0): Linear (2048 -> 512)
  (1): ReLU ()
  (2): Dropout (p = 0.0)
)), ('logit', Linear (512 -> 9488)), ('ctx2att', Linear (512 -> 512)), ('fc_embed', Sequential (
  (0): Linear (2048 -> 512)
  (1): ReLU ()
  (2): Dropout (p = 0.0)
)), ('core', TopDownCore (
  (att_lstm): LSTMCell(1536, 512)
  (lang_lstm): LSTMCell(1024, 512)
  (attention): Attention (
    (h2att): Linear (512 -> 512)
    (alpha_net): Linear (512 -> 1)
  )
))])[0m
[33m01/23 22:00:59 Loading the model dict (last checkpoint) ['embed_1.weight', 'embed.0.weight', 'att_embed.0.weight', 'att_embed.0.bias', 'logit.weight', 'logit.bias', 'ctx2att.weight', 'ctx2att.bias', 'fc_embed.0.weight', 'fc_embed.0.bias', 'core.att_lstm.weight_ih', 'core.att_lstm.weight_hh', 'core.att_lstm.bias_ih', 'core.att_lstm.bias_hh', 'core.lang_lstm.weight_ih', 'core.lang_lstm.weight_hh', 'core.lang_lstm.bias_ih', 'core.lang_lstm.bias_hh', 'core.attention.h2att.weight', 'core.attention.h2att.bias', 'core.attention.alpha_net.weight', 'core.attention.alpha_net.bias'][0m
[33m01/23 22:00:59 DataLoader loading json file: data/coco/cocotalk.json[0m
[33m01/23 22:01:00 vocab size is 9487 [0m
[33m01/23 22:01:00 DataLoader loading h5 file: data/coco/cocotalk.h5[0m
[33m01/23 22:01:00 read 123287 images of size 3x256x256[0m
[33m01/23 22:01:00 max sequence length in data is 16[0m
[33m01/23 22:01:00 assigned 82783 images to split train[0m
[33m01/23 22:01:00 assigned 5000 images to split val[0m
[33m01/23 22:01:00 assigned 5000 images to split test[0m
[32m01/23 22:01:02 using importance sampling r=CIDEr (tau=0.50) and q=Hamming (Vpool=1, tau=0.17)[0m
[33m01/23 22:01:02 Evaluating the test split (-1)[0m
[33m01/23 22:07:51 Loading reference captions..[0m
[33m01/23 22:07:52 Reference captions loaded![0m
[33m01/23 22:07:54 using 5000/5000 predictions[0m
[33m01/23 22:07:54 Loading model captions[0m
[33m01/23 22:07:54 Model captions loaded[0m
[33m01/23 22:07:54 Starting evaluation...[0m
PTBTokenizer tokenized 307085 tokens at 1284996.15 tokens per second.
PTBTokenizer tokenized 51893 tokens at 370844.61 tokens per second.
[33m01/24 12:17:45 GPU ID: 0 | available memory: 12189M[0m
[33m01/24 12:18:10 GPU ID: 0 | available memory: 12189M[0m
33m01/24 12:18:12 Starting from save/fncnn6_strat_rhamming_pool2_tsent03_a05_showtell152/model-best.pth[0m
[36m01/24 12:18:16 Setting CNN weigths from save/fncnn6_strat_rhamming_pool2_tsent03_a05_showtell152/model-cnn-best.pth[0m
[33m01/24 12:18:20 Finetuning up from 6 modules in the cnn[0m
[33m01/24 12:18:20 Show & Tell : OrderedDict([('img_embed', Linear (2048 -> 512)), ('core', LSTM(512, 512, bias=False, dropout=0.5)), ('embed', Embedding(9488, 512)), ('drop_x_lm', Dropout (p = 0.5)), ('logit', Linear (512 -> 9488))])[0m
[33m01/24 12:18:20 Loading the model dict (last checkpoint) ['img_embed.weight', 'img_embed.bias', 'core.weight_ih_l0', 'core.weight_hh_l0', 'embed.weight', 'logit.weight', 'logit.bias'][0m
[33m01/24 12:18:21 DataLoader loading json file: data/coco/cocotalk.json[0m
[33m01/24 12:18:22 vocab size is 9487 [0m
[33m01/24 12:18:22 DataLoader loading h5 file: data/coco/cocotalk.h5[0m
[33m01/24 12:18:22 read 123287 images of size 3x256x256[0m
[33m01/24 12:18:22 max sequence length in data is 16[0m
[33m01/24 12:18:22 assigned 82783 images to split train[0m
[33m01/24 12:18:22 assigned 5000 images to split val[0m
[33m01/24 12:18:22 assigned 5000 images to split test[0m
[32m01/24 12:18:22 RewardSampler (stratified sampling), r=Hamming (Vpool=2, tau=0.30)[0m
[33m01/24 12:18:22 Evaluating the test split (-1)[0m
[33m01/24 12:18:42 Starting from the best saved model[0m
[33m01/24 12:18:42 Starting from save/fncnn6_strat_rhamming_pool1_tsent017_a04_showtell152/model-best.pth[0m
[36m01/24 12:18:47 Setting CNN weigths from save/fncnn6_strat_rhamming_pool1_tsent017_a04_showtell152/model-cnn-best.pth[0m
[33m01/24 12:19:42 Finetuning up from 6 modules in the cnn[0m
[33m01/24 12:19:42 Show & Tell : OrderedDict([('img_embed', Linear (2048 -> 512)), ('core', LSTM(512, 512, bias=False, dropout=0.5)), ('embed', Embedding(9488, 512)), ('drop_x_lm', Dropout (p = 0.5)), ('logit', Linear (512 -> 9488))])[0m
[33m01/24 12:19:47 Loading the model dict (last checkpoint) ['img_embed.weight', 'img_embed.bias', 'core.weight_ih_l0', 'core.weight_hh_l0', 'embed.weight', 'logit.weight', 'logit.bias'][0m
[33m01/24 12:19:49 DataLoader loading json file: data/coco/cocotalk.json[0m
[33m01/24 12:19:50 vocab size is 9487 [0m
[33m01/24 12:19:50 DataLoader loading h5 file: data/coco/cocotalk.h5[0m
[33m01/24 12:19:50 read 123287 images of size 3x256x256[0m
[33m01/24 12:19:50 max sequence length in data is 16[0m
[33m01/24 12:19:51 assigned 82783 images to split train[0m
[33m01/24 12:19:51 assigned 5000 images to split val[0m
[33m01/24 12:19:51 assigned 5000 images to split test[0m
[32m01/24 12:19:51 RewardSampler (stratified sampling), r=Hamming (Vpool=1, tau=0.17)[0m
[33m01/24 12:19:51 Evaluating the test split (-1)[0m
[33m01/24 12:22:48 Loading reference captions..[0m
[33m01[33m01/24 12:22:54 Starting from the best saved model[0m
[33m01/24 12:22:54 Starting from save/fncnn6_importance_lazy_qhamming_limited1_tsent017_rcider_tsent05_a04_showtell152/model-best.pth[0m
4 12:22:51 Starting evaluation...[0m
PTBTokenizer tokenized 307085 tokens at 1404398.05 tokens per second.
PTBTokenizer tokenized 51360 tokens at 382953.28 tokens per second.
[36m01/24 12:23:00 Setting CNN weigths from save/fncnn6_importance_lazy_qhamming_limited1_tsent017_rcider_tsent05_a04_showtell152/model-cnn-best.pth[0m
[33m01/24 12:23:10 Finetuning up from 6 modules in the cnn[0m
[33m01/24 12:23:11 Show & Tell : OrderedDict([('img_embed', Linear (2048 -> 512)), ('core', LSTM(512, 512, bias=False, dropout=0.5)), ('embed', Embedding(9488, 512)), ('drop_x_lm', Dropout (p = 0.5)), ('logit', Linear (512 -> 9488))])[0m
[33m01/24 12:23:12 Loading the model dict (last checkpoint) ['img_embed.weight', 'img_embed.bias', 'core.weight_ih_l0', 'core.weight_hh_l0', 'embed.weight', 'logit.weight', 'logit.bias'][0m
[33m01/24 12:23:14 DataLoader loading json file: data/coco/c[33m01/24 12:23:32 Starting from the best saved model[0m
[33m01/24 12:23:32 Starting from save/fncnn6_importance_qhamming_limited1_tsent017_rcider_tsent05_a04_showtell152/model-best.pth[0m
[36m01/24 12:23:36 Setting CNN weigths from save/fncnn6_importance_qhamming_limited1_tsent017_rcider_tsent05_a04_showtell152/model-cnn-best.pth[0m
[33m01/24 12:23:39 Finetuning up from 6 modules in the cnn[0m
[33m01/24 12:23:39 Show & Tell : OrderedDict([('img_embed', Linear (2048 -> 512)), ('core', LSTM(512, 512, bias=False, dropout=0.5)), ('embed', Embedding(9488, 512)), ('drop_x_lm', Dropout (p = 0.5)), ('logit', Linear (512 -> 9488))])[0m
[33m01/24 12:23:40 Loading the model dict (last checkpoint) ['img_embed.weight', 'img_embed.bias', 'core.weight_ih_l0', 'core.weight_hh_l0', 'embed.weight', 'logit.weight', 'logit.bias'][0m
[33m01/24 12:23:40 DataLoader loading json file: data/coco/cocotalk.json[0m
[33m01/24 12:23:41 vocab size is 9487 [0m
[33m01/24 12:23:41 DataLoader loading h5 file: data/coco/cocotalk.h5[0m
[33m01/24 12:23:41 read 123287 images of size 3x256x256[0m
[33m01/24 12:23:41 max sequence length in data is 16[0m
[33m01/24 12:23:41 assigned 82783 images to split train[0m
[33m01/24 12:23:41 assigned 5000 images to split val[0m
[33m01/24 12:23:41 assigned 5000 images to split test[0m
[32m01/24 12:23:43 using importance sampling r=CIDEr (tau=0.50) and q=Hamming (Vpool=1, tau=0.17)[0m
[33m01/24 12:23:43 Evaluating the test split (-1)[0m
[33m01/24 12:25:14 GPU ID: 0 | available memory: 12207M[0m
[33m01/24 12:25:20 Starting from the best saved model[0m
[33m01/24 12:25:20 Starting from save/fncnn6_word_coco_tword009_idf10_a07_showtell152/model-best.pth[0m
[36m01/24 12:25:24 Setting CNN weigths from save/fncnn6_word_coco_tword009_idf10_a07_showtell152/model-cnn-best.pth[0m
[33m01/24 12:25:29 Finetuning up from 6 modules in the cnn[0m
[33m01/24 12:25:29 Show & Tell : OrderedDict([('img_embed', Linear (2048 -> 512)), ('core', LSTM(512, 512, bias=False, dropout=0.5)), ('embed', Embedding(9488, 512)), ('drop_x_lm', Dropout (p = 0.5)), ('logit', Linear (512 -> 9488))])[0m
[33m01/24 12:25:29 Loading the model dict (last checkpoint) ['img_embed.weight', 'img_embed.bias', 'core.weight_ih_l0', 'core.weight_hh_l0', 'embed.weight', 'logit.weight', 'logit.bias'][0m
[33m01/24 12:25:31 DataLoader loading json file: data/coco/cocotalk.json[0m
[33m01/24 12:25:31 vocab size is 9487 [0m
[33m01/24 12:25:31 DataLoader loading h5 file: data/coco/cocotalk.h5[0m
[33m01/24 12:25:31 read 123287 images of size 3x256x256[0m
[33m01/24 12:25:31 max sequence length in data is 16[0m
[33m01/24 12:25:31 assigned 82783 images to split train[0m
[33m01/24 12:25:31 assigned 5000 images to split val[0m
[33m01/24 12:25:31 assigned 5000 images to split test[0m
[32m01/24 12:25:48 Initialized Word2 loss tau=0.090, alpha=0.7[0m
[33m01/24 12:25:48 Evaluating the test split (-1)[0m
[33m01/24 12:27:59 Loading reference captions..[0m
[33m01/24 12:28:00 Reference captions loaded![0m
[33m01/24 12:28:02 using 5000/5000 predictions[0m
[33m01/24 12:28:02 Loading model captions[0m
[33m01/24 12:28:02 Model captions loaded[0m
[33m01/24 12:28:02 Starting evaluation...[0m
PTBTokenizer tokenized 307085 tokens at 1101995.19 tokens per second.
PTBTokenizer tokenized 51933 tokens at 376506.67 tokens per second.
[33m01/24 12:28:32 GPU ID: 0 | available memory: 12189M[0m
[33m01/24 12:28:44 Starting from the best saved model[0m
[33m01/24 12:28:44 Starting from save/fncnn6_baseline_GloveW_coco_frozen_showtell152/model-best.pth[0m
36m01/24 12:28:41 Setting CNN weigths from save/fncnn6_baseline_GloveW_coco_free_showtell152/model-cnn-best.pth[0m
[33m01/24 12:28:45 Finetuning up from 6 modules in the cnn[0m
[32m01/24 12:28:45 Loading weights to initialize W[0m
[33m01/24 12:28:45 Show & Tell : OrderedDict([('img_embed', Linear (2048 -> 512)), ('core', LSTM(512, 512, bias=False, dropout=0.5)), ('embed', Embedding(9488, 512)), ('drop_x_lm', Dropout (p = 0.5)), ('logit', Linear (512 -> 9488))])[0m
[33m01/24 12:28:45 Loading the model dict (last checkpoint) ['img_embed.weight', 'img_embed.bias', 'core.weight_ih_l0', 'core.weight_hh_l0', 'embed.weight', 'logit.weight', 'logit.bias'][0m
[33m01/24 12:28:46 DataLoader loading json file: data/coco/cocotalk.json[0m
[33m01/24 12:28:46 vocab size is 9487 [0m
[33m01/24 12:28:46 DataLoader loading h5 file: data/coco/cocotalk.h5[0m
[33m01/24 12:28:46 read 123287 images of size 3x256x256[0m
[33m01/24 12:28:46 max sequence length in data is 16[0m
[33m01/24 12:28:46 assigned 82783 images to split train[0m
[33m01/24 12:28:46 assigned 5000 images to split val[0m
[33m01/24 12:28:46 assigned 5000 images to split test[0m
[32m01/24 12:28:46 Default ML loss[0m
[33m01/24 12:28:46 Evaluating the test split (-1)[0m
[36m01/24 12:28:49 Setting CNN weigths from save/fncnn6_baseline_GloveW_coco_frozen_showtell152/model-cnn-best.pth[0m
[33m01/24 12:28:54 Finetuning up from 6 modules in the cnn[0m
[32m01/24 12:28:54 Loading weights to initialize W[0m
[33m01/24 12:28:54 Show & Tell : OrderedDict([('img_embed', Linear (2048 -> 512)), ('core', LSTM(512, 512, bias=False, dropout=0.5)), ('embed', Embedding(9488, 512)), ('drop_x_lm', Dropout (p = 0.5)), ('logit', Linear (512 -> 9488))])[0m
[33m01/24 12:28:55 Loading the model dict (last checkpoint) ['img_embed.weight', 'img_embed.bias', 'core.weight_ih_l0', 'core.weight_hh_l0', 'embed.weight', 'logit.weight', 'logit.bias'][0m
[33m01/24 12:28:56 DataLoader loading json file: data/coco/cocotalk.json[0m
[33m01/24 12:28:57 vocab size is 9487 [0m
[33m01/24 12:28:57 DataLoader loading h5 file: data/coco/cocotalk.h5[0m
[33m01/24 12:28:57 read 123287 images of size 3x256x256[0m
[33m01/24 12:28:57 max sequence length in data is 16[0m
[33m01/24 12:28:57 assigned 82783 images to split train[0m
[33m01/24 12:28:57 assigned 5000 images to split val[0m
[33m01/24 12:28:57 assigned 5000 images to split test[0m
[32m01/24 12:28:57 Default ML loss[0m
[33m01/24 12:28:57 Evaluating the test split (-1)[0m
[33m01/24 12:31:25 Loading reference captions..[0m
[33m01/24 12:31:27 Reference captions loaded![0m
[33m01/24 12:31:29 using 5000/5000 predictions[0m
[33m01/24 12:31:29 Loading model captions[0m
[33m01/24 12:31:29 Model captions loaded[0m
[33m01/24 12:31:29 Starting evaluation...[0m
PTBTokenizer tokenized 307085 tokens at 1142932.36 tokens per second.
PTBTokenizer tokenized 51598 tokens at 313973.81 tokens per second.
[33m01/24 12:31:51 Loading reference captions..[0m
[33m01/24 12:31:52 Reference captions loaded![0m
[33m01/24 12:31:54 using 5000/5000 predictions[0m
[33m01/24 12:31:54 Loading model captions[0m
[33m01/24 12:31:54 Model captions loaded[0m
[33m01/24 12:31:54 Starting evaluation...[0m
PTBTokenizer tokenized 307085 tokens at 1071584.84 tokens per second.
PTBTokenizer tokenized 49975 tokens at 330007.05 tokens per second.
[33m01/24 12:32:07 Loading reference captions..[0m
[33m01/24 12:32:08 Reference captions loaded![0m
[33m01/24 12:32:10 using 5000/5000 predictions[0m
[33m01/24 12:32:10 Loading model captions[0m
[33m01/24 12:32:10 Model captions loaded[0m
[33m01/24 12:32:10 Starting evaluation...[0m
PTBTokenizer tokenized 307085 tokens at 1349656.66 tokens per second.
PTBTokenizer tokenized 51902 tokens at 384234.07 tokens per second.
[33m01/24 12:33:28 Loading reference captions..[0m
[33m01/24 12:33:29 Reference captions loaded![0m
[33m01/24 12:33:31 using 5000/5000 predictions[0m
[33m01/24 12:33:31 Loading model captions[0m
[33m01/24 12:33:31 Model captions loaded[0m
[33m01/24 12:33:31 Starting evaluation...[0m
PTBTokenizer tokenized 307085 tokens at 510941,67 tokens per second.
[33m01/24 12:33:35 Starting from the best saved model[0m
[33m01/24 12:33:35 Starting from save/ml_penalize03_showtell152/model-best.pth[0m
[36m01/24 12:33:39 Setting CNN weigths from save/ml_penalize03_showtell152/model-cnn-best.pth[0m
[33m01/24 12:33:42 Finetuning up from 0 modules in the cnn[0m
[33m01/24 12:33:43 Show & Tell : OrderedDict([('img_embed', Linear (2048 -> 512)), ('core', LSTM(512, 512, bias=False, dropout=0.5)), ('embed', Embedding(9488, 512)), ('drop_x_lm', Dropout (p = 0.5)), ('logit', Linear (512 -> 9488))])[0m
[33m01/24 12:33:43 Loading the model dict (last checkpoint) ['img_embed.weight', 'img_embed.bias', 'core.weight_ih_l0', 'core.weight_hh_l0', 'embed.weight', 'logit.weight', 'logit.bias'][0m
[33m01/24 12:33:44 DataLoader loading json file: data/coco/cocotalk.json[0m
[33m01/24 12:33:45 vocab size is 9487 [0m
[33m01/24 12:33:45 DataLoader loading h5 file: data/coco/cocotalk.h5[0m
[33m01/24 12:33:45 read 123287 images of size 3x256x256[0m
[33m01/24 12:33:45 max sequence length in data is 16[0m
[33m01/24 12:33:45 assigned 82783 images to split train[0m
[33m01/24 12:33:45 assigned 5000 images to split val[0m
[33m01/24 12:33:45 assigned 5000 images to split test[0m
[32m01/24 12:33:45 Default ML loss[0m
[33m01/24 12:33:45 Evaluating the test split (-1)[0m
[33m01/24 12:35:35 GPU ID: 0 | available memory: 12207M[0m
[33m01/24 12:35:41 Starting from the best saved model[0m
[33m01/24 12:35:41 Starting from save/fncnn6_word_coco_tword015_a07_showtell152/model-best.pth[0m
[36m01/24 12:35:46 Setting CNN weigths from save/fncnn6_word_coco_tword015_a07_showtell152/model-cnn-best.pth[0m
[33m01/24 12:36:04 Finetuning up from 6 modules in the cnn[0m
[33m01/24 12:36:04 Show & Tell : OrderedDict([('img_embed', Linear (2048 -> 512)), ('core', LSTM(512, 512, bias=False, dropout=0.5)), ('embed', Embedding(9488, 512)), ('drop_x_lm', Dropout (p = 0.5)), ('logit', Linear (512 -> 9488))])[0m
[33m01/24 12:36:05 Loading the model dict (last checkpoint) ['img_embed.weight', 'img_embed.bias', 'core.weight_ih_l0', 'core.weight_hh_l0', 'embed.weight', 'logit.weight', 'logit.bias'][0m
[33m01/24 12:36:06 DataLoader loading json file: data/coco/cocotalk.json[0m
[33m01/24 12:36:06 vocab size is 9487 [0m
[33m01/24 12:36:06 DataLoader loading h5 file: data/coco/cocotalk.h5[0m
[33m01/24 12:36:06 read 123287 images of size 3x256x256[0m
[33m01/24 12:36:06 max sequence length in data is 16[0m
[33m01/24 12:36:06 assigned 82783 images to split train[0m
[33m01/24 12:36:06 assigned 5000 images to split val[0m
[33m01/24 12:36:06 assigned 5000 images to split test[0m
[32m01/24 12:36:07 Initialized Word2 loss tau=0.150, alpha=0.7[0m
[33m01/24 12:36:07 Evaluating the test split (-1)[0m
[33m01/24 12:37:13 Loading reference captions..[0m
[33m01/24 12:37:14 Reference captions loaded![0m
[33m01/24 12:37:15 using 5000/5000 predictions[0m
[33m01/24 12:37:15 Loading model captions[0m
[33m01/24 12:37:16 Model captions loaded[0m
[33m01/24 12:37:16 Starting evaluation...[0m
PTBTokenizer tokenized 307085 tokens at 1268279.08 tokens per second.
PTBTokenizer tokenized 51589 tokens at 456806.87 tokens per second.
[33m01/24 12:40:20 Loading reference captions..[0m
[33m01/24 12:40:20 Reference captions loaded![0m
[33m01/24 12:40:22 using 5000/5000 predictions[0m
[33m01/24 12:40:22 Loading model captions[0m
[33m01/24 12:40:22 Model captions loaded[0m
[33m01/24 12:40:22 Starting evaluation...[0m
PTBTokenizer tokenized 307085 tokens at 1161930.81 tokens per second.
PTBTokenizer tokenized 50027 tokens at 348392.62 tokens per second.
[33m01/24 13:15:58 GPU ID: 0 | available memory: 8112M[0m
[33m01/24 13:16:03 Starting from the best saved model[0m
[33m01/24 13:16:03 Starting from save/fncnn6_strat_lazy_rhamming_pool1_tsent017_a04_topdown/model-best.pth[0m
[36m01/24 13:16:09 Setting CNN weigths from save/fncnn6_strat_lazy_rhamming_pool1_tsent017_a04_topdown/model-cnn-best.pth[0m
[33m01/24 13:16:16 Finetuning up from 6 modules in the cnn[0m
[33m01/24 13:16:17 Top Down : OrderedDict([('embed_1', Embedding(9488, 512)), ('embed', Sequential (
  (0): Embedding(9488, 512)
  (1): ReLU ()
  (2): Dropout (p = 0.5)
)), ('att_embed', Sequential (
  (0): Linear (2048 -> 512)
  (1): ReLU ()
  (2): Dropout (p = 0.0)
)), ('logit', Linear (512 -> 9488)), ('ctx2att', Linear (512 -> 512)), ('fc_embed', Sequential (
  (0): Linear (2048 -> 512)
  (1): ReLU ()
  (2): Dropout (p = 0.0)
)), ('core', TopDownCore (
  (att_lstm): LSTMCell(1536, 512)
  (lang_lstm): LSTMCell(1024, 512)
  (attention): Attention (
    (h2att): Linear (512 -> 512)
    (alpha_net): Linear (512 -> 1)
  )
))])[0m
[33m01/24 13:16:19 Loading the model dict (last checkpoint) ['embed_1.weight', 'embed.0.weight', 'att_embed.0.weight', 'att_embed.0.bias', 'logit.weight', 'logit.bias', 'ctx2att.weight', 'ctx2att.bias', 'fc_embed.0.weight', 'fc_embed.0.bias', 'core.att_lstm.weight_ih', 'core.att_lstm.weight_hh', 'core.att_lstm.bias_ih', 'core.att_lstm.bias_hh', 'core.lang_lstm.weight_ih', 'core.lang_lstm.weight_hh', 'core.lang_lstm.bias_ih', 'core.lang_lstm.bias_hh', 'core.attention.h2att.weight', 'core.attention.h2att.bias', 'core.attention.alpha_net.weight', 'core.attention.alpha_net.bias'][0m
[33m01/24 13:16:19 DataLoader loading json file: data/coco/cocotalk.json[0m
[33m01/24 13:16:19 vocab size is 9487 [0m
[33m01/24 13:16:19 DataLoader loading h5 file: data/coco/cocotalk.h5[0m
[33m01/24 13:16:19 read 123287 images of size 3x256x256[0m
[33m01/24 13:16:19 max sequence length in data is 16[0m
[33m01/24 13:16:19 assigned 82783 images to split train[0m
[33m01/24 13:16:19 assigned 5000 images to split val[0m
[33m01/24 13:16:19 assigned 5000 images to split test[0m
[32m01/24 13:16:19 RewardSampler (stratified sampling), r=Hamming (Vpool=1, tau=0.17)[0m
[33m01/24 13:16:19 Evaluating the test split (-1)[0m
[33m01/24 13:22:39 Loading reference captions..[0m
[33m01/24 13:22:40 Reference captions loaded![0m
[33m01/24 13:22:43 using 5000/5000 predictions[0m
[33m01/24 13:22:43 Loading model captions[0m
[33m01/24 13:22:43 Model captions loaded[0m
[33m01/24 13:22:43 Starting evaluation...[0m
PTBTokenizer tokenized 307085 tokens at 1056744.33 tokens per second.
PTBTokenizer tokenized 51511 tokens at 340257.51 tokens per second.
[33m01/24 13:47:48 GPU ID: 1 | available memory: 12207M[0m
[33m01/24 13:48:30 Starting from the best saved model[0m
[33m01/24 13:48:30 Starting from save/sample2_hamming_tsent03_limited2_a04_topdown_resnet152_msc/model-best.pth[0m
[36m01/24 13:48:34 Setting CNN weigths from save/sample2_hamming_tsent03_limited2_a04_topdown_resnet152_msc/model-cnn-best.pth[0m
[33m01/24 13:49:06 Finetuning up from 0 modules in the cnn[0m
[33m01/24 13:49:07 Top Down : OrderedDict([('embed_1', Embedding(9488, 512)), ('embed', Sequential (
  (0): Embedding(9488, 512)
  (1): ReLU ()
  (2): Dropout (p = 0.5)
)), ('att_embed', Sequential (
  (0): Linear (2048 -> 512)
  (1): ReLU ()
  (2): Dropout (p = 0.0)
)), ('logit', Linear (512 -> 9488)), ('ctx2att', Linear (512 -> 512)), ('fc_embed', Sequential (
  (0): Linear (2048 -> 512)
  (1): ReLU ()
  (2): Dropout (p = 0.0)
)), ('core', TopDownCore (
  (att_lstm): LSTMCell(1536, 512)
  (lang_lstm): LSTMCell(1024, 512)
  (attention): Attention (
    (h2att): Linear (512 -> 512)
    (alpha_net): Linear (512 -> 1)
  )
))])[0m
[33m01/24 13:49:15 Loading the model dict (last checkpoint) ['embed_1.weight', 'embed.0.weight', 'att_embed.0.weight', 'att_embed.0.bias', 'logit.weight', 'logit.bias', 'ctx2att.weight', 'ctx2att.bias', 'fc_embed.0.weight', 'fc_embed.0.bias', 'core.att_lstm.weight_ih', 'core.att_lstm.weight_hh', 'core.att_lstm.bias_ih', 'core.att_lstm.bias_hh', 'core.lang_lstm.weight_ih', 'core.lang_lstm.weight_hh', 'core.lang_lstm.bias_ih', 'core.lang_lstm.bias_hh', 'core.attention.h2att.weight', 'core.attention.h2att.bias', 'core.attention.alpha_net.weight', 'core.attention.alpha_net.bias'][0m
[33m01/24 13:49:15 DataLoader loading json file: data/coco/cocotalk.json[0m
[33m01/24 13:49:16 vocab size is 9487 [0m
[33m01/24 13:49:16 DataLoader loading h5 file: data/coco/cocotalk.h5[0m
[33m01/24 13:49:16 read 123287 images of size 3x256x256[0m
[33m01/24 13:49:16 max sequence length in data is 16[0m
[33m01/24 13:49:16 assigned 82783 images to split train[0m
[33m01/24 13:49:16 assigned 5000 images to split val[0m
[33m01/24 13:49:16 assigned 5000 images to split test[0m
[32m01/24 13:49:16 RewardSampler (stratified sampling), r=Hamming (Vpool=2, tau=0.30)[0m
[33m01/24 13:49:16 Evaluating the test split (-1)[0m
[33m01/24 13:51:06 GPU ID: 0 | available memory: 11172M[0m
[33m01/24 13:51:10 Starting from the best saved model[0m
[33m01/24 13:51:10 Starting from save/importance_qhamming_limited2_tsent03_rcider_tsent05_a04_topdown/model-best.pth[0m
[36m01/24 13:51:15 Setting CNN weigths from save/importance_qhamming_limited2_tsent03_rcider_tsent05_a04_topdown/model-cnn-best.pth[0m
[33m01/24 13:51:19 Finetuning up from 0 modules in the cnn[0m
[33m01/24 13:51:19 Top Down : OrderedDict([('embed_1', Embedding(9488, 512)), ('embed', Sequential (
  (0): Embedding(9488, 512)
  (1): ReLU ()
  (2): Dropout (p = 0.5)
)), ('att_embed', Sequential (
  (0): Linear (2048 -> 512)
  (1): ReLU ()
  (2): Dropout (p = 0.0)
)), ('logit', Linear (512 -> 9488)), ('ctx2att', Linear (512 -> 512)), ('fc_embed', Sequential (
  (0): Linear (2048 -> 512)
  (1): ReLU ()
  (2): Dropout (p = 0.0)
)), ('core', TopDownCore (
  (att_lstm): LSTMCell(1536, 512)
  (lang_lstm): LSTMCell(1024, 512)
  (attention): Attention (
    (h2att): Linear (512 -> 512)
    (alpha_net): Linear (512 -> 1)
  )
))])[0m
[33m01/24 13:51:20 Loading the model dict (last checkpoint) ['embed_1.weight', 'embed.0.weight', 'att_embed.0.weight', 'att_embed.0.bias', 'logit.weight', 'logit.bias', 'ctx2att.weight', 'ctx2att.bias', 'fc_embed.0.weight', 'fc_embed.0.bias', 'core.att_lstm.weight_ih', 'core.att_lstm.weight_hh', 'core.att_lstm.bias_ih', 'core.att_lstm.bias_hh', 'core.lang_lstm.weight_ih', 'core.lang_lstm.weight_hh', 'core.lang_lstm.bias_ih', 'core.lang_lstm.bias_hh', 'core.attention.h2att.weight', 'core.attention.h2att.bias', 'core.attention.alpha_net.weight', 'core.attention.alpha_net.bias'][0m
[33m01/24 13:51:20 DataLoader loading json file: data/coco/cocotalk.json[0m
[33m01/24 13:51:20 vocab size is 9487 [0m
[33m01/24 13:51:20 DataLoader loading h5 file: data/coco/cocotalk.h5[0m
[33m01/24 13:51:20 read 123287 images of size 3x256x256[0m
[33m01/24 13:51:20 max sequence length in data is 16[0m
[33m01/24 13:51:20 assigned 82783 images to split train[0m
[33m01/24 13:51:20 assigned 5000 images to split val[0m
[33m01/24 13:51:20 assigned 5000 images to split test[0m
[32m01/24 13:51:24 using importance sampling r=CIDEr (tau=0.50) and q=Hamming (Vpool=2, tau=0.30)[0m
[33m01/24 13:51:24 Evaluating the test split (-1)[0m
[33m01/24 13:55:47 Loading reference captions..[0m
[33m01/24 13:55:51 Reference captions loaded![0m
[33m01/24 13:55:53 using 5000/5000 predictions[0m
[33m01/24 13:55:53 Loading model captions[0m
[33m01/24 13:55:53 Model captions loaded[0m
[33m01/24 13:55:53 Starting evaluation...[0m
PTBTokenizer tokenized 307085 tokens at 1223349.61 tokens per second.
PTBTokenizer tokenized 51813 tokens at 340422.93 tokens per second.
[33m01/24 13:56:56 Loading reference captions..[0m
[33m01/24 13:56:58 Reference captions loaded![0m
[33m01/24 13:57:00 using 5000/5000 predictions[0m
[33m01/24 13:57:00 Loading model captions[0m
[33m01/24 13:57:00 Model captions loaded[0m
[33m01/24 13:57:00 Starting evaluation...[0m
PTBTokenizer tokenized 307085 tokens at 392781,31 tokens per second.
PTBTokenizer tokenized 51616 tokens at 162317,83 tokens per second.
[33m01/24 14:08:57 GPU ID: 0 | available memory: 12207M[0m
[33m01/24 14:09:34 Starting from the best saved model[0m
[33m01/24 14:09:34 Starting from save/fncnn6_sample2_hamming_tsent03_limited2_a04_topdown_resnet152/model-best.pth[0m
[36m01/24 14:09:40 Setting CNN weigths from save/fncnn6_sample2_hamming_tsent03_limited2_a04_topdown_resnet152/model-cnn-best.pth[0m
[33m01/24 14:10:25 Finetuning up from 6 modules in the cnn[0m
[33m01/24 14:10:25 Top Down : OrderedDict([('embed_1', Embedding(9488, 512)), ('embed', Sequential (
  (0): Embedding(9488, 512)
  (1): ReLU ()
  (2): Dropout (p = 0.5)
)), ('att_embed', Sequential (
  (0): Linear (2048 -> 512)
  (1): ReLU ()
  (2): Dropout (p = 0.0)
)), ('logit', Linear (512 -> 9488)), ('ctx2att', Linear (512 -> 512)), ('fc_embed', Sequential (
  (0): Linear (2048 -> 512)
  (1): ReLU ()
  (2): Dropout (p = 0.0)
)), ('core', TopDownCore (
  (att_lstm): LSTMCell(1536, 512)
  (lang_lstm): LSTMCell(1024, 512)
  (attention): Attention (
    (h2att): Linear (512 -> 512)
    (alpha_net): Linear (512 -> 1)
  )
))])[0m
[33m01/24 14:10:34 Loading the model dict (last checkpoint) ['embed_1.weight', 'embed.0.weight', 'att_embed.0.weight', 'att_embed.0.bias', 'logit.weight', 'logit.bias', 'ctx2att.weight', 'ctx2att.bias', 'fc_embed.0.weight', 'fc_embed.0.bias', 'core.att_lstm.weight_ih', 'core.att_lstm.weight_hh', 'core.att_lstm.bias_ih', 'core.att_lstm.bias_hh', 'core.lang_lstm.weight_ih', 'core.lang_lstm.weight_hh', 'core.lang_lstm.bias_ih', 'core.lang_lstm.bias_hh', 'core.attention.h2att.weight', 'core.attention.h2att.bias', 'core.attention.alpha_net.weight', 'core.attention.alpha_net.bias'][0m
[33m01/24 14:10:34 DataLoader loading json file: data/coco/cocotalk.json[0m
[33m01/24 14:10:35 vocab size is 9487 [0m
[33m01/24 14:10:35 DataLoader loading h5 file: data/coco/cocotalk.h5[0m
[33m01/24 14:10:35 read 123287 images of size 3x256x256[0m
[33m01/24 14:10:35 max sequence length in data is 16[0m
[33m01/24 14:10:35 assigned 82783 images to split train[0m
[33m01/24 14:10:35 assigned 5000 images to split val[0m
[33m01/24 14:10:35 assigned 5000 images to split test[0m
[32m01/24 14:10:35 RewardSampler (stratified sampling), r=Hamming (Vpool=2, tau=0.30)[0m
[33m01/24 14:10:35 Evaluating the test split (-1)[0m
[33m01/24 14:17:26 Loading reference captions..[0m
[33m01/24 14:17:30 Reference captions loaded![0m
[33m01/24 14:17:32 using 5000/5000 predictions[0m
[33m01/24 14:17:32 Loading model captions[0m
[33m01/24 14:17:32 Model captions loaded[0m
[33m01/24 14:17:32 Starting evaluation...[0m
PTBTokenizer tokenized 307085 tokens at 980423.74 tokens per second.
PTBTokenizer tokenized 51776 tokens at 341933.87 tokens per second.
[33m01/24 17:05:14 GPU ID: 1 | available memory: 16276M[0m
[33m01/24 17:05:39 Starting from the best saved model[0m
[33m01/24 17:05:39 Starting from save/word_coco_tword009_idf10_a03_topdown/model-best.pth[0m
[36m01/24 17:05:43 Setting CNN weigths from save/word_coco_tword009_idf10_a03_topdown/model-cnn-best.pth[0m
## OAR [2018-01-24 17:05:45] Job 138209 KILLED ##
[33m01/25 03:50:33 GPU ID: 1 | available memory: 11172M[0m
[33m01/25 03:50:57 Starting from the best saved model[0m
[33m01/25 03:50:57 Starting from save/fncnn6_strat_rhamming_pool1_tsent017_a04_topdown/model-best.pth[0m
[36m01/25 03:51:02 Setting CNN weigths from save/fncnn6_strat_rhamming_pool1_tsent017_a04_topdown/model-cnn-best.pth[0m
[33m01/25 03:51:10 Finetuning up from 6 modules in the cnn[0m
[33m01/25 03:51:11 Top Down : OrderedDict([('embed_1', Embedding(9488, 512)), ('embed', Sequential (
  (0): Embedding(9488, 512)
  (1): ReLU ()
  (2): Dropout (p = 0.5)
)), ('att_embed', Sequential (
  (0): Linear (2048 -> 512)
  (1): ReLU ()
  (2): Dropout (p = 0.0)
)), ('logit', Linear (512 -> 9488)), ('ctx2att', Linear (512 -> 512)), ('fc_embed', Sequential (
  (0): Linear (2048 -> 512)
  (1): ReLU ()
  (2): Dropout (p = 0.0)
)), ('core', TopDownCore (
  (att_lstm): LSTMCell(1536, 512)
  (lang_lstm): LSTMCell(1024, 512)
  (attention): Attention (
    (h2att): Linear (512 -> 512)
    (alpha_net): Linear (512 -> 1)
  )
))])[0m
[33m01/25 03:51:12 Loading the model dict (last checkpoint) ['embed_1.weight', 'embed.0.weight', 'att_embed.0.weight', 'att_embed.0.bias', 'logit.weight', 'logit.bias', 'ctx2att.weight', 'ctx2att.bias', 'fc_embed.0.weight', 'fc_embed.0.bias', 'core.att_lstm.weight_ih', 'core.att_lstm.weight_hh', 'core.att_lstm.bias_ih', 'core.att_lstm.bias_hh', 'core.lang_lstm.weight_ih', 'core.lang_lstm.weight_hh', 'core.lang_lstm.bias_ih', 'core.lang_lstm.bias_hh', 'core.attention.h2att.weight', 'core.attention.h2att.bias', 'core.attention.alpha_net.weight', 'core.attention.alpha_net.bias'][0m
[33m01/25 03:51:12 DataLoader loading json file: data/coco/cocotalk.json[0m
[33m01/25 03:51:12 vocab size is 9487 [0m
[33m01/25 03:51:12 DataLoader loading h5 file: data/coco/cocotalk.h5[0m
[33m01/25 03:51:12 read 123287 images of size 3x256x256[0m
[33m01/25 03:51:12 max sequence length in data is 16[0m
[33m01/25 03:51:12 assigned 82783 images to split train[0m
[33m01/25 03:51:12 assigned 5000 images to split val[0m
[33m01/25 03:51:12 assigned 5000 images to split test[0m
[32m01/25 03:51:12 RewardSampler (stratified sampling), r=Hamming (Vpool=1, tau=0.17)[0m
[33m01/25 03:51:12 Evaluating the test split (-1)[0m
[33m01/25 03:57:41 Loading reference captions..[0m
[33m01/25 03:57:41 Reference captions loaded![0m
[33m01/25 03:57:44 using 5000/5000 predictions[0m
[33m01/25 03:57:44 Loading model captions[0m
[33m01/25 03:57:44 Model captions loaded[0m
[33m01/25 03:57:44 Starting evaluation...[0m
PTBTokenizer tokenized 307085 tokens at 888213.99 tokens per second.
PTBTokenizer tokenized 51824 tokens at 406516.04 tokens per second.
[33m01/25 04:17:26 GPU ID: 0 | available memory: 11143M[0m
[33m01/25 04:17:53 Starting from the best saved model[0m
[33m01/25 04:17:53 Starting from save/word_coco_tword009_idf10_a03_topdown/model-best.pth[0m
[36m01/25 04:17:57 Setting CNN weigths from save/word_coco_tword009_idf10_a03_topdown/model-cnn-best.pth[0m
[33m01/25 04:18:09 Finetuning up from 0 modules in the cnn[0m
[33m01/25 04:18:09 Top Down : OrderedDict([('embed_1', Embedding(9488, 512)), ('embed', Sequential (
  (0): Embedding(9488, 512)
  (1): ReLU ()
  (2): Dropout (p = 0.5)
)), ('att_embed', Sequential (
  (0): Linear (2048 -> 512)
  (1): ReLU ()
  (2): Dropout (p = 0.0)
)), ('logit', Linear (512 -> 9488)), ('ctx2att', Linear (512 -> 512)), ('fc_embed', Sequential (
  (0): Linear (2048 -> 512)
  (1): ReLU ()
  (2): Dropout (p = 0.0)
)), ('core', TopDownCore (
  (att_lstm): LSTMCell(1536, 512)
  (lang_lstm): LSTMCell(1024, 512)
  (attention): Attention (
    (h2att): Linear (512 -> 512)
    (alpha_net): Linear (512 -> 1)
  )
))])[0m
[33m01/25 04:18:11 Loading the model dict (last checkpoint) ['embed_1.weight', 'embed.0.weight', 'att_embed.0.weight', 'att_embed.0.bias', 'logit.weight', 'logit.bias', 'ctx2att.weight', 'ctx2att.bias', 'fc_embed.0.weight', 'fc_embed.0.bias', 'core.att_lstm.weight_ih', 'core.att_lstm.weight_hh', 'core.att_lstm.bias_ih', 'core.att_lstm.bias_hh', 'core.lang_lstm.weight_ih', 'core.lang_lstm.weight_hh', 'core.lang_lstm.bias_ih', 'core.lang_lstm.bias_hh', 'core.attention.h2att.weight', 'core.attention.h2att.bias', 'core.attention.alpha_net.weight', 'core.attention.alpha_net.bias'][0m
[33m01/25 04:18:11 DataLoader loading json file: data/coco/cocotalk.json[0m
[33m01/25 04:18:11 vocab size is 9487 [0m
[33m01/25 04:18:11 DataLoader loading h5 file: data/coco/cocotalk.h5[0m
[33m01/25 04:18:11 read 123287 images of size 3x256x256[0m
[33m01/25 04:18:11 max sequence length in data is 16[0m
[33m01/25 04:18:11 assigned 82783 images to split train[0m
[33m01/25 04:18:11 assigned 5000 images to split val[0m
[33m01/25 04:18:11 assigned 5000 images to split test[0m
[32m01/25 04:18:32 Initialized Word2 loss tau=0.090, alpha=0.3[0m
[33m01/25 04:18:32 Evaluating the test split (-1)[0m
## OAR [2018-01-25 04:19:38] Job 138260 KILLED ##
[33m01/25 04:40:14 GPU ID: 0 | available memory: 11143M[0m
[33m01/25 04:40:18 Starting from the best saved model[0m
[33m01/25 04:40:18 Starting from save/word_coco_tword009_idf10_a03_topdown/model-best.pth[0m
[36m01/25 04:40:22 Setting CNN weigths from save/word_coco_tword009_idf10_a03_topdown/model-cnn-best.pth[0m
[33m01/25 04:40:25 Finetuning up from 0 modules in the cnn[0m
[33m01/25 04:40:25 Top Down : OrderedDict([('embed_1', Embedding(9488, 512)), ('embed', Sequential (
  (0): Embedding(9488, 512)
  (1): ReLU ()
  (2): Dropout (p = 0.5)
)), ('att_embed', Sequential (
  (0): Linear (2048 -> 512)
  (1): ReLU ()
  (2): Dropout (p = 0.0)
)), ('logit', Linear (512 -> 9488)), ('ctx2att', Linear (512 -> 512)), ('fc_embed', Sequential (
  (0): Linear (2048 -> 512)
  (1): ReLU ()
  (2): Dropout (p = 0.0)
)), ('core', TopDownCore (
  (att_lstm): LSTMCell(1536, 512)
  (lang_lstm): LSTMCell(1024, 512)
  (attention): Attention (
    (h2att): Linear (512 -> 512)
    (alpha_net): Linear (512 -> 1)
  )
))])[0m
[33m01/25 04:40:25 Loading the model dict (last checkpoint) ['embed_1.weight', 'embed.0.weight', 'att_embed.0.weight', 'att_embed.0.bias', 'logit.weight', 'logit.bias', 'ctx2att.weight', 'ctx2att.bias', 'fc_embed.0.weight', 'fc_embed.0.bias', 'core.att_lstm.weight_ih', 'core.att_lstm.weight_hh', 'core.att_lstm.bias_ih', 'core.att_lstm.bias_hh', 'core.lang_lstm.weight_ih', 'core.lang_lstm.weight_hh', 'core.lang_lstm.bias_ih', 'core.lang_lstm.bias_hh', 'core.attention.h2att.weight', 'core.attention.h2att.bias', 'core.attention.alpha_net.weight', 'core.attention.alpha_net.bias'][0m
[33m01/25 04:40:26 DataLoader loading json file: data/coco/cocotalk.json[0m
[33m01/25 04:40:26 vocab size is 9487 [0m
[33m01/25 04:40:26 DataLoader loading h5 file: data/coco/cocotalk.h5[0m
[33m01/25 04:40:26 read 123287 images of size 3x256x256[0m
[33m01/25 04:40:26 max sequence length in data is 16[0m
[33m01/25 04:40:26 assigned 82783 images to split train[0m
[33m01/25 04:40:26 assigned 5000 images to split val[0m
[33m01/25 04:40:26 assigned 5000 images to split test[0m
[32m01/25 04:40:33 Initialized Word2 loss tau=0.090, alpha=0.3[0m
[33m01/25 04:40:33 Evaluating the test split (-1)[0m
[33m01/25 04:46:12 Loading reference captions..[0m
[33m01/25 04:46:14 Reference captions loaded![0m
[33m01/25 04:46:15 using 5000/5000 predictions[0m
[33m01/25 04:46:16 Loading model captions[0m
[33m01/25 04:46:16 Model captions loaded[0m
[33m01/25 04:46:16 Starting evaluation...[0m
PTBTokenizer tokenized 307085 tokens at 720018.06 tokens per second.
PTBTokenizer tokenized 51226 tokens at 302618.13 tokens per second.
[33m01/25 17:29:04 GPU ID: 0 | available memory: 3760M[0m
## OAR [2018-01-25 17:30:03] Job 138933 KILLED ##
usage: eval.py [-h] [--start_from_best START_FROM_BEST]
               [--start_from START_FROM] [-c CONFIG] [--modelname MODELNAME]
               [--caption_model CAPTION_MODEL] [--seed SEED]
               [--verbose VERBOSE] [--gpu_id GPU_ID] [--input_data INPUT_DATA]
               [--train_only TRAIN_ONLY] [--upsampling_size UPSAMPLING_SIZE]
               [--batch_size BATCH_SIZE] [--seq_per_img SEQ_PER_IMG]
               [--fliplr FLIPLR] [--cnn_model CNN_MODEL]
               [--cnn_fc_feat CNN_FC_FEAT] [--cnn_att_feat CNN_ATT_FEAT]
               [--cnn_weight CNN_WEIGHT] [--pretrained_cnn PRETRAINED_CNN]
               [--fc_feat_size FC_FEAT_SIZE] [--norm_feat NORM_FEAT]
               [--rnn_size RNN_SIZE] [--rnn_bias RNN_BIAS]
               [--num_layers NUM_LAYERS] [--rnn_type RNN_TYPE]
               [--input_encoding_size INPUT_ENCODING_SIZE]
               [--init_decoder_W INIT_DECODER_W]
               [--freeze_decoder_W FREEZE_DECODER_W] [--drop_x_lm DROP_X_LM]
               [--drop_prob_lm DROP_PROB_LM] [--drop_feat_im DROP_FEAT_IM]
               [--drop_sentinel DROP_SENTINEL] [--attend_mode ATTEND_MODE]
               [--region_size REGION_SIZE] [--att_feat_size ATT_FEAT_SIZE]
               [--att_hid_size ATT_HID_SIZE]
               [--use_adaptive_pooling USE_ADAPTIVE_POOLING]
               [--use_maxout USE_MAXOUT] [--add_fc_img ADD_FC_IMG]
               [--sample_max SAMPLE_MAX] [--forbid_unk FORBID_UNK]
               [--beam_size BEAM_SIZE] [--temperature TEMPERATURE]
               [--val_images_use VAL_IMAGES_USE]
               [--save_checkpoint_every SAVE_CHECKPOINT_EVERY]
               [--language_creativity LANGUAGE_CREATIVITY]
               [--language_eval LANGUAGE_EVAL]
               [--losses_log_every LOSSES_LOG_EVERY]
               [--load_best_score LOAD_BEST_SCORE] [--add_dirac ADD_DIRAC]
               [--save_stats SAVE_STATS]
               [--scheduled_sampling_start SCHEDULED_SAMPLING_START]
               [--scheduled_sampling_vocab SCHEDULED_SAMPLING_VOCAB]
               [--scheduled_sampling_speed SCHEDULED_SAMPLING_SPEED]
               [--scheduled_sampling_increase_every SCHEDULED_SAMPLING_INCREASE_EVERY]
               [--scheduled_sampling_increase_prob SCHEDULED_SAMPLING_INCREASE_PROB]
               [--scheduled_sampling_max_prob SCHEDULED_SAMPLING_MAX_PROB]
               [--scheduled_sampling_strategy SCHEDULED_SAMPLING_STRATEGY]
               [--match_pairs MATCH_PAIRS] [--restart RESTART]
               [--max_epochs MAX_EPOCHS] [--grad_clip GRAD_CLIP]
               [--finetune_cnn_after FINETUNE_CNN_AFTER]
               [--finetune_cnn_only FINETUNE_CNN_ONLY]
               [--finetune_cnn_slice FINETUNE_CNN_SLICE] [--optim OPTIM]
               [--optim_alpha OPTIM_ALPHA] [--optim_beta OPTIM_BETA]
               [--optim_epsilon OPTIM_EPSILON] [--weight_decay WEIGHT_DECAY]
               [--learning_rate LEARNING_RATE]
               [--learning_rate_decay_start LEARNING_RATE_DECAY_START]
               [--lr_patience LR_PATIENCE] [--lr_strategy LR_STRATEGY]
               [--learning_rate_decay_every LEARNING_RATE_DECAY_EVERY]
               [--learning_rate_decay_rate LEARNING_RATE_DECAY_RATE]
               [--cnn_optim CNN_OPTIM] [--cnn_optim_alpha CNN_OPTIM_ALPHA]
               [--cnn_optim_beta CNN_OPTIM_BETA]
               [--cnn_weight_decay CNN_WEIGHT_DECAY]
               [--cnn_learning_rate CNN_LEARNING_RATE]
               [--reset_optimizer RESET_OPTIMIZER] [--bootstrap BOOTSTRAP]
               [--bootstrap_score BOOTSTRAP_SCORE]
               [--gt_loss_version GT_LOSS_VERSION]
               [--augmented_loss_version AUGMENTED_LOSS_VERSION]
               [--alter_loss ALTER_LOSS] [--alter_mode ALTER_MODE]
               [--sum_loss SUM_LOSS] [--beta BETA] [--gamma GAMMA]
               [--combine_loss COMBINE_LOSS] [--loss_version LOSS_VERSION]
               [--normalize_batch NORMALIZE_BATCH]
               [--penalize_confidence PENALIZE_CONFIDENCE]
               [--scale_loss SCALE_LOSS]
               [--similarity_matrix SIMILARITY_MATRIX] [--use_cooc USE_COOC]
               [--margin_sim MARGIN_SIM]
               [--limited_vocab_sim LIMITED_VOCAB_SIM]
               [--rare_tfidf RARE_TFIDF] [--alpha_word ALPHA_WORD]
               [--tau_word TAU_WORD] [--lazy_rnn LAZY_RNN]
               [--mc_samples MC_SAMPLES] [--reward REWARD]
               [--stratify_reward STRATIFY_REWARD]
               [--importance_sampler IMPORTANCE_SAMPLER]
               [--alpha_sent ALPHA_SENT] [--tau_sent TAU_SENT]
               [--tau_sent_q TAU_SENT_Q] [--clip_reward CLIP_REWARD]
               [--cider_df CIDER_DF] [--limited_vocab_sub LIMITED_VOCAB_SUB]
               [--sub_idf SUB_IDF] [--ngram_length NGRAM_LENGTH]
               [--alpha_increase_every ALPHA_INCREASE_EVERY]
               [--alpha_increase_factor ALPHA_INCREASE_FACTOR]
               [--alpha_max ALPHA_MAX]
               [--alpha_increase_start ALPHA_INCREASE_START]
               [--alpha_speed ALPHA_SPEED] [--alpha_strategy ALPHA_STRATEGY]
               [--num_images NUM_IMAGES] [--dump_images DUMP_IMAGES]
               [--dump_json DUMP_JSON] [--output OUTPUT]
               [--dump_path DUMP_PATH] [--image_folder IMAGE_FOLDER]
               [--image_list IMAGE_LIST] [--max_images MAX_IMAGES]
               [--image_root IMAGE_ROOT] [--input_h5 INPUT_H5]
               [--input_json INPUT_JSON] [--split SPLIT]
               [--fliplr_eval FLIPLR_EVAL]
eval.py: error: File not found: config/-c.yaml
[33m01/26 08:25:20 GPU ID: 4 | available memory: 16276M[0m
[33m01/26 08:25:27 Starting from the best saved model[0m
[33m01/26 08:25:27 Starting from save/fncnn6_reset_baseline_GloveW_coco_free_showtell152/model-best.pth[0m
[36m01/26 08:25:32 Setting CNN weigths from save/fncnn6_reset_baseline_GloveW_coco_free_showtell152/model-cnn-best.pth[0m
[33m01/26 08:25:35 Finetuning up from 6 modules in the cnn[0m
[32m01/26 08:25:35 Loading weights to initialize W[0m
[33m01/26 08:25:36 Show & Tell : OrderedDict([('img_embed', Linear (2048 -> 512)), ('core', LSTM(512, 512, bias=False, dropout=0.5)), ('embed', Embedding(9488, 512)), ('drop_x_lm', Dropout (p = 0.5)), ('logit', Linear (512 -> 9488))])[0m
[33m01/26 08:25:36 Loading the model dict (last checkpoint) ['img_embed.weight', 'img_embed.bias', 'core.weight_ih_l0', 'core.weight_hh_l0', 'embed.weight', 'logit.weight', 'logit.bias'][0m
[33m01/26 08:25:37 DataLoader loading json file: data/coco/cocotalk.json[0m
[33m01/26 08:25:37 vocab size is 9487 [0m
[33m01/26 08:25:37 DataLoader loading h5 file: data/coco/cocotalk.h5[0m
[33m01/26 08:25:37 read 123287 images of size 3x256x256[0m
[33m01/26 08:25:37 max sequence length in data is 16[0m
[33m01/26 08:25:37 assigned 82783 images to split train[0m
[33m01/26 08:25:37 assigned 5000 images to split val[0m
[33m01/26 08:25:37 assigned 5000 images to split test[0m
[32m01/26 08:25:37 Default ML loss[0m
[33m01/26 08:25:37 Evaluating the test split (-1)[0m
[33m01/26 08:26:15 GPU ID: 0 | available memory: 12207M[0m
[33m01/26 08:26:53 Starting from the best saved model[0m
[33m01/26 08:26:53 Starting from save/fncnn6_reset_baseline_GloveW_coco_frozen_showtell152/model-best.pth[0m
[36m01/26 08:26:57 Setting CNN weigths from save/fncnn6_reset_baseline_GloveW_coco_frozen_showtell152/model-cnn-best.pth[0m
[33m01/26 08:27:35 Finetuning up from 6 modules in the cnn[0m
[32m01/26 08:27:35 Loading weights to initialize W[0m
[33m01/26 08:27:36 Show & Tell : OrderedDict([('img_embed', Linear (2048 -> 512)), ('core', LSTM(512, 512, bias=False, dropout=0.5)), ('embed', Embedding(9488, 512)), ('drop_x_lm', Dropout (p = 0.5)), ('logit', Linear (512 -> 9488))])[0m
[33m01/26 08:27:42 Loading the model dict (last checkpoint) ['img_embed.weight', 'img_embed.bias', 'core.weight_ih_l0', 'core.weight_hh_l0', 'embed.weight', 'logit.weight', 'logit.bias'][0m
[33m01/26 08:27:46 DataLoader loading json file: data/coco/cocotalk.json[0m
[33m01/26 08:27:46 vocab size is 9487 [0m
[33m01/26 08:27:46 DataLoader loading h5 file: data/coco/cocotalk.h5[0m
[33m01/26 08:27:46 read 123287 images of size 3x256x256[0m
[33m01/26 08:27:46 max sequence length in data is 16[0m
[33m01/26 08:27:47 assigned 82783 images to split train[0m
[33m01/26 08:27:47 assigned 5000 images to split val[0m
[33m01/26 08:27:47 assigned 5000 images to split test[0m
[32m01/26 08:27:47 Default ML loss[0m
[33m01/26 08:27:47 Evaluating the test split (-1)[0m
[33m01/26 08:29:52 Loading reference captions..[0m
[33m01/26 08:29:53 Reference captions loaded![0m
[33m01/26 08:29:55 using 5000/5000 predictions[0m
[33m01/26 08:29:55 Loading model captions[0m
[33m01/26 08:29:55 Model captions loaded[0m
[33m01/26 08:29:55 Starting evaluation...[0m
PTBTokenizer tokenized 307085 tokens at 1235114.19 tokens per second.
PTBTokenizer tokenized 51857 tokens at 367946.73 tokens per second.
Parsing reference captions
Error: Could not score batched file input:
org.fusesource.lmdbjni.LMDBException: MDB_CURSOR_FULL: Internal error - cursor stack limit reached
	at org.fusesource.lmdbjni.Util.checkErrorCode(Util.java:44)
	at org.fusesource.lmdbjni.Database.get(Database.java:202)
	at org.fusesource.lmdbjni.Database.get(Database.java:193)
	at org.fusesource.lmdbjni.Database.get(Database.java:186)
	at org.fusesource.lmdbjni.Database.get(Database.java:161)
	at edu.anu.spice.LmdbTupleDB.getTransaction(LmdbTupleDB.java:96)
	at edu.anu.spice.SpiceParser.loadTuplesFromDB(SpiceParser.java:195)
	at edu.anu.spice.SpiceParser.loadTuples(SpiceParser.java:245)
	at edu.anu.spice.SpiceParser.parseCaptions(SpiceParser.java:251)
	at edu.anu.spice.SpiceScorer.scoreBatch(SpiceScorer.java:109)
	at edu.anu.spice.SpiceScorer.main(SpiceScorer.java:60)
[33m01/26 08:33:06 GPU ID: 1 | available memory: 8114M[0m
[33m01/26 08:33:23 Starting from the best saved model[0m
[33m01/26 08:33:23 Starting from save/fncnn6_reset_baseline_resnet152/model-best.pth[0m
[36m01/26 08:33:29 Setting CNN weigths from save/fncnn6_reset_baseline_resnet152/model-cnn-best.pth[0m
[33m01/26 08:33:37 Finetuning up from 6 modules in the cnn[0m
[33m01/26 08:33:38 Show & Tell : OrderedDict([('img_embed', Linear (2048 -> 512)), ('core', LSTM(512, 512, bias=False, dropout=0.5)), ('embed', Embedding(9488, 512)), ('drop_x_lm', Dropout (p = 0.5)), ('logit', Linear (512 -> 9488))])[0m
[33m01/26 08:33:39 Loading the model dict (last checkpoint) ['img_embed.weight', 'img_embed.bias', 'core.weight_ih_l0', 'core.weight_hh_l0', 'embed.weight', 'logit.weight', 'logit.bias'][0m
[33m01/26 08:33:40 DataLoader loading json file: data/coco/cocotalk.json[0m
[33m01/26 08:33:40 vocab size is 9487 [0m
[33m01/26 08:33:40 DataLoader loading h5 file: data/coco/cocotalk.h5[0m
[33m01/26 08:33:40 read 123287 images of size 3x256x256[0m
[33m01/26 08:33:40 max sequence length in data is 16[0m
[33m01/26 08:33:40 assigned 82783 images to split train[0m
[33m01/26 08:33:40 assigned 5000 images to split val[0m
[33m01/26 08:33:40 assigned 5000 images to split test[0m
[32m01/26 08:33:40 Default ML loss[0m
[33m01/26 08:33:40 Evaluating the test split (-1)[0m
