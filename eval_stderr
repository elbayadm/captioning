[33m01/27 11:01:29 GPU ID: 2 | available memory: 16276M[0m
[33m01/27 11:01:38 Starting from the best saved model[0m
[33m01/27 11:01:38 Starting from save/fncnn6_reset_word_coco_tword009_idf10_a03_topdown/model-best.pth[0m
[36m01/27 11:01:43 Setting CNN weigths from save/fncnn6_reset_word_coco_tword009_idf10_a03_topdown/model-cnn-best.pth[0m
[33m01/27 11:01:46 Finetuning up from 6 modules in the cnn[0m
[33m01/27 11:01:47 Top Down : OrderedDict([('embed_1', Embedding(9488, 512)), ('embed', Sequential (
  (0): Embedding(9488, 512)
  (1): ReLU ()
  (2): Dropout (p = 0.5)
)), ('att_embed', Sequential (
  (0): Linear (2048 -> 512)
  (1): ReLU ()
  (2): Dropout (p = 0.0)
)), ('logit', Linear (512 -> 9488)), ('ctx2att', Linear (512 -> 512)), ('fc_embed', Sequential (
  (0): Linear (2048 -> 512)
  (1): ReLU ()
  (2): Dropout (p = 0.0)
)), ('core', TopDownCore (
  (att_lstm): LSTMCell(1536, 512)
  (lang_lstm): LSTMCell(1024, 512)
  (attention): Attention (
    (h2att): Linear (512 -> 512)
    (alpha_net): Linear (512 -> 1)
  )
))])[0m
[33m01/27 11:01:47 Loading the model dict (last checkpoint) ['embed_1.weight', 'embed.0.weight', 'att_embed.0.weight', 'att_embed.0.bias', 'logit.weight', 'logit.bias', 'ctx2att.weight', 'ctx2att.bias', 'fc_embed.0.weight', 'fc_embed.0.bias', 'core.att_lstm.weight_ih', 'core.att_lstm.weight_hh', 'core.att_lstm.bias_ih', 'core.att_lstm.bias_hh', 'core.lang_lstm.weight_ih', 'core.lang_lstm.weight_hh', 'core.lang_lstm.bias_ih', 'core.lang_lstm.bias_hh', 'core.attention.h2att.weight', 'core.attention.h2att.bias', 'core.attention.alpha_net.weight', 'core.attention.alpha_net.bias'][0m
[33m01/27 11:01:47 DataLoader loading json file: data/coco/cocotalk.json[0m
[33m01/27 11:01:47 vocab size is 9487 [0m
[33m01/27 11:01:47 DataLoader loading h5 file: data/coco/cocotalk.h5[0m
[33m01/27 11:01:47 read 123287 images of size 3x256x256[0m
[33m01/27 11:01:47 max sequence length in data is 16[0m
[33m01/27 11:01:47 assigned 82783 images to split train[0m
[33m01/27 11:01:47 assigned 5000 images to split val[0m
[33m01/27 11:01:47 assigned 5000 images to split test[0m
[32m01/27 11:01:50 Initialized Word2 loss tau=0.090, alpha=0.3[0m
[33m01/27 11:01:50 Evaluating the test split (-1)[0m
## OAR [2018-01-27 11:05:04] Job 139640 KILLED ##
[33m01/27 11:05:31 GPU ID: 2 | available memory: 16276M[0m
[33m01/27 11:05:41 Starting from the best saved model[0m
[33m01/27 11:05:41 Starting from save/fncnn6_reset_word_coco_tword009_idf10_a03_topdown/model-best.pth[0m
[36m01/27 11:05:45 Setting CNN weigths from save/fncnn6_reset_word_coco_tword009_idf10_a03_topdown/model-cnn-best.pth[0m
[33m01/27 11:05:47 Finetuning up from 6 modules in the cnn[0m
[33m01/27 11:05:47 Top Down : OrderedDict([('embed_1', Embedding(9488, 512)), ('embed', Sequential (
  (0): Embedding(9488, 512)
  (1): ReLU ()
  (2): Dropout (p = 0.5)
)), ('att_embed', Sequential (
  (0): Linear (2048 -> 512)
  (1): ReLU ()
  (2): Dropout (p = 0.0)
)), ('logit', Linear (512 -> 9488)), ('ctx2att', Linear (512 -> 512)), ('fc_embed', Sequential (
  (0): Linear (2048 -> 512)
  (1): ReLU ()
  (2): Dropout (p = 0.0)
)), ('core', TopDownCore (
  (att_lstm): LSTMCell(1536, 512)
  (lang_lstm): LSTMCell(1024, 512)
  (attention): Attention (
    (h2att): Linear (512 -> 512)
    (alpha_net): Linear (512 -> 1)
  )
))])[0m
[33m01/27 11:05:47 Loading the model dict (last checkpoint) ['embed_1.weight', 'embed.0.weight', 'att_embed.0.weight', 'att_embed.0.bias', 'logit.weight', 'logit.bias', 'ctx2att.weight', 'ctx2att.bias', 'fc_embed.0.weight', 'fc_embed.0.bias', 'core.att_lstm.weight_ih', 'core.att_lstm.weight_hh', 'core.att_lstm.bias_ih', 'core.att_lstm.bias_hh', 'core.lang_lstm.weight_ih', 'core.lang_lstm.weight_hh', 'core.lang_lstm.bias_ih', 'core.lang_lstm.bias_hh', 'core.attention.h2att.weight', 'core.attention.h2att.bias', 'core.attention.alpha_net.weight', 'core.attention.alpha_net.bias'][0m
[33m01/27 11:05:48 DataLoader loading json file: data/coco/cocotalk.json[0m
[33m01/27 11:05:48 vocab size is 9487 [0m
[33m01/27 11:05:48 DataLoader loading h5 file: data/coco/cocotalk.h5[0m
[33m01/27 11:05:48 read 123287 images of size 3x256x256[0m
[33m01/27 11:05:48 max sequence length in data is 16[0m
[33m01/27 11:05:48 assigned 82783 images to split train[0m
[33m01/27 11:05:48 assigned 5000 images to split val[0m
[33m01/27 11:05:48 assigned 5000 images to split test[0m
[32m01/27 11:05:49 Initialized Word2 loss tau=0.090, alpha=0.3[0m
[33m01/27 11:05:49 Evaluating the test split (-1)[0m
## OAR [2018-01-27 11:10:03] Job 139641 KILLED ##
[33m01/27 11:10:30 GPU ID: 2 | available memory: 16276M[0m
[33m01/27 11:10:39 Starting from the best saved model[0m
[33m01/27 11:10:39 Starting from save/fncnn6_reset_word_coco_tword009_idf10_a03_topdown/model-best.pth[0m
[36m01/27 11:10:43 Setting CNN weigths from save/fncnn6_reset_word_coco_tword009_idf10_a03_topdown/model-cnn-best.pth[0m
[33m01/27 11:10:45 Finetuning up from 6 modules in the cnn[0m
[33m01/27 11:10:46 Top Down : OrderedDict([('embed_1', Embedding(9488, 512)), ('embed', Sequential (
  (0): Embedding(9488, 512)
  (1): ReLU ()
  (2): Dropout (p = 0.5)
)), ('att_embed', Sequential (
  (0): Linear (2048 -> 512)
  (1): ReLU ()
  (2): Dropout (p = 0.0)
)), ('logit', Linear (512 -> 9488)), ('ctx2att', Linear (512 -> 512)), ('fc_embed', Sequential (
  (0): Linear (2048 -> 512)
  (1): ReLU ()
  (2): Dropout (p = 0.0)
)), ('core', TopDownCore (
  (att_lstm): LSTMCell(1536, 512)
  (lang_lstm): LSTMCell(1024, 512)
  (attention): Attention (
    (h2att): Linear (512 -> 512)
    (alpha_net): Linear (512 -> 1)
  )
))])[0m
[33m01/27 11:10:46 Loading the model dict (last checkpoint) ['embed_1.weight', 'embed.0.weight', 'att_embed.0.weight', 'att_embed.0.bias', 'logit.weight', 'logit.bias', 'ctx2att.weight', 'ctx2att.bias', 'fc_embed.0.weight', 'fc_embed.0.bias', 'core.att_lstm.weight_ih', 'core.att_lstm.weight_hh', 'core.att_lstm.bias_ih', 'core.att_lstm.bias_hh', 'core.lang_lstm.weight_ih', 'core.lang_lstm.weight_hh', 'core.lang_lstm.bias_ih', 'core.lang_lstm.bias_hh', 'core.attention.h2att.weight', 'core.attention.h2att.bias', 'core.attention.alpha_net.weight', 'core.attention.alpha_net.bias'][0m
[33m01/27 11:10:46 DataLoader loading json file: data/coco/cocotalk.json[0m
[33m01/27 11:10:46 vocab size is 9487 [0m
[33m01/27 11:10:46 DataLoader loading h5 file: data/coco/cocotalk.h5[0m
[33m01/27 11:10:46 read 123287 images of size 3x256x256[0m
[33m01/27 11:10:46 max sequence length in data is 16[0m
[33m01/27 11:10:46 assigned 82783 images to split train[0m
[33m01/27 11:10:46 assigned 5000 images to split val[0m
[33m01/27 11:10:46 assigned 5000 images to split test[0m
[32m01/27 11:10:48 Initialized Word2 loss tau=0.090, alpha=0.3[0m
[33m01/27 11:10:48 Evaluating the test split (-1)[0m
## OAR [2018-01-27 11:15:04] Job 139642 KILLED ##
[33m01/27 11:15:30 GPU ID: 2 | available memory: 16276M[0m
[33m01/27 11:15:40 Starting from the best saved model[0m
[33m01/27 11:15:40 Starting from save/fncnn6_reset_word_coco_tword009_idf10_a03_topdown/model-best.pth[0m
[36m01/27 11:15:44 Setting CNN weigths from save/fncnn6_reset_word_coco_tword009_idf10_a03_topdown/model-cnn-best.pth[0m
[33m01/27 11:15:46 Finetuning up from 6 modules in the cnn[0m
[33m01/27 11:15:47 Top Down : OrderedDict([('embed_1', Embedding(9488, 512)), ('embed', Sequential (
  (0): Embedding(9488, 512)
  (1): ReLU ()
  (2): Dropout (p = 0.5)
)), ('att_embed', Sequential (
  (0): Linear (2048 -> 512)
  (1): ReLU ()
  (2): Dropout (p = 0.0)
)), ('logit', Linear (512 -> 9488)), ('ctx2att', Linear (512 -> 512)), ('fc_embed', Sequential (
  (0): Linear (2048 -> 512)
  (1): ReLU ()
  (2): Dropout (p = 0.0)
)), ('core', TopDownCore (
  (att_lstm): LSTMCell(1536, 512)
  (lang_lstm): LSTMCell(1024, 512)
  (attention): Attention (
    (h2att): Linear (512 -> 512)
    (alpha_net): Linear (512 -> 1)
  )
))])[0m
[33m01/27 11:15:47 Loading the model dict (last checkpoint) ['embed_1.weight', 'embed.0.weight', 'att_embed.0.weight', 'att_embed.0.bias', 'logit.weight', 'logit.bias', 'ctx2att.weight', 'ctx2att.bias', 'fc_embed.0.weight', 'fc_embed.0.bias', 'core.att_lstm.weight_ih', 'core.att_lstm.weight_hh', 'core.att_lstm.bias_ih', 'core.att_lstm.bias_hh', 'core.lang_lstm.weight_ih', 'core.lang_lstm.weight_hh', 'core.lang_lstm.bias_ih', 'core.lang_lstm.bias_hh', 'core.attention.h2att.weight', 'core.attention.h2att.bias', 'core.attention.alpha_net.weight', 'core.attention.alpha_net.bias'][0m
[33m01/27 11:15:47 DataLoader loading json file: data/coco/cocotalk.json[0m
[33m01/27 11:15:47 vocab size is 9487 [0m
[33m01/27 11:15:47 DataLoader loading h5 file: data/coco/cocotalk.h5[0m
[33m01/27 11:15:47 read 123287 images of size 3x256x256[0m
[33m01/27 11:15:47 max sequence length in data is 16[0m
[33m01/27 11:15:47 assigned 82783 images to split train[0m
[33m01/27 11:15:47 assigned 5000 images to split val[0m
[33m01/27 11:15:47 assigned 5000 images to split test[0m
[32m01/27 11:15:49 Initialized Word2 loss tau=0.090, alpha=0.3[0m
[33m01/27 11:15:49 Evaluating the test split (-1)[0m
## OAR [2018-01-27 11:20:03] Job 139643 KILLED ##
[33m01/27 11:20:28 GPU ID: 0 | available memory: 11172M[0m
[33m01/27 11:20:33 Starting from the best saved model[0m
[33m01/27 11:20:33 Starting from save/fncnn6_reset_word_coco_tword009_idf10_a03_topdown/model-best.pth[0m
[36m01/27 11:20:38 Setting CNN weigths from save/fncnn6_reset_word_coco_tword009_idf10_a03_topdown/model-cnn-best.pth[0m
[33m01/27 11:20:41 Finetuning up from 6 modules in the cnn[0m
[33m01/27 11:20:42 Top Down : OrderedDict([('embed_1', Embedding(9488, 512)), ('embed', Sequential (
  (0): Embedding(9488, 512)
  (1): ReLU ()
  (2): Dropout (p = 0.5)
)), ('att_embed', Sequential (
  (0): Linear (2048 -> 512)
  (1): ReLU ()
  (2): Dropout (p = 0.0)
)), ('logit', Linear (512 -> 9488)), ('ctx2att', Linear (512 -> 512)), ('fc_embed', Sequential (
  (0): Linear (2048 -> 512)
  (1): ReLU ()
  (2): Dropout (p = 0.0)
)), ('core', TopDownCore (
  (att_lstm): LSTMCell(1536, 512)
  (lang_lstm): LSTMCell(1024, 512)
  (attention): Attention (
    (h2att): Linear (512 -> 512)
    (alpha_net): Linear (512 -> 1)
  )
))])[0m
[33m01/27 11:20:43 Loading the model dict (last checkpoint) ['embed_1.weight', 'embed.0.weight', 'att_embed.0.weight', 'att_embed.0.bias', 'logit.weight', 'logit.bias', 'ctx2att.weight', 'ctx2att.bias', 'fc_embed.0.weight', 'fc_embed.0.bias', 'core.att_lstm.weight_ih', 'core.att_lstm.weight_hh', 'core.att_lstm.bias_ih', 'core.att_lstm.bias_hh', 'core.lang_lstm.weight_ih', 'core.lang_lstm.weight_hh', 'core.lang_lstm.bias_ih', 'core.lang_lstm.bias_hh', 'core.attention.h2att.weight', 'core.attention.h2att.bias', 'core.attention.alpha_net.weight', 'core.attention.alpha_net.bias'][0m
[33m01/27 11:20:43 DataLoader loading json file: data/coco/cocotalk.json[0m
[33m01/27 11:20:43 vocab size is 9487 [0m
[33m01/27 11:20:43 DataLoader loading h5 file: data/coco/cocotalk.h5[0m
[33m01/27 11:20:43 read 123287 images of size 3x256x256[0m
[33m01/27 11:20:43 max sequence length in data is 16[0m
[33m01/27 11:20:43 assigned 82783 images to split train[0m
[33m01/27 11:20:43 assigned 5000 images to split val[0m
[33m01/27 11:20:43 assigned 5000 images to split test[0m
[32m01/27 11:20:50 Initialized Word2 loss tau=0.090, alpha=0.3[0m
[33m01/27 11:20:50 Evaluating the test split (-1)[0m
## OAR [2018-01-27 11:25:04] Job 139644 KILLED ##
[33m01/27 11:25:26 GPU ID: 0 | available memory: 11172M[0m
[33m01/27 11:25:32 Starting from the best saved model[0m
[33m01/27 11:25:32 Starting from save/fncnn6_reset_word_coco_tword009_idf10_a03_topdown/model-best.pth[0m
[36m01/27 11:25:36 Setting CNN weigths from save/fncnn6_reset_word_coco_tword009_idf10_a03_topdown/model-cnn-best.pth[0m
[33m01/27 11:25:39 Finetuning up from 6 modules in the cnn[0m
[33m01/27 11:25:39 Top Down : OrderedDict([('embed_1', Embedding(9488, 512)), ('embed', Sequential (
  (0): Embedding(9488, 512)
  (1): ReLU ()
  (2): Dropout (p = 0.5)
)), ('att_embed', Sequential (
  (0): Linear (2048 -> 512)
  (1): ReLU ()
  (2): Dropout (p = 0.0)
)), ('logit', Linear (512 -> 9488)), ('ctx2att', Linear (512 -> 512)), ('fc_embed', Sequential (
  (0): Linear (2048 -> 512)
  (1): ReLU ()
  (2): Dropout (p = 0.0)
)), ('core', TopDownCore (
  (att_lstm): LSTMCell(1536, 512)
  (lang_lstm): LSTMCell(1024, 512)
  (attention): Attention (
    (h2att): Linear (512 -> 512)
    (alpha_net): Linear (512 -> 1)
  )
))])[0m
[33m01/27 11:25:39 Loading the model dict (last checkpoint) ['embed_1.weight', 'embed.0.weight', 'att_embed.0.weight', 'att_embed.0.bias', 'logit.weight', 'logit.bias', 'ctx2att.weight', 'ctx2att.bias', 'fc_embed.0.weight', 'fc_embed.0.bias', 'core.att_lstm.weight_ih', 'core.att_lstm.weight_hh', 'core.att_lstm.bias_ih', 'core.att_lstm.bias_hh', 'core.lang_lstm.weight_ih', 'core.lang_lstm.weight_hh', 'core.lang_lstm.bias_ih', 'core.lang_lstm.bias_hh', 'core.attention.h2att.weight', 'core.attention.h2att.bias', 'core.attention.alpha_net.weight', 'core.attention.alpha_net.bias'][0m
[33m01/27 11:25:39 DataLoader loading json file: data/coco/cocotalk.json[0m
[33m01/27 11:25:39 vocab size is 9487 [0m
[33m01/27 11:25:39 DataLoader loading h5 file: data/coco/cocotalk.h5[0m
[33m01/27 11:25:39 read 123287 images of size 3x256x256[0m
[33m01/27 11:25:39 max sequence length in data is 16[0m
[33m01/27 11:25:39 assigned 82783 images to split train[0m
[33m01/27 11:25:39 assigned 5000 images to split val[0m
[33m01/27 11:25:39 assigned 5000 images to split test[0m
[32m01/27 11:25:41 Initialized Word2 loss tau=0.090, alpha=0.3[0m
[33m01/27 11:25:41 Evaluating the test split (-1)[0m
## OAR [2018-01-27 11:30:03] Job 139645 KILLED ##
[33m01/27 11:30:26 GPU ID: 0 | available memory: 11172M[0m
[33m01/27 11:30:31 Starting from the best saved model[0m
[33m01/27 11:30:31 Starting from save/fncnn6_reset_word_coco_tword009_idf10_a03_topdown/model-best.pth[0m
[36m01/27 11:30:35 Setting CNN weigths from save/fncnn6_reset_word_coco_tword009_idf10_a03_topdown/model-cnn-best.pth[0m
[33m01/27 11:30:37 Finetuning up from 6 modules in the cnn[0m
[33m01/27 11:30:38 Top Down : OrderedDict([('embed_1', Embedding(9488, 512)), ('embed', Sequential (
  (0): Embedding(9488, 512)
  (1): ReLU ()
  (2): Dropout (p = 0.5)
)), ('att_embed', Sequential (
  (0): Linear (2048 -> 512)
  (1): ReLU ()
  (2): Dropout (p = 0.0)
)), ('logit', Linear (512 -> 9488)), ('ctx2att', Linear (512 -> 512)), ('fc_embed', Sequential (
  (0): Linear (2048 -> 512)
  (1): ReLU ()
  (2): Dropout (p = 0.0)
)), ('core', TopDownCore (
  (att_lstm): LSTMCell(1536, 512)
  (lang_lstm): LSTMCell(1024, 512)
  (attention): Attention (
    (h2att): Linear (512 -> 512)
    (alpha_net): Linear (512 -> 1)
  )
))])[0m
[33m01/27 11:30:39 Loading the model dict (last checkpoint) ['embed_1.weight', 'embed.0.weight', 'att_embed.0.weight', 'att_embed.0.bias', 'logit.weight', 'logit.bias', 'ctx2att.weight', 'ctx2att.bias', 'fc_embed.0.weight', 'fc_embed.0.bias', 'core.att_lstm.weight_ih', 'core.att_lstm.weight_hh', 'core.att_lstm.bias_ih', 'core.att_lstm.bias_hh', 'core.lang_lstm.weight_ih', 'core.lang_lstm.weight_hh', 'core.lang_lstm.bias_ih', 'core.lang_lstm.bias_hh', 'core.attention.h2att.weight', 'core.attention.h2att.bias', 'core.attention.alpha_net.weight', 'core.attention.alpha_net.bias'][0m
[33m01/27 11:30:39 DataLoader loading json file: data/coco/cocotalk.json[0m
[33m01/27 11:30:39 vocab size is 9487 [0m
[33m01/27 11:30:39 DataLoader loading h5 file: data/coco/cocotalk.h5[0m
[33m01/27 11:30:39 read 123287 images of size 3x256x256[0m
[33m01/27 11:30:39 max sequence length in data is 16[0m
[33m01/27 11:30:39 assigned 82783 images to split train[0m
[33m01/27 11:30:39 assigned 5000 images to split val[0m
[33m01/27 11:30:39 assigned 5000 images to split test[0m
[32m01/27 11:30:41 Initialized Word2 loss tau=0.090, alpha=0.3[0m
[33m01/27 11:30:41 Evaluating the test split (-1)[0m
[33m01/27 11:34:51 Loading reference captions..[0m
[33m01/27 11:34:52 Reference captions loaded![0m
[33m01/27 11:34:54 using 5000/5000 predictions[0m
[33m01/27 11:34:54 Loading model captions[0m
[33m01/27 11:34:54 Model captions loaded[0m
[33m01/27 11:34:54 Starting evaluation...[0m
PTBTokenizer tokenized 307085 tokens at 1178654,31 tokens per second.
PTBTokenizer tokenized 51183 tokens at 186142,71 tokens per second.
## OAR [2018-01-27 11:35:03] Job 139646 KILLED ##
[33m01/27 11:35:25 GPU ID: 0 | available memory: 11172M[0m
[33m01/27 11:35:31 Starting from the best saved model[0m
[33m01/27 11:35:31 Starting from save/fncnn6_reset_word_coco_tword009_idf10_a03_topdown/model-best.pth[0m
[36m01/27 11:35:35 Setting CNN weigths from save/fncnn6_reset_word_coco_tword009_idf10_a03_topdown/model-cnn-best.pth[0m
[33m01/27 11:35:37 Finetuning up from 6 modules in the cnn[0m
[33m01/27 11:35:38 Top Down : OrderedDict([('embed_1', Embedding(9488, 512)), ('embed', Sequential (
  (0): Embedding(9488, 512)
  (1): ReLU ()
  (2): Dropout (p = 0.5)
)), ('att_embed', Sequential (
  (0): Linear (2048 -> 512)
  (1): ReLU ()
  (2): Dropout (p = 0.0)
)), ('logit', Linear (512 -> 9488)), ('ctx2att', Linear (512 -> 512)), ('fc_embed', Sequential (
  (0): Linear (2048 -> 512)
  (1): ReLU ()
  (2): Dropout (p = 0.0)
)), ('core', TopDownCore (
  (att_lstm): LSTMCell(1536, 512)
  (lang_lstm): LSTMCell(1024, 512)
  (attention): Attention (
    (h2att): Linear (512 -> 512)
    (alpha_net): Linear (512 -> 1)
  )
))])[0m
[33m01/27 11:35:38 Loading the model dict (last checkpoint) ['embed_1.weight', 'embed.0.weight', 'att_embed.0.weight', 'att_embed.0.bias', 'logit.weight', 'logit.bias', 'ctx2att.weight', 'ctx2att.bias', 'fc_embed.0.weight', 'fc_embed.0.bias', 'core.att_lstm.weight_ih', 'core.att_lstm.weight_hh', 'core.att_lstm.bias_ih', 'core.att_lstm.bias_hh', 'core.lang_lstm.weight_ih', 'core.lang_lstm.weight_hh', 'core.lang_lstm.bias_ih', 'core.lang_lstm.bias_hh', 'core.attention.h2att.weight', 'core.attention.h2att.bias', 'core.attention.alpha_net.weight', 'core.attention.alpha_net.bias'][0m
[33m01/27 11:35:38 DataLoader loading json file: data/coco/cocotalk.json[0m
[33m01/27 11:35:38 vocab size is 9487 [0m
[33m01/27 11:35:38 DataLoader loading h5 file: data/coco/cocotalk.h5[0m
[33m01/27 11:35:38 read 123287 images of size 3x256x256[0m
[33m01/27 11:35:38 max sequence length in data is 16[0m
[33m01/27 11:35:38 assigned 82783 images to split train[0m
[33m01/27 11:35:38 assigned 5000 images to split val[0m
[33m01/27 11:35:38 assigned 5000 images to split test[0m
[32m01/27 11:35:40 Initialized Word2 loss tau=0.090, alpha=0.3[0m
[33m01/27 11:35:40 Evaluating the test split (-1)[0m
[33m01/27 11:39:43 Loading reference captions..[0m
[33m01/27 11:39:44 Reference captions loaded![0m
[33m01/27 11:39:45 using 5000/5000 predictions[0m
[33m01/27 11:39:45 Loading model captions[0m
[33m01/27 11:39:46 Model captions loaded[0m
[33m01/27 11:39:46 Starting evaluation...[0m
PTBTokenizer tokenized 307085 tokens at 1221084,53 tokens per second.
PTBTokenizer tokenized 51183 tokens at 286371,67 tokens per second.
Parsing reference captions
Error: Could not score batched file input:
org.fusesource.lmdbjni.LMDBException: MDB_CURSOR_FULL: Internal error - cursor stack limit reached
	at org.fusesource.lmdbjni.Util.checkErrorCode(Util.java:44)
	at org.fusesource.lmdbjni.Database.get(Database.java:202)
	at org.fusesource.lmdbjni.Database.get(Database.java:193)
	at org.fusesource.lmdbjni.Database.get(Database.java:186)
	at org.fusesource.lmdbjni.Database.get(Database.java:161)
	at edu.anu.spice.LmdbTupleDB.getTransaction(LmdbTupleDB.java:96)
	at edu.anu.spice.SpiceParser.loadTuplesFromDB(SpiceParser.java:195)
	at edu.anu.spice.SpiceParser.loadTuples(SpiceParser.java:245)
	at edu.anu.spice.SpiceParser.parseCaptions(SpiceParser.java:251)
	at edu.anu.spice.SpiceScorer.scoreBatch(SpiceScorer.java:109)
	at edu.anu.spice.SpiceScorer.main(SpiceScorer.java:60)
[33m01/27 12:12:33 GPU ID: 2 | available memory: 16276M[0m
[33m01/27 12:12:45 Starting from the best saved model[0m
[33m01/27 12:12:45 Starting from save/fncnn6_reset_strat_rhamming_pool2_tsent03_a04_topdown/model-best.pth[0m
[36m01/27 12:12:49 Setting CNN weigths from save/fncnn6_res[33m01/27 12:12:55 Starting from the best saved model[0m
[33m01/27 12:12:55 Starting from save/strat_lazy_rhamming_pool2_tsent03_a04_topdown/model-best.pth[0m
[36m01/27 12:13:01 Setting CNN weigths from save/strat_lazy_rhamming_pool2_tsent03_a04_topdown/model-cnn-best.pth[0m
 Dropout (p = 0.5)
)), ('att_embed', Sequential (
  (0): Linear (2048 -> 512)
  (1): ReLU ()
  (2): Dropout (p = 0.0)
)), ('logit', Linear (512 -> 9488)), ('ctx2att', Linear (512 -> 512)), ('fc_embed', Sequential (
  (0): Linear (2048 -> 512)
  (1): ReLU ()
  (2): Dropout (p = 0.0)
)), ('core', TopDownCore (
  (att_lstm): LSTMCell(1536, 512)
  (lang_lstm): LSTMCell(1024, 512)
  (attention): Attention (
    (h2att): Linear (512 -> 512)
    (alpha_net): Linear (512 -> 1)
  )
))])[0m
[33m01/27 12:12:53 Loading the model dict (last checkpoint) ['embed_1.weight', 'embed.0.weight', 'att_embed.0.weight', 'att_embed.0.bias', 'logit.weight', 'logit.bias', 'ctx2att.weight', 'ctx2att.bias', 'fc_embed.0.weight', 'fc_embed.0.bias', 'core.att_lstm.weight_ih', 'core.att_lstm.weight_hh', 'core.att_lstm.bias_ih', 'core.att_lstm.bias_hh', 'core.lang_lstm.weight_ih', 'core.lang_lstm.weight_hh', 'core.lang_lstm.bias_ih', 'core.lang_lstm.bias_hh', 'core.attention.h2att.weight', 'core.attention.h2att.bias', 'core.attention.alpha_net.weight', 'core.attention.alpha_net.bias'][0m
[33m01/27 12:12:53 DataLoader loading json file: data/coco/cocotalk.json[0m
[33m01/27 12:12:54 vocab size is 9487 [0m
[33m01/27 12:12:54 DataLoader loading h5 file: data/coco/cocotalk.h5[0m
[33m01/27 12:12:54 read 123287 images of size 3x256x256[0m
[33m01/27 12:12:54 max sequence length in data is 16[0m
[33m01/27 12:12:54 assigned 82783 images to split train[0m
[33m01/27 12:12:54 assigned 5000 images to split val[0m
[33m01/27 12:12:54 assigned 5000 images to split test[0m
[32m01/27 12:12:54 RewardSampler (stratified sampling), r=Hamming (Vpool=2, tau=0.30)[0m
[33m01/27 12:12:54 Evaluating the test split (-1)[0m
[33m01/27 12:13:09 Finetuning up from 0 modules in the cnn[0m
[33m01/27 12:13:11 Top Down : OrderedDict([('embed_1', Embedding(9488, 512)), ('embed', Sequential (
  (0): Embedding(9488, 512)
  (1): ReLU ()
  (2): Dropout (p = 0.5)
)), ('att_embed', Sequential (
  (0): Linear (2048 -> 512)
  (1): ReLU ()
  (2): Dropout (p = 0.0)
)), ('logit', Linear (512 -> 9488)), ('ctx2att', Linear (512 -> 512)), ('fc_embed', Sequential (
  (0): Linear (2048 -> 512)
  (1): ReLU ()
  (2): Dropout (p = 0.0)
)), ('core', TopDownCore (
  (att_lstm): LSTMCell(1536, 512)
  (lang_lstm): LSTMCell(1024, 512)
  (attention): Attention (
    (h2att): Linear (512 -> 512)
    (alpha_net): Linear (512 -> 1)
  )
))])[0m
[33m01/27 12:13:13 Loading the model dict (last checkpoint) ['embed_1.weight', 'embed.0.weight', 'att_embed.0.weight', 'att_embed.0.bias', 'logit.weight', 'logit.bias', 'ctx2att.weight', 'ctx2att.bias', 'fc_embed.0.weight', 'fc_embed.0.bias', 'core.att_lstm.weight_ih', 'core.att_lstm.weight_hh', 'core.att_lstm.bias_ih', 'core.att_lstm.bias_hh', 'core.lang_lstm.weight_ih', 'core.lang_lstm.weight_hh', 'core.lang_lstm.bias_ih', 'core.lang_lstm.bias_hh', 'core.attention.h2att.weight', 'core.attention.h2att.bias', 'core.attention.alpha_net.weight', 'core.attention.alpha_net.bias'][0m
[33m01/27 12:13:13 DataLoader loading json file: data/coco/cocotalk.json[0m
[33m01/27 12:13:13 vocab size is 9487 [0m
[33m01/27 12:13:13 DataLoader loading h5 file: data/coco/cocotalk.h5[0m
[33m01/27 12:13:13 read 123287 images of size 3x256x256[0m
[33m01/27 12:13:13 max sequence length in data is 16[0m
[33m01/27 12:13:13 assigned 82783 images to split train[0m
[33m01/27 12:13:13 assigned 5000 images to split val[0m
[33m01/27 12:13:13 assigned 5000 images to split test[0m
[32m01/27 12:13:13 RewardSampler (stratified sampling), r=Hamming (Vpool=2, tau=0.30)[0m
[33m01/27 12:13:13 Evaluating the test split (-1)[0m
[33m01/27 12:17:54 Loading reference captions..[0m
[33m01/27 12:17:55 Reference captions loaded![0m
[33m01/27 12:17:57 using 5000/5000 predictions[0m
[33m01/27 12:17:57 Loading model captions[0m
[33m01/27 12:17:57 Model captions loaded[0m
[33m01/27 12:17:57 Starting evaluation...[0m
PTBTokenizer tokenized 307085 tokens at 1261297.64 tokens per second.
PTBTokenizer tokenized 51948 tokens at 417185.10 tokens per second.
Parsing reference captions
Error: Could not score batched file input:
org.fusesource.lmdbjni.LMDBException: MDB_CURSOR_FULL: Internal error - cursor stack limit reached
	at org.fusesource.lmdbjni.Util.checkErrorCode(Util.java:44)
	at org.fusesource.lmdbjni.Database.get(Database.java:202)
	at org.fusesource.lmdbjni.Database.get(Database.java:193)
	at org.fusesource.lmdbjni.Database.get(Database.java:186)
	at org.fusesource.lmdbjni.Database.get(Database.java:161)
	at edu.anu.spice.LmdbTupleDB.getTransaction(LmdbTupleDB.java:96)
	at edu.anu.spice.SpiceParser.loadTuplesFromDB(SpiceParser.java:195)
	at edu.anu.spice.SpiceParser.loadTuples(SpiceParser.java:245)
	at edu.anu.spice.SpiceParser.parseCaptions(SpiceParser.java:251)
	at edu.anu.spice.SpiceScorer.scoreBatch(SpiceScorer.java:109)
	at edu.anu.spice.SpiceScorer.main(SpiceScorer.java:60)
[33m01/27 12:19:25 Loading reference captions..[0m
[33m01/27 12:19:25 Reference captions loaded![0m
[33m01/27 12:19:28 using 5000/5000 predictions[0m
[33m01/27 12:19:28 Loading model captions[0m
[33m01/27 12:19:28 Model captions loaded[0m
[33m01/27 12:19:28 Starting evaluation...[0m
PTBTokenizer tokenized 307085 tokens at 1074725.04 tokens per second.
PTBTokenizer tokenized 51012 tokens at 336175.64 tokens per second.
Parsing reference captions
Error: Could not score batched file input:
org.fusesource.lmdbjni.LMDBException: MDB_CURSOR_FULL: Internal error - cursor stack limit reached
	at org.fusesource.lmdbjni.Util.checkErrorCode(Util.java:44)
	at org.fusesource.lmdbjni.Database.get(Database.java:202)
	at org.fusesource.lmdbjni.Database.get(Database.java:193)
	at org.fusesource.lmdbjni.Database.get(Database.java:186)
	at org.fusesource.lmdbjni.Database.get(Database.java:161)
	at edu.anu.spice.LmdbTupleDB.getTransaction(LmdbTupleDB.java:96)
	at edu.anu.spice.SpiceParser.loadTuplesFromDB(SpiceParser.java:195)
	at edu.anu.spice.SpiceParser.loadTuples(SpiceParser.java:245)
	at edu.anu.spice.SpiceParser.parseCaptions(SpiceParser.java:251)
	at edu.anu.spice.SpiceScorer.scoreBatch(SpiceScorer.java:109)
	at edu.anu.spice.SpiceScorer.main(SpiceScorer.java:60)
