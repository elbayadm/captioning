[33m01/23 18:28:22 GPU ID: 1 | available memory: 12207M[0m
[33m01/23 18:28:48 Starting from the best saved model[0m
[33m01/23 18:28:48 Starting from save/word_coco_tword015_a07_showtell152/model-best.pth[0m
[36m01/23 18:28:51 Setting CNN weigths from save/word_coco_tword015_a07_showtell152/model-cnn-best.pth[0m
[33m01/23 18:29:09 Finetuning up from 0 modules in the cnn[0m
[33m01/23 18:29:10 Show & Tell : OrderedDict([('img_embed', Linear (2048 -> 512)), ('core', LSTM(512, 512, bias=False, dropout=0.5)), ('embed', Embedding(9488, 512)), ('drop_x_lm', Dropout (p = 0.5)), ('logit', Linear (512 -> 9488))])[0m
[33m01/23 18:29:10 Loading the model dict (last checkpoint) ['img_embed.weight', 'img_embed.bias', 'core.weight_ih_l0', 'core.weight_hh_l0', 'embed.weight', 'logit.weight', 'logit.bias'][0m
[33m01/23 18:29:14 DataLoader loading json file: data/coco/cocotalk.json[0m
[33m01/23 18:29:15 vocab size is 9487 [0m
[33m01/23 18:29:15 DataLoader loading h5 file: data/coco/cocotalk.h5[0m
[33m01/23 18:29:15 read 123287 images of size 3x256x256[0m
[33m01/23 18:29:15 max sequence length in data is 16[0m
[33m01/23 18:29:15 assigned 82783 images to split train[0m
[33m01/23 18:29:15 assigned 5000 images to split val[0m
[33m01/23 18:29:15 assigned 5000 images to split test[0m
## OAR [2018-01-23 18:29:27] Job 137964 KILLED ##
[33m01/23 18:29:43 GPU ID: 1 | available memory: 12189M[0m
[33m01/23 18:29:48 Starting from the best saved model[0m
[33m01/23 18:29:48 Starting from save/word_coco_tword015_a07_showtell152/model-best.pth[0m
[36m01/23 18:29:52 Setting CNN weigths from save/word_coco_tword015_a07_showtell152/model-cnn-best.pth[0m
odel-cnn-best.pth[0m
[33m01/23 18:30:00 GPU ID: 5 | available memory: 16276M[0m
[33m01/23 18:30:13 Finetuning up from 0 modules in the cnn[0m
[33m01/23 18:30:14 Show & Tell : OrderedDict([('img_embed', Linear (2048 -> 512)), ('core', LSTM(512, 512, bias=False, dropout=0.5)), ('embed', Embedding(9488, 512)), ('drop_x_lm', Dropout (p = 0.5)), ('logit', Linear (512 -> 9488))])[0m
[33m01/23 18:30:17 Loading the model dict (last checkpoint) ['img_embed.weight', 'img_embed.bias', 'core.weight_ih_l0', 'core.weight_hh_l0', 'embed.weight', 'logit.weight', 'logit.bias'][0m
[33m01/23 18:30:18 DataLoader loading json file: data/coco/cocotalk.json[0m
[33m01/23 18:30:18 vocab size is 9487 [0m
[33m01/23 18:30:18 DataLoader loading h5 file: data/coco/cocotalk.h5[0m
[33m01/23 18:30:18 read 123287 images of size 3x256x256[0m
[33m01/23 18:30:18 max sequence length in data is 16[0m
[33m01/23 18:30:18 assigned 82783 images to split train[0m
[33m01/23 18:30:18 assigned 5000 images to split val[0m
[33m01/23 18:30:18 assigned 5000 images to split test[0m
[32m01/23 18:30:18 RewardSampler (stratified sampling), r=Hamming (Vpool=1, tau=0.17)[0m
[33m01/23 18:30:18 Evaluating the test split (-1)[0m
assigned 82783 images to split train[0m
[33m01/23 18:30:20 assigned 5000 images to split val[0m
[33m01/23 18:30:20 assigned 5000 images to split test[0m
[32m01/23 18:30:20 RewardSampler (stratified sampling), r=Hamming (Vpool=2, tau=0.30)[0m
[33m01/23 18:30:20 Evaluating the test split (-1)[0m
[32m01/23 18:30:51 Initialized Word2 loss tau=0.150, alpha=0.7[0m
[33m01/23 18:30:51 Evaluating the test split (-1)[0m
[33m01/23 18:35:58 Loading reference captions..[0m
[33m01/23 18:36:00 Reference captions loaded![0m
[33m01/23 18:36:02 using 5000/5000 predictions[0m
[33m01/23 18:36:02 Loading model captions[0m
[33m01/23 18:36:02 Model captions loaded[0m
[33m01/23 18:36:02 Starting evaluation...[0m
PTBTokenizer tokenized 307085 tokens at 1150313.53 tokens per second.
PTBTokenizer tokenized 51263 tokens at 360003.18 tokens per second.
[33m01/23 18:36:20 Loading reference captions..[0m
[33m01/23 18:36:22 Reference captions loaded![0m
[33m01/23 18:36:24 using 5000/5000 predictions[0m
[33m01/23 18:36:24 Loading model captions[0m
[33m01/23 18:36:24 Model captions loaded[0m
[33m01/23 18:36:24 Starting evaluation...[0m
PTBTokenizer tokenized 307085 tokens at 923650.88 tokens per second.

PTBTokenizer tokenized 49687 tokens at 425840.69 tokens per second.
PTBTokenizer tokenized 51758 tokens at 432839.72 tokens per second.
[33m01/23 18:37:26 GPU ID: 1 | available memory: 12207M[0m
[33m01/23 18:37:30 Starting from the best saved model[0m
[33m01/23 18:37:30 Starting from save/importance_lazy_qhamming_limited1_tsent017_rcider_tsent05_a04_showtell152/model-best.pth[0m
[36m01/23 18:37:34 Setting CNN weigths from save/importance_lazy_qhamming_limited1_tsent017_rcider_tsent05_a04_showtell152/model-cnn-best.pth[0m
[33m01/23 18:37:45 Finetuning up from 0 modules in the cnn[0m
[33m01/23 18:37:46 Show & Tell : OrderedDict([('img_embed', Linear (2048 -> 512)), ('core', LSTM(512, 512, bias=False, dropout=0.5)), ('embed', Embedding(9488, 512)), ('drop_x_lm', Dropout (p = 0.5)), ('logit', Linear (512 -> 9488))])[0m
[33m01/23 18:37:47 Loading the model dict (last checkpoint) ['img_embed.weight', 'img_embed.bias', 'core.weight_ih_l0', 'core.weight_hh_l0', 'embed.weight', 'logit.weight', 'logit.bias'][0m
[33m01/23 18:37:47 DataLoader loading json file: data/coco/cocotalk.json[0m
[33m01/23 18:37:47 vocab size is 9487 [0m
[33m01/23 18:37:47 DataLoader loading h5 file: data/coco/cocotalk.h5[0m
[33m01/23 18:37:47 read 123287 images of size 3x256x256[0m
[33m01/23 18:37:47 max sequence length in data is 16[0m
[33m01/23 18:37:47 assigned 82783 images to split train[0m
[33m01/23 18:37:47 assigned 5000 images to split val[0m
[33m01/23 18:37:47 assigned 5000 images to split test[0m
[32m01/23 18:37:53 using importance sampling r=CIDEr (tau=0.50) and q=Hamming (Vpool=1, tau=0.17)[0m
[33m01/23 18:37:53 Evaluating the test split (-1)[0m
[33m01/23 18:37:59 GPU ID: 0 | available memory: 12205M[0m
[33m01/23 18:38:05 Starting from the best saved model[0m
[33m01/23 18:38:05 Starting from save/importance_qhamming_limited1_tsent017_rcider_tsent05_a04_showtell152/model-best.pth[0m
[36m01/23 18:38:09 Setting CNN weigths from save/importance_qhamming_limited1_tsent017_rcider_tsent05_a04_showtell152/model-cnn-best.pth[0m
[33m01/23 18:38:16 Finetuning up from 0 modules in the cnn[0m
[33m01/23 18:38:17 Show & Tell : OrderedDict([('img_embed', Linear (2048 -> 512)), ('core', LSTM(512, 512, bias=False, dropout=0.5)), ('embed', Embedding(9488, 512)), ('drop_x_lm', Dropout (p = 0.5)), ('logit', Linear (512 -> 9488))])[0m
[33m01/23 18:38:18 Loading the model dict (last checkpoint) ['img_embed.weight', 'img_embed.bias', 'core.weight_ih_l0', 'core.weight_hh_l0', 'embed.weight', 'logit.weight', 'logit.bias'][0m
[33m01/23 18:38:19 DataLoader loading json file: data/coco/cocotalk.json[0m
[33m01/23 18:38:19 vocab size is 9487 [0m
[33m01/23 18:38:19 DataLoader loading h5 file: data/coco/cocotalk.h5[0m
[33m01/23 18:38:19 read 123287 images of size 3x256x256[0m
[33m01/23 18:38:19 max sequence length in data is 16[0m
[33m01/23 18:38:19 assigned 82783 images to split train[0m
[33m01/23 18:38:19 assigned 5000 images to split val[0m
[33m01/23 18:38:19 assigned 5000 images to split test[0m
[32m01/23 18:38:22 using importance sampling r=CIDEr (tau=0.50) and q=Hamming (Vpool=1, tau=0.17)[0m
[33m01/23 18:38:22 Evaluating the test split (-1)[0m
[33m01/23 18:42:32 Loading reference captions..[0m
[33m01/23 18:42:33 Reference captions loaded![0m
[33m01/23 18:42:34 using 5000/5000 predictions[0m
[33m01/23 18:42:35 Loading model captions[0m
[33m01/23 18:42:35 Model captions loaded[0m
[33m01/23 18:42:35 Starting evaluation...[0m
PTBTokenizer tokenized 307085 tokens at 1392237.86 tokens per second.
PTBTokenizer tokenized 51403 tokens at 443307.31 tokens per second.
[33m01/23 18:43:57 Loading reference captions..[0m
[33m01/23 18:43:58 Reference captions loaded![0m
[33m01/23 18:44:00 using 5000/5000 predictions[0m
[33m01/23 18:44:00 Loading model captions[0m
[33m01/23 18:44:00 Model captions loaded[0m
[33m01/23 18:44:00 Starting evaluation...[0m
PTBTokenizer tokenized 307085 tokens at 806774.17 tokens per second.
PTBTokenizer tokenized 51784 tokens at 153366.24 tokens per second.
[33m01/23 19:54:40 GPU ID: 1 | available memory: 12189M[0m
[33m01/23 19:54:52 Starting from the best saved model[0m
[33m01/23 19:54:52 Starting from save/topdown_resnet152_msc/model-best.pth[0m
Traceback (most recent call last):
  File "eval.py", line 76, in <module>
    str(vars(opt)[k]) + ' vs. ' + str(vars(infos['opt'])[k]))
AssertionError: region_size option not consistent 14 vs. 7
[33m01/23 19:55:54 GPU ID: 1 | available memory: 12189M[0m
[33m01/23 19:55:58 Starting from the best saved model[0m
[33m01/23 19:55:58 Starting from save/fncnn6_topdown_resnet152_msc/model-best.pth[0m
Traceback (most recent call last):
  File "eval.py", line 76, in <module>
    str(vars(opt)[k]) + ' vs. ' + str(vars(infos['opt'])[k]))
AssertionError: region_size option not consistent 14 vs. 7
[33m01/23 19:57:24 GPU ID: 1 | available memory: 12189M[0m
[33m01/23 19:57:28 Starting from the best saved model[0m
[33m01/23 19:57:28 Starting from save/fncnn6_topdown_resnet152_msc/model-best.pth[0m
Traceback (most recent call last):
  File "eval.py", line 76, in <module>
    str(vars(opt)[k]) + ' vs. ' + str(vars(infos['opt'])[k]))
AssertionError: use_adaptive_pooling option not consistent 1 vs. 0
[33m01/23 19:57:31 GPU ID: 0 | available memory: 12205M[0m
[33m01/23 19:57:36 Starting from the best saved model[0m
[33m01/23 19:57:36 Starting from save/topdown_resnet152_msc/model-best.pth[0m
Traceback (most recent call last):
  File "eval.py", line 76, in <module>
    str(vars(opt)[k]) + ' vs. ' + str(vars(infos['opt'])[k]))
AssertionError: use_adaptive_pooling option not consistent 1 vs. 0
[33m01/23 19:58:04 GPU ID: 1 | available memory: 12189M[0m
[33m01/23 19:58:08 Starting from the best saved model[0m
[33m01/23 19:58:08 Starting from save/topdown_resnet152_msc/model-best.pth[0m
[36m01/23 19:58:12 Setting CNN weigths from save/topdown_resnet152_msc/model-cnn-best.pth[0m
[33m01/23 19:58:16 Finetuning up from 0 modules in the cnn[0m
[33m01/23 19:58:17 Top Down : OrderedDict([('embed_1', Embedding(9488, 512)), ('embed', Sequential (
  (0): Embedding(9488, 512)
  (1): ReLU ()
  (2): Dropout (p = 0.5)
)), ('att_embed', Sequential (
  (0): Linear (2048 -> 512)
  (1): ReLU ()
  (2): Dropout (p = 0.0)
)), ('logit', Linear (512 -> 9488)), ('ctx2att', Linear (512 -> 512)), ('fc_embed', Sequential (
  (0): Linear (2048 -> 512)
  (1): ReLU ()
  (2): Dropout (p = 0.0)
)), ('core', TopDownCore (
  (att_lstm): LSTMCell(1536, 512)
  (lang_lstm): LSTMCell(1024, 512)
  (attention): Attention (
    (h2att): Linear (512 -> 512)
    (alpha_net): Linear (512 -> 1)
  )
))])[0m
[33m01/23 19:58:17 Loading the model dict (last checkpoint) ['embed_1.weight', 'embed.0.weight', 'att_embed.0.weight', 'att_embed.0.bias', 'logit.weight', 'logit.bias', 'ctx2att.weight', 'ctx2att.bias', 'fc_embed.0.weight', 'fc_embed.0.bias', 'core.att_lstm.weight_ih', 'core.att_lstm.weight_hh', 'core.att_lstm.bias_ih', 'core.att_lstm.bias_hh', 'core.lang_lstm.weight_ih', 'core.lang_lstm.weight_hh', 'core.lang_lstm.bias_ih', 'core.lang_lstm.bias_hh', 'core.attention.h2att.weight', 'core.attention.h2att.bias', 'core.attention.alpha_net.weight', 'core.attention.alpha_net.bias'][0m
[33m01/23 19:58:17 DataLoader loading json file: data/coco/cocotalk.json[0m
[33m01/23 19:58:17 vocab size is 9487 [0m
[33m01/23 19:58:17 DataLoader loading h5 file: data/coco/cocotalk.h5[0m
[33m01/23 19:58:17 read 123287 images of size 3x256x256[0m
[33m01/23 19:58:17 max sequence length in data is 16[0m
[33m01/23 19:58:17 assigned 82783 images to split train[0m
[33m01/23 19:58:17 assigned 5000 images to split val[0m
[33m01/23 19:58:17 assigned 5000 images to split test[0m
[32m01/23 19:58:17 Default ML loss[0m
[33m01/23 19:58:17 Evaluating the test split (-1)[0m
[33m01/23 19:58:45 GPU ID: 0 | available memory: 12205M[0m
[33m01/23 19:58:50 Starting from the best saved model[0m
[33m01/23 19:58:50 Starting from save/fncnn6_topdown_resnet152_msc/model-best.pth[0m
[36m01/23 19:58:55 Setting CNN weigths from save/fncnn6_topdown_resnet152_msc/model-cnn-best.pth[0m
[33m01/23 19:59:02 Finetuning up from 6 modules in the cnn[0m
[33m01/23 19:59:02 Top Down : OrderedDict([('embed_1', Embedding(9488, 512)), ('embed', Sequential (
  (0): Embedding(9488, 512)
  (1): ReLU ()
  (2): Dropout (p = 0.5)
)), ('att_embed', Sequential (
  (0): Linear (2048 -> 512)
  (1): ReLU ()
  (2): Dropout (p = 0.0)
)), ('logit', Linear (512 -> 9488)), ('ctx2att', Linear (512 -> 512)), ('fc_embed', Sequential (
  (0): Linear (2048 -> 512)
  (1): ReLU ()
  (2): Dropout (p = 0.0)
)), ('core', TopDownCore (
  (att_lstm): LSTMCell(1536, 512)
  (lang_lstm): LSTMCell(1024, 512)
  (attention): Attention (
    (h2att): Linear (512 -> 512)
    (alpha_net): Linear (512 -> 1)
  )
))])[0m
[33m01/23 19:59:04 Loading the model dict (last checkpoint) ['embed_1.weight', 'embed.0.weight', 'att_embed.0.weight', 'att_embed.0.bias', 'logit.weight', 'logit.bias', 'ctx2att.weight', 'ctx2att.bias', 'fc_embed.0.weight', 'fc_embed.0.bias', 'core.att_lstm.weight_ih', 'core.att_lstm.weight_hh', 'core.att_lstm.bias_ih', 'core.att_lstm.bias_hh', 'core.lang_lstm.weight_ih', 'core.lang_lstm.weight_hh', 'core.lang_lstm.bias_ih', 'core.lang_lstm.bias_hh', 'core.attention.h2att.weight', 'core.attention.h2att.bias', 'core.attention.alpha_net.weight', 'core.attention.alpha_net.bias'][0m
[33m01/23 19:59:04 DataLoader loading json file: data/coco/cocotalk.json[0m
[33m01/23 19:59:04 vocab size is 9487 [0m
[33m01/23 19:59:04 DataLoader loading h5 file: data/coco/cocotalk.h5[0m
[33m01/23 19:59:04 read 123287 images of size 3x256x256[0m
[33m01/23 19:59:04 max sequence length in data is 16[0m
[33m01/23 19:59:04 assigned 82783 images to split train[0m
[33m01/23 19:59:04 assigned 5000 images to split val[0m
[33m01/23 19:59:04 assigned 5000 images to split test[0m
[32m01/23 19:59:04 Default ML loss[0m
[33m01/23 19:59:04 Evaluating the test split (-1)[0m
[33m01/23 20:02:25 Loading reference captions..[0m
[33m01/23 20:02:26 Reference captions loaded![0m
[33m01/23 20:02:27 using 5000/5000 predictions[0m
[33m01/23 20:02:28 Loading model captions[0m
[33m01/23 20:02:28 Model captions loaded[0m
[33m01/23 20:02:28 Starting evaluation...[0m
PTBTokenizer tokenized 307085 tokens at 1256736,39 tokens per second.
PTBTokenizer tokenized 51828 tokens at 413418,32 tokens per second.
[33m01/23 20:04:14 Loading reference captions..[0m
[33m01/23 20:04:15 Reference captions loaded![0m
[33m01/23 20:04:17 using 5000/5000 predictions[0m
[33m01/23 20:04:17 Loading model captions[0m
[33m01/23 20:04:17 Model captions loaded[0m
[33m01/23 20:04:17 Starting evaluation...[0m
PTBTokenizer tokenized 307085 tokens at 525931.13 tokens per second.
PTBTokenizer tokenized 51957 tokens at 320091.68 tokens per second.
[33m01/23 20:11:38 GPU ID: 3 | available memory: 16276M[0m
[33m01/23 20:11:46 Starting from the best saved model[0m
[33m01/23 20:11:46 Starting from save/strat_lazy_rhamming_pool1_tsent017_a04_topdown/model-best.pth[0m
[36m01/23 20:11:50 Setting CNN weigths from save/strat_lazy_rhamming_pool1_tsent017_a04_topdown/model-cnn-best.pth[0m
[33m01/23 20:11:53 Finetuning up from 0 modules in the cnn[0m
[33m01/23 20:11:54 Top Down : OrderedDict([('embed_1', Embedding(9488, 512)), ('embed', Sequential (
  (0): Embedding(9488, 512)
  (1): ReLU ()
  (2): Dropout (p = 0.5)
)), ('att_embed', Sequential (
  (0): Linear (2048 -> 512)
  (1): ReLU ()
  (2): Dropout (p = 0.0)
)), ('logit', Linear (512 -> 9488)), ('ctx2att', Linear (512 -> 512)), ('fc_embed', Sequential (
  (0): Linear (2048 -> 512)
  (1): ReLU ()
  (2): Dropout (p = 0.0)
)), ('core', TopDownCore (
  (att_lstm): LSTMCell(1536, 512)
  (lang_lstm): LSTMCell(1024, 512)
  (attention): Attention (
    (h2att): Linear (512 -> 512)
    (alpha_net): Linear (512 -> 1)
  )
))])[0m
[33m01/23 20:11:54 Loading the model dict (last checkpoint) ['embed_1.weight', 'embed.0.weight', 'att_embed.0.weight', 'att_embed.0.bias', 'logit.weight', 'logit.bias', 'ctx2att.weight', 'ctx2att.bias', 'fc_embed.0.weight', 'fc_embed.0.bias', 'core.att_lstm.weight_ih', 'core.att_lstm.weight_hh', 'core.att_lstm.bias_ih', 'core.att_lstm.bias_hh', 'core.lang_lstm.weight_ih', 'core.lang_lstm.weight_hh', 'core.lang_lstm.bias_ih', 'core.lang_lstm.bias_hh', 'core.attention.h2att.weight', 'core.attention.h2att.bias', 'core.attention.alpha_net.weight', 'core.attention.alpha_net.bias'][0m
[33m01/23 20:11:54 DataLoader loading json file: data/coco/cocotalk.json[0m
[33m01/23 20:11:54 vocab size is 9487 [0m
[33m01/23 20:11:54 DataLoader loading h5 file: data/coco/cocotalk.h5[0m
[33m01/23 20:11:54 read 123287 images of size 3x256x256[0m
[33m01/23 20:11:54 max sequence length in data is 16[0m
[33m01/23 20:11:54 assigned 82783 images to split train[0m
[33m01/23 20:11:54 assigned 5000 images to split val[0m
[33m01/23 20:11:54 assigned 5000 images to split test[0m
[32m01/23 20:11:54 RewardSampler (stratified sampling), r=Hamming (Vpool=1, tau=0.17)[0m
[33m01/23 20:11:54 Evaluating the test split (-1)[0m
[33m01/23 20:17:15 Loading reference captions..[0m
[33m01/23 20:17:15 Reference captions loaded![0m
[33m01/23 20:17:18 using 5000/5000 predictions[0m
[33m01/23 20:17:18 Loading model captions[0m
[33m01/23 20:17:18 Model captions loaded[0m
[33m01/23 20:17:18 Starting evaluation...[0m
PTBTokenizer tokenized 307085 tokens at 1184015.35 tokens per second.
PTBTokenizer tokenized 51523 tokens at 355437.63 tokens per second.
[33m01/23 20:17:43 GPU ID: 3 | available memory: 16276M[0m
[33m01/23 20:17:51 Starting from the best saved model[0m
[33m01/23 20:17:51 Starting from save/strat_rhamming_pool1_tsent017_a04_topdown/model-best.pth[0m
[36m01/23 20:17:56 Setting CNN weigths from save/strat_rhamming_pool1_tsent017_a04_topdown/model-cnn-best.pth[0m
[33m01/23 20:17:59 Finetuning up from 0 modules in the cnn[0m
[33m01/23 20:18:00 Top Down : OrderedDict([('embed_1', Embedding(9488, 512)), ('embed', Sequential (
  (0): Embedding(9488, 512)
  (1): ReLU ()
  (2): Dropout (p = 0.5)
)), ('att_embed', Sequential (
  (0): Linear (2048 -> 512)
  (1): ReLU ()
  (2): Dropout (p = 0.0)
)), ('logit', Linear (512 -> 9488)), ('ctx2att', Linear (512 -> 512)), ('fc_embed', Sequential (
  (0): Linear (2048 -> 512)
  (1): ReLU ()
  (2): Dropout (p = 0.0)
)), ('core', TopDownCore (
  (att_lstm): LSTMCell(1536, 512)
  (lang_lstm): LSTMCell(1024, 512)
  (attention): Attention (
    (h2att): Linear (512 -> 512)
    (alpha_net): Linear (512 -> 1)
  )
))])[0m
[33m01/23 20:18:00 Loading the model dict (last checkpoint) ['embed_1.weight', 'embed.0.weight', 'att_embed.0.weight', 'att_embed.0.bias', 'logit.weight', 'logit.bias', 'ctx2att.weight', 'ctx2att.bias', 'fc_embed.0.weight', 'fc_embed.0.bias', 'core.att_lstm.weight_ih', 'core.att_lstm.weight_hh', 'core.att_lstm.bias_ih', 'core.att_lstm.bias_hh', 'core.lang_lstm.weight_ih', 'core.lang_lstm.weight_hh', 'core.lang_lstm.bias_ih', 'core.lang_lstm.bias_hh', 'core.attention.h2att.weight', 'core.attention.h2att.bias', 'core.attention.alpha_net.weight', 'core.attention.alpha_net.bias'][0m
[33m01/23 20:18:00 DataLoader loading json file: data/coco/cocotalk.json[0m
[33m01/23 20:18:00 vocab size is 9487 [0m
[33m01/23 20:18:00 DataLoader loading h5 file: data/coco/cocotalk.h5[0m
[33m01/23 20:18:00 read 123287 images of size 3x256x256[0m
[33m01/23 20:18:00 max sequence length in data is 16[0m
[33m01/23 20:18:00 assigned 82783 images to split train[0m
[33m01/23 20:18:00 assigned 5000 images to split val[0m
[33m01/23 20:18:00 assigned 5000 images to split test[0m
[32m01/23 20:18:00 RewardSampler (stratified sampling), r=Hamming (Vpool=1, tau=0.17)[0m
[33m01/23 20:18:00 Evaluating the test split (-1)[0m
