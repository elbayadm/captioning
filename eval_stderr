[33m01/27 11:01:29 GPU ID: 2 | available memory: 16276M[0m
[33m01/27 11:01:38 Starting from the best saved model[0m
[33m01/27 11:01:38 Starting from save/fncnn6_reset_word_coco_tword009_idf10_a03_topdown/model-best.pth[0m
[36m01/27 11:01:43 Setting CNN weigths from save/fncnn6_reset_word_coco_tword009_idf10_a03_topdown/model-cnn-best.pth[0m
[33m01/27 11:01:46 Finetuning up from 6 modules in the cnn[0m
[33m01/27 11:01:47 Top Down : OrderedDict([('embed_1', Embedding(9488, 512)), ('embed', Sequential (
  (0): Embedding(9488, 512)
  (1): ReLU ()
  (2): Dropout (p = 0.5)
)), ('att_embed', Sequential (
  (0): Linear (2048 -> 512)
  (1): ReLU ()
  (2): Dropout (p = 0.0)
)), ('logit', Linear (512 -> 9488)), ('ctx2att', Linear (512 -> 512)), ('fc_embed', Sequential (
  (0): Linear (2048 -> 512)
  (1): ReLU ()
  (2): Dropout (p = 0.0)
)), ('core', TopDownCore (
  (att_lstm): LSTMCell(1536, 512)
  (lang_lstm): LSTMCell(1024, 512)
  (attention): Attention (
    (h2att): Linear (512 -> 512)
    (alpha_net): Linear (512 -> 1)
  )
))])[0m
[33m01/27 11:01:47 Loading the model dict (last checkpoint) ['embed_1.weight', 'embed.0.weight', 'att_embed.0.weight', 'att_embed.0.bias', 'logit.weight', 'logit.bias', 'ctx2att.weight', 'ctx2att.bias', 'fc_embed.0.weight', 'fc_embed.0.bias', 'core.att_lstm.weight_ih', 'core.att_lstm.weight_hh', 'core.att_lstm.bias_ih', 'core.att_lstm.bias_hh', 'core.lang_lstm.weight_ih', 'core.lang_lstm.weight_hh', 'core.lang_lstm.bias_ih', 'core.lang_lstm.bias_hh', 'core.attention.h2att.weight', 'core.attention.h2att.bias', 'core.attention.alpha_net.weight', 'core.attention.alpha_net.bias'][0m
[33m01/27 11:01:47 DataLoader loading json file: data/coco/cocotalk.json[0m
[33m01/27 11:01:47 vocab size is 9487 [0m
[33m01/27 11:01:47 DataLoader loading h5 file: data/coco/cocotalk.h5[0m
[33m01/27 11:01:47 read 123287 images of size 3x256x256[0m
[33m01/27 11:01:47 max sequence length in data is 16[0m
[33m01/27 11:01:47 assigned 82783 images to split train[0m
[33m01/27 11:01:47 assigned 5000 images to split val[0m
[33m01/27 11:01:47 assigned 5000 images to split test[0m
[32m01/27 11:01:50 Initialized Word2 loss tau=0.090, alpha=0.3[0m
[33m01/27 11:01:50 Evaluating the test split (-1)[0m
## OAR [2018-01-27 11:05:04] Job 139640 KILLED ##
[33m01/27 11:05:31 GPU ID: 2 | available memory: 16276M[0m
[33m01/27 11:05:41 Starting from the best saved model[0m
[33m01/27 11:05:41 Starting from save/fncnn6_reset_word_coco_tword009_idf10_a03_topdown/model-best.pth[0m
[36m01/27 11:05:45 Setting CNN weigths from save/fncnn6_reset_word_coco_tword009_idf10_a03_topdown/model-cnn-best.pth[0m
[33m01/27 11:05:47 Finetuning up from 6 modules in the cnn[0m
[33m01/27 11:05:47 Top Down : OrderedDict([('embed_1', Embedding(9488, 512)), ('embed', Sequential (
  (0): Embedding(9488, 512)
  (1): ReLU ()
  (2): Dropout (p = 0.5)
)), ('att_embed', Sequential (
  (0): Linear (2048 -> 512)
  (1): ReLU ()
  (2): Dropout (p = 0.0)
)), ('logit', Linear (512 -> 9488)), ('ctx2att', Linear (512 -> 512)), ('fc_embed', Sequential (
  (0): Linear (2048 -> 512)
  (1): ReLU ()
  (2): Dropout (p = 0.0)
)), ('core', TopDownCore (
  (att_lstm): LSTMCell(1536, 512)
  (lang_lstm): LSTMCell(1024, 512)
  (attention): Attention (
    (h2att): Linear (512 -> 512)
    (alpha_net): Linear (512 -> 1)
  )
))])[0m
[33m01/27 11:05:47 Loading the model dict (last checkpoint) ['embed_1.weight', 'embed.0.weight', 'att_embed.0.weight', 'att_embed.0.bias', 'logit.weight', 'logit.bias', 'ctx2att.weight', 'ctx2att.bias', 'fc_embed.0.weight', 'fc_embed.0.bias', 'core.att_lstm.weight_ih', 'core.att_lstm.weight_hh', 'core.att_lstm.bias_ih', 'core.att_lstm.bias_hh', 'core.lang_lstm.weight_ih', 'core.lang_lstm.weight_hh', 'core.lang_lstm.bias_ih', 'core.lang_lstm.bias_hh', 'core.attention.h2att.weight', 'core.attention.h2att.bias', 'core.attention.alpha_net.weight', 'core.attention.alpha_net.bias'][0m
[33m01/27 11:05:48 DataLoader loading json file: data/coco/cocotalk.json[0m
[33m01/27 11:05:48 vocab size is 9487 [0m
[33m01/27 11:05:48 DataLoader loading h5 file: data/coco/cocotalk.h5[0m
[33m01/27 11:05:48 read 123287 images of size 3x256x256[0m
[33m01/27 11:05:48 max sequence length in data is 16[0m
[33m01/27 11:05:48 assigned 82783 images to split train[0m
[33m01/27 11:05:48 assigned 5000 images to split val[0m
[33m01/27 11:05:48 assigned 5000 images to split test[0m
[32m01/27 11:05:49 Initialized Word2 loss tau=0.090, alpha=0.3[0m
[33m01/27 11:05:49 Evaluating the test split (-1)[0m
## OAR [2018-01-27 11:10:03] Job 139641 KILLED ##
[33m01/27 11:10:30 GPU ID: 2 | available memory: 16276M[0m
[33m01/27 11:10:39 Starting from the best saved model[0m
[33m01/27 11:10:39 Starting from save/fncnn6_reset_word_coco_tword009_idf10_a03_topdown/model-best.pth[0m
[36m01/27 11:10:43 Setting CNN weigths from save/fncnn6_reset_word_coco_tword009_idf10_a03_topdown/model-cnn-best.pth[0m
[33m01/27 11:10:45 Finetuning up from 6 modules in the cnn[0m
[33m01/27 11:10:46 Top Down : OrderedDict([('embed_1', Embedding(9488, 512)), ('embed', Sequential (
  (0): Embedding(9488, 512)
  (1): ReLU ()
  (2): Dropout (p = 0.5)
)), ('att_embed', Sequential (
  (0): Linear (2048 -> 512)
  (1): ReLU ()
  (2): Dropout (p = 0.0)
)), ('logit', Linear (512 -> 9488)), ('ctx2att', Linear (512 -> 512)), ('fc_embed', Sequential (
  (0): Linear (2048 -> 512)
  (1): ReLU ()
  (2): Dropout (p = 0.0)
)), ('core', TopDownCore (
  (att_lstm): LSTMCell(1536, 512)
  (lang_lstm): LSTMCell(1024, 512)
  (attention): Attention (
    (h2att): Linear (512 -> 512)
    (alpha_net): Linear (512 -> 1)
  )
))])[0m
[33m01/27 11:10:46 Loading the model dict (last checkpoint) ['embed_1.weight', 'embed.0.weight', 'att_embed.0.weight', 'att_embed.0.bias', 'logit.weight', 'logit.bias', 'ctx2att.weight', 'ctx2att.bias', 'fc_embed.0.weight', 'fc_embed.0.bias', 'core.att_lstm.weight_ih', 'core.att_lstm.weight_hh', 'core.att_lstm.bias_ih', 'core.att_lstm.bias_hh', 'core.lang_lstm.weight_ih', 'core.lang_lstm.weight_hh', 'core.lang_lstm.bias_ih', 'core.lang_lstm.bias_hh', 'core.attention.h2att.weight', 'core.attention.h2att.bias', 'core.attention.alpha_net.weight', 'core.attention.alpha_net.bias'][0m
[33m01/27 11:10:46 DataLoader loading json file: data/coco/cocotalk.json[0m
[33m01/27 11:10:46 vocab size is 9487 [0m
[33m01/27 11:10:46 DataLoader loading h5 file: data/coco/cocotalk.h5[0m
[33m01/27 11:10:46 read 123287 images of size 3x256x256[0m
[33m01/27 11:10:46 max sequence length in data is 16[0m
[33m01/27 11:10:46 assigned 82783 images to split train[0m
[33m01/27 11:10:46 assigned 5000 images to split val[0m
[33m01/27 11:10:46 assigned 5000 images to split test[0m
[32m01/27 11:10:48 Initialized Word2 loss tau=0.090, alpha=0.3[0m
[33m01/27 11:10:48 Evaluating the test split (-1)[0m
## OAR [2018-01-27 11:15:04] Job 139642 KILLED ##
[33m01/27 11:15:30 GPU ID: 2 | available memory: 16276M[0m
[33m01/27 11:15:40 Starting from the best saved model[0m
[33m01/27 11:15:40 Starting from save/fncnn6_reset_word_coco_tword009_idf10_a03_topdown/model-best.pth[0m
[36m01/27 11:15:44 Setting CNN weigths from save/fncnn6_reset_word_coco_tword009_idf10_a03_topdown/model-cnn-best.pth[0m
[33m01/27 11:15:46 Finetuning up from 6 modules in the cnn[0m
[33m01/27 11:15:47 Top Down : OrderedDict([('embed_1', Embedding(9488, 512)), ('embed', Sequential (
  (0): Embedding(9488, 512)
  (1): ReLU ()
  (2): Dropout (p = 0.5)
)), ('att_embed', Sequential (
  (0): Linear (2048 -> 512)
  (1): ReLU ()
  (2): Dropout (p = 0.0)
)), ('logit', Linear (512 -> 9488)), ('ctx2att', Linear (512 -> 512)), ('fc_embed', Sequential (
  (0): Linear (2048 -> 512)
  (1): ReLU ()
  (2): Dropout (p = 0.0)
)), ('core', TopDownCore (
  (att_lstm): LSTMCell(1536, 512)
  (lang_lstm): LSTMCell(1024, 512)
  (attention): Attention (
    (h2att): Linear (512 -> 512)
    (alpha_net): Linear (512 -> 1)
  )
))])[0m
[33m01/27 11:15:47 Loading the model dict (last checkpoint) ['embed_1.weight', 'embed.0.weight', 'att_embed.0.weight', 'att_embed.0.bias', 'logit.weight', 'logit.bias', 'ctx2att.weight', 'ctx2att.bias', 'fc_embed.0.weight', 'fc_embed.0.bias', 'core.att_lstm.weight_ih', 'core.att_lstm.weight_hh', 'core.att_lstm.bias_ih', 'core.att_lstm.bias_hh', 'core.lang_lstm.weight_ih', 'core.lang_lstm.weight_hh', 'core.lang_lstm.bias_ih', 'core.lang_lstm.bias_hh', 'core.attention.h2att.weight', 'core.attention.h2att.bias', 'core.attention.alpha_net.weight', 'core.attention.alpha_net.bias'][0m
[33m01/27 11:15:47 DataLoader loading json file: data/coco/cocotalk.json[0m
[33m01/27 11:15:47 vocab size is 9487 [0m
[33m01/27 11:15:47 DataLoader loading h5 file: data/coco/cocotalk.h5[0m
[33m01/27 11:15:47 read 123287 images of size 3x256x256[0m
[33m01/27 11:15:47 max sequence length in data is 16[0m
[33m01/27 11:15:47 assigned 82783 images to split train[0m
[33m01/27 11:15:47 assigned 5000 images to split val[0m
[33m01/27 11:15:47 assigned 5000 images to split test[0m
[32m01/27 11:15:49 Initialized Word2 loss tau=0.090, alpha=0.3[0m
[33m01/27 11:15:49 Evaluating the test split (-1)[0m
## OAR [2018-01-27 11:20:03] Job 139643 KILLED ##
[33m01/27 11:20:28 GPU ID: 0 | available memory: 11172M[0m
[33m01/27 11:20:33 Starting from the best saved model[0m
[33m01/27 11:20:33 Starting from save/fncnn6_reset_word_coco_tword009_idf10_a03_topdown/model-best.pth[0m
[36m01/27 11:20:38 Setting CNN weigths from save/fncnn6_reset_word_coco_tword009_idf10_a03_topdown/model-cnn-best.pth[0m
[33m01/27 11:20:41 Finetuning up from 6 modules in the cnn[0m
[33m01/27 11:20:42 Top Down : OrderedDict([('embed_1', Embedding(9488, 512)), ('embed', Sequential (
  (0): Embedding(9488, 512)
  (1): ReLU ()
  (2): Dropout (p = 0.5)
)), ('att_embed', Sequential (
  (0): Linear (2048 -> 512)
  (1): ReLU ()
  (2): Dropout (p = 0.0)
)), ('logit', Linear (512 -> 9488)), ('ctx2att', Linear (512 -> 512)), ('fc_embed', Sequential (
  (0): Linear (2048 -> 512)
  (1): ReLU ()
  (2): Dropout (p = 0.0)
)), ('core', TopDownCore (
  (att_lstm): LSTMCell(1536, 512)
  (lang_lstm): LSTMCell(1024, 512)
  (attention): Attention (
    (h2att): Linear (512 -> 512)
    (alpha_net): Linear (512 -> 1)
  )
))])[0m
[33m01/27 11:20:43 Loading the model dict (last checkpoint) ['embed_1.weight', 'embed.0.weight', 'att_embed.0.weight', 'att_embed.0.bias', 'logit.weight', 'logit.bias', 'ctx2att.weight', 'ctx2att.bias', 'fc_embed.0.weight', 'fc_embed.0.bias', 'core.att_lstm.weight_ih', 'core.att_lstm.weight_hh', 'core.att_lstm.bias_ih', 'core.att_lstm.bias_hh', 'core.lang_lstm.weight_ih', 'core.lang_lstm.weight_hh', 'core.lang_lstm.bias_ih', 'core.lang_lstm.bias_hh', 'core.attention.h2att.weight', 'core.attention.h2att.bias', 'core.attention.alpha_net.weight', 'core.attention.alpha_net.bias'][0m
[33m01/27 11:20:43 DataLoader loading json file: data/coco/cocotalk.json[0m
[33m01/27 11:20:43 vocab size is 9487 [0m
[33m01/27 11:20:43 DataLoader loading h5 file: data/coco/cocotalk.h5[0m
[33m01/27 11:20:43 read 123287 images of size 3x256x256[0m
[33m01/27 11:20:43 max sequence length in data is 16[0m
[33m01/27 11:20:43 assigned 82783 images to split train[0m
[33m01/27 11:20:43 assigned 5000 images to split val[0m
[33m01/27 11:20:43 assigned 5000 images to split test[0m
[32m01/27 11:20:50 Initialized Word2 loss tau=0.090, alpha=0.3[0m
[33m01/27 11:20:50 Evaluating the test split (-1)[0m
## OAR [2018-01-27 11:25:04] Job 139644 KILLED ##
[33m01/27 11:25:26 GPU ID: 0 | available memory: 11172M[0m
[33m01/27 11:25:32 Starting from the best saved model[0m
[33m01/27 11:25:32 Starting from save/fncnn6_reset_word_coco_tword009_idf10_a03_topdown/model-best.pth[0m
[36m01/27 11:25:36 Setting CNN weigths from save/fncnn6_reset_word_coco_tword009_idf10_a03_topdown/model-cnn-best.pth[0m
[33m01/27 11:25:39 Finetuning up from 6 modules in the cnn[0m
[33m01/27 11:25:39 Top Down : OrderedDict([('embed_1', Embedding(9488, 512)), ('embed', Sequential (
  (0): Embedding(9488, 512)
  (1): ReLU ()
  (2): Dropout (p = 0.5)
)), ('att_embed', Sequential (
  (0): Linear (2048 -> 512)
  (1): ReLU ()
  (2): Dropout (p = 0.0)
)), ('logit', Linear (512 -> 9488)), ('ctx2att', Linear (512 -> 512)), ('fc_embed', Sequential (
  (0): Linear (2048 -> 512)
  (1): ReLU ()
  (2): Dropout (p = 0.0)
)), ('core', TopDownCore (
  (att_lstm): LSTMCell(1536, 512)
  (lang_lstm): LSTMCell(1024, 512)
  (attention): Attention (
    (h2att): Linear (512 -> 512)
    (alpha_net): Linear (512 -> 1)
  )
))])[0m
[33m01/27 11:25:39 Loading the model dict (last checkpoint) ['embed_1.weight', 'embed.0.weight', 'att_embed.0.weight', 'att_embed.0.bias', 'logit.weight', 'logit.bias', 'ctx2att.weight', 'ctx2att.bias', 'fc_embed.0.weight', 'fc_embed.0.bias', 'core.att_lstm.weight_ih', 'core.att_lstm.weight_hh', 'core.att_lstm.bias_ih', 'core.att_lstm.bias_hh', 'core.lang_lstm.weight_ih', 'core.lang_lstm.weight_hh', 'core.lang_lstm.bias_ih', 'core.lang_lstm.bias_hh', 'core.attention.h2att.weight', 'core.attention.h2att.bias', 'core.attention.alpha_net.weight', 'core.attention.alpha_net.bias'][0m
[33m01/27 11:25:39 DataLoader loading json file: data/coco/cocotalk.json[0m
[33m01/27 11:25:39 vocab size is 9487 [0m
[33m01/27 11:25:39 DataLoader loading h5 file: data/coco/cocotalk.h5[0m
[33m01/27 11:25:39 read 123287 images of size 3x256x256[0m
[33m01/27 11:25:39 max sequence length in data is 16[0m
[33m01/27 11:25:39 assigned 82783 images to split train[0m
[33m01/27 11:25:39 assigned 5000 images to split val[0m
[33m01/27 11:25:39 assigned 5000 images to split test[0m
[32m01/27 11:25:41 Initialized Word2 loss tau=0.090, alpha=0.3[0m
[33m01/27 11:25:41 Evaluating the test split (-1)[0m
## OAR [2018-01-27 11:30:03] Job 139645 KILLED ##
[33m01/27 11:30:26 GPU ID: 0 | available memory: 11172M[0m
[33m01/27 11:30:31 Starting from the best saved model[0m
[33m01/27 11:30:31 Starting from save/fncnn6_reset_word_coco_tword009_idf10_a03_topdown/model-best.pth[0m
[36m01/27 11:30:35 Setting CNN weigths from save/fncnn6_reset_word_coco_tword009_idf10_a03_topdown/model-cnn-best.pth[0m
[33m01/27 11:30:37 Finetuning up from 6 modules in the cnn[0m
[33m01/27 11:30:38 Top Down : OrderedDict([('embed_1', Embedding(9488, 512)), ('embed', Sequential (
  (0): Embedding(9488, 512)
  (1): ReLU ()
  (2): Dropout (p = 0.5)
)), ('att_embed', Sequential (
  (0): Linear (2048 -> 512)
  (1): ReLU ()
  (2): Dropout (p = 0.0)
)), ('logit', Linear (512 -> 9488)), ('ctx2att', Linear (512 -> 512)), ('fc_embed', Sequential (
  (0): Linear (2048 -> 512)
  (1): ReLU ()
  (2): Dropout (p = 0.0)
)), ('core', TopDownCore (
  (att_lstm): LSTMCell(1536, 512)
  (lang_lstm): LSTMCell(1024, 512)
  (attention): Attention (
    (h2att): Linear (512 -> 512)
    (alpha_net): Linear (512 -> 1)
  )
))])[0m
[33m01/27 11:30:39 Loading the model dict (last checkpoint) ['embed_1.weight', 'embed.0.weight', 'att_embed.0.weight', 'att_embed.0.bias', 'logit.weight', 'logit.bias', 'ctx2att.weight', 'ctx2att.bias', 'fc_embed.0.weight', 'fc_embed.0.bias', 'core.att_lstm.weight_ih', 'core.att_lstm.weight_hh', 'core.att_lstm.bias_ih', 'core.att_lstm.bias_hh', 'core.lang_lstm.weight_ih', 'core.lang_lstm.weight_hh', 'core.lang_lstm.bias_ih', 'core.lang_lstm.bias_hh', 'core.attention.h2att.weight', 'core.attention.h2att.bias', 'core.attention.alpha_net.weight', 'core.attention.alpha_net.bias'][0m
[33m01/27 11:30:39 DataLoader loading json file: data/coco/cocotalk.json[0m
[33m01/27 11:30:39 vocab size is 9487 [0m
[33m01/27 11:30:39 DataLoader loading h5 file: data/coco/cocotalk.h5[0m
[33m01/27 11:30:39 read 123287 images of size 3x256x256[0m
[33m01/27 11:30:39 max sequence length in data is 16[0m
[33m01/27 11:30:39 assigned 82783 images to split train[0m
[33m01/27 11:30:39 assigned 5000 images to split val[0m
[33m01/27 11:30:39 assigned 5000 images to split test[0m
[32m01/27 11:30:41 Initialized Word2 loss tau=0.090, alpha=0.3[0m
[33m01/27 11:30:41 Evaluating the test split (-1)[0m
[33m01/27 11:34:51 Loading reference captions..[0m
[33m01/27 11:34:52 Reference captions loaded![0m
[33m01/27 11:34:54 using 5000/5000 predictions[0m
[33m01/27 11:34:54 Loading model captions[0m
[33m01/27 11:34:54 Model captions loaded[0m
[33m01/27 11:34:54 Starting evaluation...[0m
PTBTokenizer tokenized 307085 tokens at 1178654,31 tokens per second.
PTBTokenizer tokenized 51183 tokens at 186142,71 tokens per second.
## OAR [2018-01-27 11:35:03] Job 139646 KILLED ##
[33m01/27 11:35:25 GPU ID: 0 | available memory: 11172M[0m
[33m01/27 11:35:31 Starting from the best saved model[0m
[33m01/27 11:35:31 Starting from save/fncnn6_reset_word_coco_tword009_idf10_a03_topdown/model-best.pth[0m
[36m01/27 11:35:35 Setting CNN weigths from save/fncnn6_reset_word_coco_tword009_idf10_a03_topdown/model-cnn-best.pth[0m
[33m01/27 11:35:37 Finetuning up from 6 modules in the cnn[0m
[33m01/27 11:35:38 Top Down : OrderedDict([('embed_1', Embedding(9488, 512)), ('embed', Sequential (
  (0): Embedding(9488, 512)
  (1): ReLU ()
  (2): Dropout (p = 0.5)
)), ('att_embed', Sequential (
  (0): Linear (2048 -> 512)
  (1): ReLU ()
  (2): Dropout (p = 0.0)
)), ('logit', Linear (512 -> 9488)), ('ctx2att', Linear (512 -> 512)), ('fc_embed', Sequential (
  (0): Linear (2048 -> 512)
  (1): ReLU ()
  (2): Dropout (p = 0.0)
)), ('core', TopDownCore (
  (att_lstm): LSTMCell(1536, 512)
  (lang_lstm): LSTMCell(1024, 512)
  (attention): Attention (
    (h2att): Linear (512 -> 512)
    (alpha_net): Linear (512 -> 1)
  )
))])[0m
[33m01/27 11:35:38 Loading the model dict (last checkpoint) ['embed_1.weight', 'embed.0.weight', 'att_embed.0.weight', 'att_embed.0.bias', 'logit.weight', 'logit.bias', 'ctx2att.weight', 'ctx2att.bias', 'fc_embed.0.weight', 'fc_embed.0.bias', 'core.att_lstm.weight_ih', 'core.att_lstm.weight_hh', 'core.att_lstm.bias_ih', 'core.att_lstm.bias_hh', 'core.lang_lstm.weight_ih', 'core.lang_lstm.weight_hh', 'core.lang_lstm.bias_ih', 'core.lang_lstm.bias_hh', 'core.attention.h2att.weight', 'core.attention.h2att.bias', 'core.attention.alpha_net.weight', 'core.attention.alpha_net.bias'][0m
[33m01/27 11:35:38 DataLoader loading json file: data/coco/cocotalk.json[0m
[33m01/27 11:35:38 vocab size is 9487 [0m
[33m01/27 11:35:38 DataLoader loading h5 file: data/coco/cocotalk.h5[0m
[33m01/27 11:35:38 read 123287 images of size 3x256x256[0m
[33m01/27 11:35:38 max sequence length in data is 16[0m
[33m01/27 11:35:38 assigned 82783 images to split train[0m
[33m01/27 11:35:38 assigned 5000 images to split val[0m
[33m01/27 11:35:38 assigned 5000 images to split test[0m
[32m01/27 11:35:40 Initialized Word2 loss tau=0.090, alpha=0.3[0m
[33m01/27 11:35:40 Evaluating the test split (-1)[0m
[33m01/27 11:39:43 Loading reference captions..[0m
[33m01/27 11:39:44 Reference captions loaded![0m
[33m01/27 11:39:45 using 5000/5000 predictions[0m
[33m01/27 11:39:45 Loading model captions[0m
[33m01/27 11:39:46 Model captions loaded[0m
[33m01/27 11:39:46 Starting evaluation...[0m
PTBTokenizer tokenized 307085 tokens at 1221084,53 tokens per second.
PTBTokenizer tokenized 51183 tokens at 286371,67 tokens per second.
Parsing reference captions
Error: Could not score batched file input:
org.fusesource.lmdbjni.LMDBException: MDB_CURSOR_FULL: Internal error - cursor stack limit reached
	at org.fusesource.lmdbjni.Util.checkErrorCode(Util.java:44)
	at org.fusesource.lmdbjni.Database.get(Database.java:202)
	at org.fusesource.lmdbjni.Database.get(Database.java:193)
	at org.fusesource.lmdbjni.Database.get(Database.java:186)
	at org.fusesource.lmdbjni.Database.get(Database.java:161)
	at edu.anu.spice.LmdbTupleDB.getTransaction(LmdbTupleDB.java:96)
	at edu.anu.spice.SpiceParser.loadTuplesFromDB(SpiceParser.java:195)
	at edu.anu.spice.SpiceParser.loadTuples(SpiceParser.java:245)
	at edu.anu.spice.SpiceParser.parseCaptions(SpiceParser.java:251)
	at edu.anu.spice.SpiceScorer.scoreBatch(SpiceScorer.java:109)
	at edu.anu.spice.SpiceScorer.main(SpiceScorer.java:60)
[33m01/27 12:12:33 GPU ID: 2 | available memory: 16276M[0m
[33m01/27 12:12:45 Starting from the best saved model[0m
[33m01/27 12:12:45 Starting from save/fncnn6_reset_strat_rhamming_pool2_tsent03_a04_topdown/model-best.pth[0m
[36m01/27 12:12:49 Setting CNN weigths from save/fncnn6_res[33m01/27 12:12:55 Starting from the best saved model[0m
[33m01/27 12:12:55 Starting from save/strat_lazy_rhamming_pool2_tsent03_a04_topdown/model-best.pth[0m
[36m01/27 12:13:01 Setting CNN weigths from save/strat_lazy_rhamming_pool2_tsent03_a04_topdown/model-cnn-best.pth[0m
 Dropout (p = 0.5)
)), ('att_embed', Sequential (
  (0): Linear (2048 -> 512)
  (1): ReLU ()
  (2): Dropout (p = 0.0)
)), ('logit', Linear (512 -> 9488)), ('ctx2att', Linear (512 -> 512)), ('fc_embed', Sequential (
  (0): Linear (2048 -> 512)
  (1): ReLU ()
  (2): Dropout (p = 0.0)
)), ('core', TopDownCore (
  (att_lstm): LSTMCell(1536, 512)
  (lang_lstm): LSTMCell(1024, 512)
  (attention): Attention (
    (h2att): Linear (512 -> 512)
    (alpha_net): Linear (512 -> 1)
  )
))])[0m
[33m01/27 12:12:53 Loading the model dict (last checkpoint) ['embed_1.weight', 'embed.0.weight', 'att_embed.0.weight', 'att_embed.0.bias', 'logit.weight', 'logit.bias', 'ctx2att.weight', 'ctx2att.bias', 'fc_embed.0.weight', 'fc_embed.0.bias', 'core.att_lstm.weight_ih', 'core.att_lstm.weight_hh', 'core.att_lstm.bias_ih', 'core.att_lstm.bias_hh', 'core.lang_lstm.weight_ih', 'core.lang_lstm.weight_hh', 'core.lang_lstm.bias_ih', 'core.lang_lstm.bias_hh', 'core.attention.h2att.weight', 'core.attention.h2att.bias', 'core.attention.alpha_net.weight', 'core.attention.alpha_net.bias'][0m
[33m01/27 12:12:53 DataLoader loading json file: data/coco/cocotalk.json[0m
[33m01/27 12:12:54 vocab size is 9487 [0m
[33m01/27 12:12:54 DataLoader loading h5 file: data/coco/cocotalk.h5[0m
[33m01/27 12:12:54 read 123287 images of size 3x256x256[0m
[33m01/27 12:12:54 max sequence length in data is 16[0m
[33m01/27 12:12:54 assigned 82783 images to split train[0m
[33m01/27 12:12:54 assigned 5000 images to split val[0m
[33m01/27 12:12:54 assigned 5000 images to split test[0m
[32m01/27 12:12:54 RewardSampler (stratified sampling), r=Hamming (Vpool=2, tau=0.30)[0m
[33m01/27 12:12:54 Evaluating the test split (-1)[0m
[33m01/27 12:13:09 Finetuning up from 0 modules in the cnn[0m
[33m01/27 12:13:11 Top Down : OrderedDict([('embed_1', Embedding(9488, 512)), ('embed', Sequential (
  (0): Embedding(9488, 512)
  (1): ReLU ()
  (2): Dropout (p = 0.5)
)), ('att_embed', Sequential (
  (0): Linear (2048 -> 512)
  (1): ReLU ()
  (2): Dropout (p = 0.0)
)), ('logit', Linear (512 -> 9488)), ('ctx2att', Linear (512 -> 512)), ('fc_embed', Sequential (
  (0): Linear (2048 -> 512)
  (1): ReLU ()
  (2): Dropout (p = 0.0)
)), ('core', TopDownCore (
  (att_lstm): LSTMCell(1536, 512)
  (lang_lstm): LSTMCell(1024, 512)
  (attention): Attention (
    (h2att): Linear (512 -> 512)
    (alpha_net): Linear (512 -> 1)
  )
))])[0m
[33m01/27 12:13:13 Loading the model dict (last checkpoint) ['embed_1.weight', 'embed.0.weight', 'att_embed.0.weight', 'att_embed.0.bias', 'logit.weight', 'logit.bias', 'ctx2att.weight', 'ctx2att.bias', 'fc_embed.0.weight', 'fc_embed.0.bias', 'core.att_lstm.weight_ih', 'core.att_lstm.weight_hh', 'core.att_lstm.bias_ih', 'core.att_lstm.bias_hh', 'core.lang_lstm.weight_ih', 'core.lang_lstm.weight_hh', 'core.lang_lstm.bias_ih', 'core.lang_lstm.bias_hh', 'core.attention.h2att.weight', 'core.attention.h2att.bias', 'core.attention.alpha_net.weight', 'core.attention.alpha_net.bias'][0m
[33m01/27 12:13:13 DataLoader loading json file: data/coco/cocotalk.json[0m
[33m01/27 12:13:13 vocab size is 9487 [0m
[33m01/27 12:13:13 DataLoader loading h5 file: data/coco/cocotalk.h5[0m
[33m01/27 12:13:13 read 123287 images of size 3x256x256[0m
[33m01/27 12:13:13 max sequence length in data is 16[0m
[33m01/27 12:13:13 assigned 82783 images to split train[0m
[33m01/27 12:13:13 assigned 5000 images to split val[0m
[33m01/27 12:13:13 assigned 5000 images to split test[0m
[32m01/27 12:13:13 RewardSampler (stratified sampling), r=Hamming (Vpool=2, tau=0.30)[0m
[33m01/27 12:13:13 Evaluating the test split (-1)[0m
[33m01/27 12:17:54 Loading reference captions..[0m
[33m01/27 12:17:55 Reference captions loaded![0m
[33m01/27 12:17:57 using 5000/5000 predictions[0m
[33m01/27 12:17:57 Loading model captions[0m
[33m01/27 12:17:57 Model captions loaded[0m
[33m01/27 12:17:57 Starting evaluation...[0m
PTBTokenizer tokenized 307085 tokens at 1261297.64 tokens per second.
PTBTokenizer tokenized 51948 tokens at 417185.10 tokens per second.
Parsing reference captions
Error: Could not score batched file input:
org.fusesource.lmdbjni.LMDBException: MDB_CURSOR_FULL: Internal error - cursor stack limit reached
	at org.fusesource.lmdbjni.Util.checkErrorCode(Util.java:44)
	at org.fusesource.lmdbjni.Database.get(Database.java:202)
	at org.fusesource.lmdbjni.Database.get(Database.java:193)
	at org.fusesource.lmdbjni.Database.get(Database.java:186)
	at org.fusesource.lmdbjni.Database.get(Database.java:161)
	at edu.anu.spice.LmdbTupleDB.getTransaction(LmdbTupleDB.java:96)
	at edu.anu.spice.SpiceParser.loadTuplesFromDB(SpiceParser.java:195)
	at edu.anu.spice.SpiceParser.loadTuples(SpiceParser.java:245)
	at edu.anu.spice.SpiceParser.parseCaptions(SpiceParser.java:251)
	at edu.anu.spice.SpiceScorer.scoreBatch(SpiceScorer.java:109)
	at edu.anu.spice.SpiceScorer.main(SpiceScorer.java:60)
[33m01/27 12:19:25 Loading reference captions..[0m
[33m01/27 12:19:25 Reference captions loaded![0m
[33m01/27 12:19:28 using 5000/5000 predictions[0m
[33m01/27 12:19:28 Loading model captions[0m
[33m01/27 12:19:28 Model captions loaded[0m
[33m01/27 12:19:28 Starting evaluation...[0m
PTBTokenizer tokenized 307085 tokens at 1074725.04 tokens per second.
PTBTokenizer tokenized 51012 tokens at 336175.64 tokens per second.
Parsing reference captions
Error: Could not score batched file input:
org.fusesource.lmdbjni.LMDBException: MDB_CURSOR_FULL: Internal error - cursor stack limit reached
	at org.fusesource.lmdbjni.Util.checkErrorCode(Util.java:44)
	at org.fusesource.lmdbjni.Database.get(Database.java:202)
	at org.fusesource.lmdbjni.Database.get(Database.java:193)
	at org.fusesource.lmdbjni.Database.get(Database.java:186)
	at org.fusesource.lmdbjni.Database.get(Database.java:161)
	at edu.anu.spice.LmdbTupleDB.getTransaction(LmdbTupleDB.java:96)
	at edu.anu.spice.SpiceParser.loadTuplesFromDB(SpiceParser.java:195)
	at edu.anu.spice.SpiceParser.loadTuples(SpiceParser.java:245)
	at edu.anu.spice.SpiceParser.parseCaptions(SpiceParser.java:251)
	at edu.anu.spice.SpiceScorer.scoreBatch(SpiceScorer.java:109)
	at edu.anu.spice.SpiceScorer.main(SpiceScorer.java:60)
[33m01/27 17:33:00 GPU ID: 0 | available memory: 12207M[0m
[33m01/27 17:34:22 Starting from the best saved model[0m
[33m01/27 17:34:22 Starting from save/fncnn6_reset_importance_qhamming_limited2_tsent03_rcider_tsent05_a04_topdown/model-best.pth[0m
[36m01/27 17:34:27 Setting CNN weigths from save/fncnn6_reset_importance_qhamming_limited2_tsent03_rcider_tsent05_a04_topdown/model-cnn-best.pth[0m
[33m01/27 17:35:19 Finetuning up from 6 modules in the cnn[0m
[33m01/27 17:35:20 Top Down : OrderedDict([('embed_1', Embedding(9488, 512)), ('embed', Sequential (
  (0): Embedding(9488, 512)
  (1): ReLU ()
  (2): Dropout (p = 0.5)
)), ('att_embed', Sequential (
  (0): Linear (2048 -> 512)
  (1): ReLU ()
  (2): Dropout (p = 0.0)
)), ('logit', Linear (512 -> 9488)), ('ctx2att', Linear (512 -> 512)), ('fc_embed', Sequential (
  (0): Linear (2048 -> 512)
  (1): ReLU ()
  (2): Dropout (p = 0.0)
)), ('core', TopDownCore (
  (att_lstm): LSTMCell(1536, 512)
  (lang_lstm): LSTMCell(1024, 512)
  (attention): Attention (
    (h2att): Linear (512 -> 512)
    (alpha_net): Linear (512 -> 1)
  )
))])[0m
[33m01/27 17:35:21 Loading the model dict (last checkpoint) ['embed_1.weight', 'embed.0.weight', 'att_embed.0.weight', 'att_embed.0.bias', 'logit.weight', 'logit.bias', 'ctx2att.weight', 'ctx2att.bias', 'fc_embed.0.weight', 'fc_embed.0.bias', 'core.att_lstm.weight_ih', 'core.att_lstm.weight_hh', 'core.att_lstm.bias_ih', 'core.att_lstm.bias_hh', 'core.lang_lstm.weight_ih', 'core.lang_lstm.weight_hh', 'core.lang_lstm.bias_ih', 'core.lang_lstm.bias_hh', 'core.attention.h2att.weight', 'core.attention.h2att.bias', 'core.attention.alpha_net.weight', 'core.attention.alpha_net.bias'][0m
[33m01/27 17:35:21 DataLoader loading json file: data/coco/cocotalk.json[0m
[33m01/27 17:35:22 vocab size is 9487 [0m
[33m01/27 17:35:22 DataLoader loading h5 file: data/coco/cocotalk.h5[0m
[33m01/27 17:35:22 read 123287 images of size 3x256x256[0m
[33m01/27 17:35:22 max sequence length in data is 16[0m
[33m01/27 17:35:22 assigned 82783 images to split train[0m
[33m01/27 17:35:22 assigned 5000 images to split val[0m
[33m01/27 17:35:22 assigned 5000 images to split test[0m
[32m01/27 17:35:31 using importance sampling r=CIDEr (tau=0.50) and q=Hamming (Vpool=2, tau=0.30)[0m
[33m01/27 17:35:31 Evaluating the test split (-1)[0m
[33m01/27 17:45:13 Loading reference captions..[0m
[33m01/27 17:45:17 Reference captions loaded![0m
[33m01/27 17:45:19 using 5000/5000 predictions[0m
[33m01/27 17:45:19 Loading model captions[0m
[33m01/27 17:45:20 Model captions loaded[0m
[33m01/27 17:45:20 Starting evaluation...[0m
PTBTokenizer tokenized 307085 tokens at 1174117.83 tokens per second.
PTBTokenizer tokenized 51866 tokens at 320774.07 tokens per second.
Parsing reference captions
Error: Could not score batched file input:
org.fusesource.lmdbjni.LMDBException: MDB_CURSOR_FULL: Internal error - cursor stack limit reached
	at org.fusesource.lmdbjni.Util.checkErrorCode(Util.java:44)
	at org.fusesource.lmdbjni.Database.get(Database.java:202)
	at org.fusesource.lmdbjni.Database.get(Database.java:193)
	at org.fusesource.lmdbjni.Database.get(Database.java:186)
	at org.fusesource.lmdbjni.Database.get(Database.java:161)
	at edu.anu.spice.LmdbTupleDB.getTransaction(LmdbTupleDB.java:96)
	at edu.anu.spice.SpiceParser.loadTuplesFromDB(SpiceParser.java:195)
	at edu.anu.spice.SpiceParser.loadTuples(SpiceParser.java:245)
	at edu.anu.spice.SpiceParser.parseCaptions(SpiceParser.java:251)
	at edu.anu.spice.SpiceScorer.scoreBatch(SpiceScorer.java:109)
	at edu.anu.spice.SpiceScorer.main(SpiceScorer.java:60)
[33m01/27 22:35:43 GPU ID: 5 | available memory: 16276M[0m
[33m01/27 22:35:51 Starting from the best saved model[0m
[33m01/27 22:35:51 Starting from save/fncnn6_reset_strat_rhamming_pool0_tsent01_a04_showtell152/model-best.pth[0m
[36m01/27 22:35:55 Setting CNN weigths from save/fncnn6_reset_strat_rhamming_pool0_tsent01_a04_showtell152/model-cnn-best.pth[0m
[33m01/27 22:35:59 Finetuning up from 6 modules in the cnn[0m
[33m01/27 22:36:00 Show & Tell : OrderedDict([('img_embed', Linear (2048 -> 512)), ('core', LSTM(512, 512, bias=False, dropout=0.5)), ('embed', Embedding(9488, 512)), ('drop_x_lm', Dropout (p = 0.5)), ('logit', Linear (512 -> 9488))])[0m
[33m01/27 22:36:00 Loading the model dict (last checkpoint) ['img_embed.weight', 'img_embed.bias', 'core.weight_ih_l0', 'core.weight_hh_l0', 'embed.weight', 'logit.weight', 'logit.bias'][0m
[33m01/27 22:36:01 DataLoader loading json file: data/coco/cocotalk.json[0m
[33m01/27 22:36:01 vocab size is 9487 [0m
[33m01/27 22:36:01 DataLoader loading h5 file: data/coco/cocotalk.h5[0m
[33m01/27 22:36:01 read 123287 images of size 3x256x256[0m
[33m01/27 22:36:01 max sequence length in data is 16[0m
[33m01/27 22:36:01 assigned 82783 images to split train[0m
[33m01/27 22:36:01 assigned 5000 images to split val[0m
[33m01/27 22:36:01 assigned 5000 images to split test[0m
[32m01/27 22:36:01 RewardSampler (stratified sampling), r=Hamming (Vpool=0, tau=0.10)[0m
[33m01/27 22:36:01 Evaluating the test split (-1)[0m
## OAR [2018-01-27 22:40:03] Job 139915 KILLED ##
[33m01/27 22:40:21 GPU ID: 5 | available memory: 16276M[0m
[33m01/27 22:40:28 Starting from the best saved model[0m
[33m01/27 22:40:28 Starting from save/fncnn6_reset_ml_penalize01_showtell152/model-best.pth[0m
[36m01/27 22:40:33 Setting CNN weigths from save/fncnn6_reset_ml_penalize01_showtell152/model-cnn-best.pth[0m
[33m01/27 22:40:36 Finetuning up from 6 modules in the cnn[0m
[33m01/27 22:40:37 Show & Tell : OrderedDict([('img_embed', Linear (2048 -> 512)), ('core', LSTM(512, 512, bias=False, dropout=0.5)), ('embed', Embedding(9488, 512)), ('drop_x_lm', Dropout (p = 0.5)), ('logit', Linear (512 -> 9488))])[0m
[33m01/27 22:40:37 Loading the model dict (last checkpoint) ['img_embed.weight', 'img_embed.bias', 'core.weight_ih_l0', 'core.weight_hh_l0', 'embed.weight', 'logit.weight', 'logit.bias'][0m
[33m01/27 22:40:38 DataLoader loading json file: data/coco/cocotalk.json[0m
[33m01/27 22:40:38 vocab size is 9487 [0m
[33m01/27 22:40:38 DataLoader loading h5 file: data/coco/cocotalk.h5[0m
[33m01/27 22:40:38 read 123287 images of size 3x256x256[0m
[33m01/27 22:40:38 max sequence length in data is 16[0m
[33m01/27 22:40:38 assigned 82783 images to split train[0m
[33m01/27 22:40:38 assigned 5000 images to split val[0m
[33m01/27 22:40:38 assigned 5000 images to split test[0m
[32m01/27 22:40:38 Default ML loss[0m
[33m01/27 22:40:38 Evaluating the test split (-1)[0m
[33m01/27 22:44:08 GPU ID: 0 | available memory: 16276M[0m
[33m01/27 22:44:15 Starting from the best saved model[0m
[33m01/27 22:44:15 Starting from save/fncnn6_reset_strat_rhamming_pool0_tsent01_a04_showtell152/model-best.pth[0m
[36m01/27 22:44:19 Setting CNN weigths from save/fncnn6_reset_strat_rhamming_pool0_tsent01_a04_showtell152/model-cnn-best.pth[0m
[33m01/27 22:44:22 Finetuning up from 6 modules in the cnn[0m
[33m01/27 22:44:23 Show & Tell : OrderedDict([('img_embed', Linear (2048 -> 512)), ('core', LSTM(512, 512, bias=False, dropout=0.5)), ('embed', Embedding(9488, 512)), ('drop_x_lm', Dropout (p = 0.5)), ('logit', Linear (512 -> 9488))])[0m
[33m01/27 22:44:23 Loading the model dict (last checkpoint) ['img_embed.weight', 'img_embed.bias', 'core.weight_ih_l0', 'core.weight_hh_l0', 'embed.weight', 'logit.weight', 'logit.bias'][0m
[33m01/27 22:44:23 DataLoader loading json file: data/coco/cocotalk.json[0m
[33m01/27 22:44:24 vocab size is 9487 [0m
[33m01/27 22:44:24 DataLoader loading h5 file: data/coco/cocotalk.h5[0m
[33m01/27 22:44:24 read 123287 images of size 3x256x256[0m
[33m01/27 22:44:24 max sequence length in data is 16[0m
[33m01/27 22:44:24 assigned 82783 images to split train[0m
[33m01/27 22:44:24 assigned 5000 images to split val[0m
[33m01/27 22:44:24 assigned 5000 images to split test[0m
[32m01/27 22:44:24 RewardSampler (stratified sampling), r=Hamming (Vpool=0, tau=0.10)[0m
[33m01/27 22:44:24 Evaluating the test split (-1)[0m
## OAR [2018-01-27 22:45:05] Job 139917 KILLED ##
[33m01/27 22:45:21 Loading reference captions..[0m
[33m01/27 22:45:22 Reference captions loaded![0m
[33m01/27 22:45:24 using 5000/5000 predictions[0m
[33m01/27 22:45:24 Loading model captions[0m
[33m01/27 22:45:24 Model captions loaded[0m
[33m01/27 22:45:24 Starting evaluation...[0m
PTBTokenizer tokenized 307085 tokens at 1199376.20 tokens per second.
PTBTokenizer tokenized 51910 tokens at 455622.96 tokens per second.
[33m01/27 22:45:31 GPU ID: 0 | available memory: 16276M[0m
[33m01/27 22:45:38 Starting from the best saved model[0m
[33m01/27 22:45:38 Starting from save/fncnn6_reset_strat_rhamming_pool0_tsent01_a04_showtell152/model-best.pth[0m
Parsing reference captions
Error: Could not score batched file input:
org.fusesource.lmdbjni.LMDBException: MDB_CURSOR_FULL: Internal error - cursor stack limit reached
	at org.fusesource.lmdbjni.Util.checkErrorCode(Util.java:44)
	at org.fusesource.lmdbjni.Database.get(Database.java:202)
	at org.fusesource.lmdbjni.Database.get(Database.java:193)
	at org.fusesource.lmdbjni.Database.get(Database.java:186)
	at org.fusesource.lmdbjni.Database.get(Database.java:161)
	at edu.anu.spice.LmdbTupleDB.getTransaction(LmdbTupleDB.java:96)
	at edu.anu.spice.SpiceParser.loadTuplesFromDB(SpiceParser.java:195)
	at edu.anu.spice.SpiceParser.loadTuples(SpiceParser.java:245)
	at edu.anu.spice.SpiceParser.parseCaptions(SpiceParser.java:251)
	at edu.anu.spice.SpiceScorer.scoreBatch(SpiceScorer.java:109)
	at edu.anu.spice.SpiceScorer.main(SpiceScorer.java:60)
[36m01/27 22:45:43 Setting CNN weigths from save/fncnn6_reset_strat_rhamming_pool0_tsent01_a04_showtell152/model-cnn-best.pth[0m
[33m01/27 22:45:46 Finetuning up from 6 modules in the cnn[0m
[33m01/27 22:45:46 Show & Tell : OrderedDict([('img_embed', Linear (2048 -> 512)), ('core', LSTM(512, 512, bias=False, dropout=0.5)), ('embed', Embedding(9488, 512)), ('drop_x_lm', Dropout (p = 0.5)), ('logit', Linear (512 -> 9488))])[0m
[33m01/27 22:45:46 Loading the model dict (last checkpoint) ['img_embed.weight', 'img_embed.bias', 'core.weight_ih_l0', 'core.weight_hh_l0', 'embed.weight', 'logit.weight', 'logit.bias'][0m
[33m01/27 22:45:47 DataLoader loading json file: data/coco/cocotalk.json[0m
[33m01/27 22:45:47 vocab size is 9487 [0m
[33m01/27 22:45:47 DataLoader loading h5 file: data/coco/cocotalk.h5[0m
[33m01/27 22:45:47 read 123287 images of size 3x256x256[0m
[33m01/27 22:45:47 max sequence length in data is 16[0m
[33m01/27 22:45:47 assigned 82783 images to split train[0m
[33m01/27 22:45:47 assigned 5000 images to split val[0m
[33m01/27 22:45:47 assigned 5000 images to split test[0m
[32m01/27 22:45:47 RewardSampler (stratified sampling), r=Hamming (Vpool=0, tau=0.10)[0m
[33m01/27 22:45:47 Evaluating the test split (-1)[0m
## OAR [2018-01-27 22:50:03] Job 139918 KILLED ##
[33m01/27 22:50:32 GPU ID: 0 | available memory: 16276M[0m
[33m01/27 22:50:39 Starting from the best saved model[0m
[33m01/27 22:50:39 Starting from save/fncnn6_reset_strat_rhamming_pool0_tsent01_a04_showtell152/model-best.pth[0m
[36m01/27 22:50:43 Setting CNN weigths from save/fncnn6_reset_strat_rhamming_pool0_tsent01_a04_showtell152/model-cnn-best.pth[0m
[33m01/27 22:50:46 Finetuning up from 6 modules in the cnn[0m
[33m01/27 22:50:47 Show & Tell : OrderedDict([('img_embed', Linear (2048 -> 512)), ('core', LSTM(512, 512, bias=False, dropout=0.5)), ('embed', Embedding(9488, 512)), ('drop_x_lm', Dropout (p = 0.5)), ('logit', Linear (512 -> 9488))])[0m
[33m01/27 22:50:47 Loading the model dict (last checkpoint) ['img_embed.weight', 'img_embed.bias', 'core.weight_ih_l0', 'core.weight_hh_l0', 'embed.weight', 'logit.weight', 'logit.bias'][0m
[33m01/27 22:50:48 DataLoader loading json file: data/coco/cocotalk.json[0m
[33m01/27 22:50:48 vocab size is 9487 [0m
[33m01/27 22:50:48 DataLoader loading h5 file: data/coco/cocotalk.h5[0m
[33m01/27 22:50:48 read 123287 images of size 3x256x256[0m
[33m01/27 22:50:48 max sequence length in data is 16[0m
[33m01/27 22:50:48 assigned 82783 images to split train[0m
[33m01/27 22:50:48 assigned 5000 images to split val[0m
[33m01/27 22:50:48 assigned 5000 images to split test[0m
[32m01/27 22:50:48 RewardSampler (stratified sampling), r=Hamming (Vpool=0, tau=0.10)[0m
[33m01/27 22:50:48 Evaluating the test split (-1)[0m
## OAR [2018-01-27 22:55:04] Job 139919 KILLED ##
[33m01/27 22:55:32 GPU ID: 0 | available memory: 16276M[0m
[33m01/27 22:55:39 Starting from the best saved model[0m
[33m01/27 22:55:39 Starting from save/fncnn6_reset_strat_rhamming_pool0_tsent01_a04_showtell152/model-best.pth[0m
[36m01/27 22:55:44 Setting CNN weigths from save/fncnn6_reset_strat_rhamming_pool0_tsent01_a04_showtell152/model-cnn-best.pth[0m
[33m01/27 22:55:47 Finetuning up from 6 modules in the cnn[0m
[33m01/27 22:55:47 Show & Tell : OrderedDict([('img_embed', Linear (2048 -> 512)), ('core', LSTM(512, 512, bias=False, dropout=0.5)), ('embed', Embedding(9488, 512)), ('drop_x_lm', Dropout (p = 0.5)), ('logit', Linear (512 -> 9488))])[0m
[33m01/27 22:55:47 Loading the model dict (last checkpoint) ['img_embed.weight', 'img_embed.bias', 'core.weight_ih_l0', 'core.weight_hh_l0', 'embed.weight', 'logit.weight', 'logit.bias'][0m
[33m01/27 22:55:48 DataLoader loading json file: data/coco/cocotalk.json[0m
[33m01/27 22:55:48 vocab size is 9487 [0m
[33m01/27 22:55:48 DataLoader loading h5 file: data/coco/cocotalk.h5[0m
[33m01/27 22:55:48 read 123287 images of size 3x256x256[0m
[33m01/27 22:55:48 max sequence length in data is 16[0m
[33m01/27 22:55:48 assigned 82783 images to split train[0m
[33m01/27 22:55:48 assigned 5000 images to split val[0m
[33m01/27 22:55:48 assigned 5000 images to split test[0m
[32m01/27 22:55:48 RewardSampler (stratified sampling), r=Hamming (Vpool=0, tau=0.10)[0m
[33m01/27 22:55:48 Evaluating the test split (-1)[0m
## OAR [2018-01-27 23:00:04] Job 139920 KILLED ##
[33m01/27 23:00:32 GPU ID: 0 | available memory: 16276M[0m
[33m01/27 23:00:40 Starting from the best saved model[0m
[33m01/27 23:00:40 Starting from save/fncnn6_reset_strat_rhamming_pool0_tsent01_a04_showtell152/model-best.pth[0m
[36m01/27 23:00:44 Setting CNN weigths from save/fncnn6_reset_strat_rhamming_pool0_tsent01_a04_showtell152/model-cnn-best.pth[0m
[33m01/27 23:00:47 Finetuning up from 6 modules in the cnn[0m
[33m01/27 23:00:48 Show & Tell : OrderedDict([('img_embed', Linear (2048 -> 512)), ('core', LSTM(512, 512, bias=False, dropout=0.5)), ('embed', Embedding(9488, 512)), ('drop_x_lm', Dropout (p = 0.5)), ('logit', Linear (512 -> 9488))])[0m
[33m01/27 23:00:48 Loading the model dict (last checkpoint) ['img_embed.weight', 'img_embed.bias', 'core.weight_ih_l0', 'core.weight_hh_l0', 'embed.weight', 'logit.weight', 'logit.bias'][0m
[33m01/27 23:00:48 DataLoader loading json file: data/coco/cocotalk.json[0m
[33m01/27 23:00:49 vocab size is 9487 [0m
[33m01/27 23:00:49 DataLoader loading h5 file: data/coco/cocotalk.h5[0m
[33m01/27 23:00:49 read 123287 images of size 3x256x256[0m
[33m01/27 23:00:49 max sequence length in data is 16[0m
[33m01/27 23:00:49 assigned 82783 images to split train[0m
[33m01/27 23:00:49 assigned 5000 images to split val[0m
[33m01/27 23:00:49 assigned 5000 images to split test[0m
[32m01/27 23:00:49 RewardSampler (stratified sampling), r=Hamming (Vpool=0, tau=0.10)[0m
[33m01/27 23:00:49 Evaluating the test split (-1)[0m
## OAR [2018-01-27 23:05:04] Job 139921 KILLED ##
[33m01/27 23:05:32 GPU ID: 0 | available memory: 16276M[0m
[33m01/27 23:05:39 Starting from the best saved model[0m
[33m01/27 23:05:39 Starting from save/fncnn6_reset_strat_rhamming_pool0_tsent01_a04_showtell152/model-best.pth[0m
[36m01/27 23:05:44 Setting CNN weigths from save/fncnn6_reset_strat_rhamming_pool0_tsent01_a04_showtell152/model-cnn-best.pth[0m
[33m01/27 23:05:47 Finetuning up from 6 modules in the cnn[0m
[33m01/27 23:05:47 Show & Tell : OrderedDict([('img_embed', Linear (2048 -> 512)), ('core', LSTM(512, 512, bias=False, dropout=0.5)), ('embed', Embedding(9488, 512)), ('drop_x_lm', Dropout (p = 0.5)), ('logit', Linear (512 -> 9488))])[0m
[33m01/27 23:05:47 Loading the model dict (last checkpoint) ['img_embed.weight', 'img_embed.bias', 'core.weight_ih_l0', 'core.weight_hh_l0', 'embed.weight', 'logit.weight', 'logit.bias'][0m
[33m01/27 23:05:48 DataLoader loading json file: data/coco/cocotalk.json[0m
[33m01/27 23:05:48 vocab size is 9487 [0m
[33m01/27 23:05:48 DataLoader loading h5 file: data/coco/cocotalk.h5[0m
[33m01/27 23:05:48 read 123287 images of size 3x256x256[0m
[33m01/27 23:05:48 max sequence length in data is 16[0m
[33m01/27 23:05:48 assigned 82783 images to split train[0m
[33m01/27 23:05:48 assigned 5000 images to split val[0m
[33m01/27 23:05:48 assigned 5000 images to split test[0m
[32m01/27 23:05:48 RewardSampler (stratified sampling), r=Hamming (Vpool=0, tau=0.10)[0m
[33m01/27 23:05:48 Evaluating the test split (-1)[0m
## OAR [2018-01-27 23:10:03] Job 139922 KILLED ##
[33m01/27 23:10:31 GPU ID: 0 | available memory: 16276M[0m
[33m01/27 23:10:39 Starting from the best saved model[0m
[33m01/27 23:10:39 Starting from save/fncnn6_reset_strat_rhamming_pool0_tsent01_a04_showtell152/model-best.pth[0m
[36m01/27 23:10:43 Setting CNN weigths from save/fncnn6_reset_strat_rhamming_pool0_tsent01_a04_showtell152/model-cnn-best.pth[0m
[33m01/27 23:10:46 Finetuning up from 6 modules in the cnn[0m
[33m01/27 23:10:46 Show & Tell : OrderedDict([('img_embed', Linear (2048 -> 512)), ('core', LSTM(512, 512, bias=False, dropout=0.5)), ('embed', Embedding(9488, 512)), ('drop_x_lm', Dropout (p = 0.5)), ('logit', Linear (512 -> 9488))])[0m
[33m01/27 23:10:46 Loading the model dict (last checkpoint) ['img_embed.weight', 'img_embed.bias', 'core.weight_ih_l0', 'core.weight_hh_l0', 'embed.weight', 'logit.weight', 'logit.bias'][0m
[33m01/27 23:10:47 DataLoader loading json file: data/coco/cocotalk.json[0m
[33m01/27 23:10:47 vocab size is 9487 [0m
[33m01/27 23:10:47 DataLoader loading h5 file: data/coco/cocotalk.h5[0m
[33m01/27 23:10:47 read 123287 images of size 3x256x256[0m
[33m01/27 23:10:47 max sequence length in data is 16[0m
[33m01/27 23:10:47 assigned 82783 images to split train[0m
[33m01/27 23:10:47 assigned 5000 images to split val[0m
[33m01/27 23:10:47 assigned 5000 images to split test[0m
[32m01/27 23:10:47 RewardSampler (stratified sampling), r=Hamming (Vpool=0, tau=0.10)[0m
[33m01/27 23:10:47 Evaluating the test split (-1)[0m
## OAR [2018-01-27 23:15:04] Job 139923 KILLED ##
[33m01/27 23:15:44 GPU ID: 0 | available memory: 16276M[0m
[33m01/27 23:15:51 Starting from the best saved model[0m
[33m01/27 23:15:51 Starting from save/fncnn6_reset_strat_rhamming_pool0_tsent01_a04_showtell152/model-best.pth[0m
[36m01/27 23:15:56 Setting CNN weigths from save/fncnn6_reset_strat_rhamming_pool0_tsent01_a04_showtell152/model-cnn-best.pth[0m
[33m01/27 23:15:59 Finetuning up from 6 modules in the cnn[0m
[33m01/27 23:15:59 Show & Tell : OrderedDict([('img_embed', Linear (2048 -> 512)), ('core', LSTM(512, 512, bias=False, dropout=0.5)), ('embed', Embedding(9488, 512)), ('drop_x_lm', Dropout (p = 0.5)), ('logit', Linear (512 -> 9488))])[0m
[33m01/27 23:15:59 Loading the model dict (last checkpoint) ['img_embed.weight', 'img_embed.bias', 'core.weight_ih_l0', 'core.weight_hh_l0', 'embed.weight', 'logit.weight', 'logit.bias'][0m
[33m01/27 23:16:00 DataLoader loading json file: data/coco/cocotalk.json[0m
[33m01/27 23:16:00 vocab size is 9487 [0m
[33m01/27 23:16:00 DataLoader loading h5 file: data/coco/cocotalk.h5[0m
[33m01/27 23:16:00 read 123287 images of size 3x256x256[0m
[33m01/27 23:16:00 max sequence length in data is 16[0m
[33m01/27 23:16:00 assigned 82783 images to split train[0m
[33m01/27 23:16:00 assigned 5000 images to split val[0m
[33m01/27 23:16:00 assigned 5000 images to split test[0m
[32m01/27 23:16:00 RewardSampler (stratified sampling), r=Hamming (Vpool=0, tau=0.10)[0m
[33m01/27 23:16:00 Evaluating the test split (-1)[0m
## OAR [2018-01-27 23:20:04] Job 139996 KILLED ##
[33m01/27 23:20:42 GPU ID: 0 | available memory: 16276M[0m
[33m01/27 23:20:50 Starting from the best saved model[0m
[33m01/27 23:20:50 Starting from save/fncnn6_reset_strat_rhamming_pool0_tsent01_a04_showtell152/model-best.pth[0m
[36m01/27 23:20:54 Setting CNN weigths from save/fncnn6_reset_strat_rhamming_pool0_tsent01_a04_showtell152/model-cnn-best.pth[0m
[33m01/27 23:20:57 Finetuning up from 6 modules in the cnn[0m
[33m01/27 23:20:57 Show & Tell : OrderedDict([('img_embed', Linear (2048 -> 512)), ('core', LSTM(512, 512, bias=False, dropout=0.5)), ('embed', Embedding(9488, 512)), ('drop_x_lm', Dropout (p = 0.5)), ('logit', Linear (512 -> 9488))])[0m
[33m01/27 23:20:57 Loading the model dict (last checkpoint) ['img_embed.weight', 'img_embed.bias', 'core.weight_ih_l0', 'core.weight_hh_l0', 'embed.weight', 'logit.weight', 'logit.bias'][0m
[33m01/27 23:20:58 DataLoader loading json file: data/coco/cocotalk.json[0m
[33m01/27 23:20:58 vocab size is 9487 [0m
[33m01/27 23:20:58 DataLoader loading h5 file: data/coco/cocotalk.h5[0m
[33m01/27 23:20:58 read 123287 images of size 3x256x256[0m
[33m01/27 23:20:58 max sequence length in data is 16[0m
[33m01/27 23:20:58 assigned 82783 images to split train[0m
[33m01/27 23:20:58 assigned 5000 images to split val[0m
[33m01/27 23:20:58 assigned 5000 images to split test[0m
[32m01/27 23:20:58 RewardSampler (stratified sampling), r=Hamming (Vpool=0, tau=0.10)[0m
[33m01/27 23:20:58 Evaluating the test split (-1)[0m
## OAR [2018-01-27 23:25:04] Job 139997 KILLED ##
[33m01/27 23:25:43 GPU ID: 0 | available memory: 16276M[0m
[33m01/27 23:25:50 Starting from the best saved model[0m
[33m01/27 23:25:50 Starting from save/fncnn6_reset_strat_rhamming_pool0_tsent01_a04_showtell152/model-best.pth[0m
[36m01/27 23:25:55 Setting CNN weigths from save/fncnn6_reset_strat_rhamming_pool0_tsent01_a04_showtell152/model-cnn-best.pth[0m
[33m01/27 23:25:57 Finetuning up from 6 modules in the cnn[0m
[33m01/27 23:25:58 Show & Tell : OrderedDict([('img_embed', Linear (2048 -> 512)), ('core', LSTM(512, 512, bias=False, dropout=0.5)), ('embed', Embedding(9488, 512)), ('drop_x_lm', Dropout (p = 0.5)), ('logit', Linear (512 -> 9488))])[0m
[33m01/27 23:25:58 Loading the model dict (last checkpoint) ['img_embed.weight', 'img_embed.bias', 'core.weight_ih_l0', 'core.weight_hh_l0', 'embed.weight', 'logit.weight', 'logit.bias'][0m
[33m01/27 23:25:59 DataLoader loading json file: data/coco/cocotalk.json[0m
[33m01/27 23:25:59 vocab size is 9487 [0m
[33m01/27 23:25:59 DataLoader loading h5 file: data/coco/cocotalk.h5[0m
[33m01/27 23:25:59 read 123287 images of size 3x256x256[0m
[33m01/27 23:25:59 max sequence length in data is 16[0m
[33m01/27 23:25:59 assigned 82783 images to split train[0m
[33m01/27 23:25:59 assigned 5000 images to split val[0m
[33m01/27 23:25:59 assigned 5000 images to split test[0m
[32m01/27 23:25:59 RewardSampler (stratified sampling), r=Hamming (Vpool=0, tau=0.10)[0m
[33m01/27 23:25:59 Evaluating the test split (-1)[0m
## OAR [2018-01-27 23:30:04] Job 139998 KILLED ##
[33m01/27 23:30:44 GPU ID: 0 | available memory: 16276M[0m
[33m01/27 23:30:52 Starting from the best saved model[0m
[33m01/27 23:30:52 Starting from save/fncnn6_reset_strat_rhamming_pool0_tsent01_a04_showtell152/model-best.pth[0m
[36m01/27 23:30:56 Setting CNN weigths from save/fncnn6_reset_strat_rhamming_pool0_tsent01_a04_showtell152/model-cnn-best.pth[0m
[33m01/27 23:30:59 Finetuning up from 6 modules in the cnn[0m
[33m01/27 23:31:00 Show & Tell : OrderedDict([('img_embed', Linear (2048 -> 512)), ('core', LSTM(512, 512, bias=False, dropout=0.5)), ('embed', Embedding(9488, 512)), ('drop_x_lm', Dropout (p = 0.5)), ('logit', Linear (512 -> 9488))])[0m
[33m01/27 23:31:00 Loading the model dict (last checkpoint) ['img_embed.weight', 'img_embed.bias', 'core.weight_ih_l0', 'core.weight_hh_l0', 'embed.weight', 'logit.weight', 'logit.bias'][0m
[33m01/27 23:31:00 DataLoader loading json file: data/coco/cocotalk.json[0m
[33m01/27 23:31:01 vocab size is 9487 [0m
[33m01/27 23:31:01 DataLoader loading h5 file: data/coco/cocotalk.h5[0m
[33m01/27 23:31:01 read 123287 images of size 3x256x256[0m
[33m01/27 23:31:01 max sequence length in data is 16[0m
[33m01/27 23:31:01 assigned 82783 images to split train[0m
[33m01/27 23:31:01 assigned 5000 images to split val[0m
[33m01/27 23:31:01 assigned 5000 images to split test[0m
[32m01/27 23:31:01 RewardSampler (stratified sampling), r=Hamming (Vpool=0, tau=0.10)[0m
[33m01/27 23:31:01 Evaluating the test split (-1)[0m
## OAR [2018-01-27 23:35:04] Job 139999 KILLED ##
[33m01/27 23:35:43 GPU ID: 0 | available memory: 16276M[0m
[33m01/27 23:35:51 Starting from the best saved model[0m
[33m01/27 23:35:51 Starting from save/fncnn6_reset_strat_rhamming_pool0_tsent01_a04_showtell152/model-best.pth[0m
[36m01/27 23:35:55 Setting CNN weigths from save/fncnn6_reset_strat_rhamming_pool0_tsent01_a04_showtell152/model-cnn-best.pth[0m
[33m01/27 23:35:58 Finetuning up from 6 modules in the cnn[0m
[33m01/27 23:35:59 Show & Tell : OrderedDict([('img_embed', Linear (2048 -> 512)), ('core', LSTM(512, 512, bias=False, dropout=0.5)), ('embed', Embedding(9488, 512)), ('drop_x_lm', Dropout (p = 0.5)), ('logit', Linear (512 -> 9488))])[0m
[33m01/27 23:35:59 Loading the model dict (last checkpoint) ['img_embed.weight', 'img_embed.bias', 'core.weight_ih_l0', 'core.weight_hh_l0', 'embed.weight', 'logit.weight', 'logit.bias'][0m
[33m01/27 23:35:59 DataLoader loading json file: data/coco/cocotalk.json[0m
[33m01/27 23:35:59 vocab size is 9487 [0m
[33m01/27 23:35:59 DataLoader loading h5 file: data/coco/cocotalk.h5[0m
[33m01/27 23:35:59 read 123287 images of size 3x256x256[0m
[33m01/27 23:35:59 max sequence length in data is 16[0m
[33m01/27 23:35:59 assigned 82783 images to split train[0m
[33m01/27 23:35:59 assigned 5000 images to split val[0m
[33m01/27 23:35:59 assigned 5000 images to split test[0m
[32m01/27 23:35:59 RewardSampler (stratified sampling), r=Hamming (Vpool=0, tau=0.10)[0m
[33m01/27 23:35:59 Evaluating the test split (-1)[0m
## OAR [2018-01-27 23:40:04] Job 140000 KILLED ##
[33m01/27 23:40:43 GPU ID: 0 | available memory: 16276M[0m
[33m01/27 23:40:50 Starting from the best saved model[0m
[33m01/27 23:40:50 Starting from save/fncnn6_reset_strat_rhamming_pool0_tsent01_a04_showtell152/model-best.pth[0m
[36m01/27 23:40:55 Setting CNN weigths from save/fncnn6_reset_strat_rhamming_pool0_tsent01_a04_showtell152/model-cnn-best.pth[0m
[33m01/27 23:40:58 Finetuning up from 6 modules in the cnn[0m
[33m01/27 23:40:58 Show & Tell : OrderedDict([('img_embed', Linear (2048 -> 512)), ('core', LSTM(512, 512, bias=False, dropout=0.5)), ('embed', Embedding(9488, 512)), ('drop_x_lm', Dropout (p = 0.5)), ('logit', Linear (512 -> 9488))])[0m
[33m01/27 23:40:58 Loading the model dict (last checkpoint) ['img_embed.weight', 'img_embed.bias', 'core.weight_ih_l0', 'core.weight_hh_l0', 'embed.weight', 'logit.weight', 'logit.bias'][0m
[33m01/27 23:40:59 DataLoader loading json file: data/coco/cocotalk.json[0m
[33m01/27 23:40:59 vocab size is 9487 [0m
[33m01/27 23:40:59 DataLoader loading h5 file: data/coco/cocotalk.h5[0m
[33m01/27 23:40:59 read 123287 images of size 3x256x256[0m
[33m01/27 23:40:59 max sequence length in data is 16[0m
[33m01/27 23:40:59 assigned 82783 images to split train[0m
[33m01/27 23:40:59 assigned 5000 images to split val[0m
[33m01/27 23:40:59 assigned 5000 images to split test[0m
[32m01/27 23:40:59 RewardSampler (stratified sampling), r=Hamming (Vpool=0, tau=0.10)[0m
[33m01/27 23:40:59 Evaluating the test split (-1)[0m
## OAR [2018-01-27 23:45:04] Job 140001 KILLED ##
[33m01/27 23:45:43 GPU ID: 0 | available memory: 16276M[0m
[33m01/27 23:45:50 Starting from the best saved model[0m
[33m01/27 23:45:50 Starting from save/fncnn6_reset_strat_rhamming_pool0_tsent01_a04_showtell152/model-best.pth[0m
[36m01/27 23:45:54 Setting CNN weigths from save/fncnn6_reset_strat_rhamming_pool0_tsent01_a04_showtell152/model-cnn-best.pth[0m
[33m01/27 23:45:57 Finetuning up from 6 modules in the cnn[0m
[33m01/27 23:45:58 Show & Tell : OrderedDict([('img_embed', Linear (2048 -> 512)), ('core', LSTM(512, 512, bias=False, dropout=0.5)), ('embed', Embedding(9488, 512)), ('drop_x_lm', Dropout (p = 0.5)), ('logit', Linear (512 -> 9488))])[0m
[33m01/27 23:45:58 Loading the model dict (last checkpoint) ['img_embed.weight', 'img_embed.bias', 'core.weight_ih_l0', 'core.weight_hh_l0', 'embed.weight', 'logit.weight', 'logit.bias'][0m
[33m01/27 23:45:58 DataLoader loading json file: data/coco/cocotalk.json[0m
[33m01/27 23:45:59 vocab size is 9487 [0m
[33m01/27 23:45:59 DataLoader loading h5 file: data/coco/cocotalk.h5[0m
[33m01/27 23:45:59 read 123287 images of size 3x256x256[0m
[33m01/27 23:45:59 max sequence length in data is 16[0m
[33m01/27 23:45:59 assigned 82783 images to split train[0m
[33m01/27 23:45:59 assigned 5000 images to split val[0m
[33m01/27 23:45:59 assigned 5000 images to split test[0m
[32m01/27 23:45:59 RewardSampler (stratified sampling), r=Hamming (Vpool=0, tau=0.10)[0m
[33m01/27 23:45:59 Evaluating the test split (-1)[0m
## OAR [2018-01-27 23:50:04] Job 140002 KILLED ##
[33m01/27 23:50:42 GPU ID: 0 | available memory: 16276M[0m
[33m01/27 23:50:50 Starting from the best saved model[0m
[33m01/27 23:50:50 Starting from save/fncnn6_reset_strat_rhamming_pool0_tsent01_a04_showtell152/model-best.pth[0m
[36m01/27 23:50:55 Setting CNN weigths from save/fncnn6_reset_strat_rhamming_pool0_tsent01_a04_showtell152/model-cnn-best.pth[0m
[33m01/27 23:50:57 Finetuning up from 6 modules in the cnn[0m
[33m01/27 23:50:58 Show & Tell : OrderedDict([('img_embed', Linear (2048 -> 512)), ('core', LSTM(512, 512, bias=False, dropout=0.5)), ('embed', Embedding(9488, 512)), ('drop_x_lm', Dropout (p = 0.5)), ('logit', Linear (512 -> 9488))])[0m
[33m01/27 23:50:58 Loading the model dict (last checkpoint) ['img_embed.weight', 'img_embed.bias', 'core.weight_ih_l0', 'core.weight_hh_l0', 'embed.weight', 'logit.weight', 'logit.bias'][0m
[33m01/27 23:50:59 DataLoader loading json file: data/coco/cocotalk.json[0m
[33m01/27 23:50:59 vocab size is 9487 [0m
[33m01/27 23:50:59 DataLoader loading h5 file: data/coco/cocotalk.h5[0m
[33m01/27 23:50:59 read 123287 images of size 3x256x256[0m
[33m01/27 23:50:59 max sequence length in data is 16[0m
[33m01/27 23:50:59 assigned 82783 images to split train[0m
[33m01/27 23:50:59 assigned 5000 images to split val[0m
[33m01/27 23:50:59 assigned 5000 images to split test[0m
[32m01/27 23:50:59 RewardSampler (stratified sampling), r=Hamming (Vpool=0, tau=0.10)[0m
[33m01/27 23:50:59 Evaluating the test split (-1)[0m
## OAR [2018-01-27 23:55:28] Job 140003 KILLED ##
[33m01/28 09:49:07 GPU ID: 0 | available memory: 12207M[0m
[33m01/28 09:49:30 GPU ID: 0 | available memory: 16276M[0m
[33m01/28 09:49:33 Starting from the best saved model[0m
[33m01/28 09:49:33 Starting from save/strat_lazy_rhamming_pool0_tsent01_a04_topdown/model-best.pth[0m
[33m01/28 09:49:41 Starting from the best saved model[0m
[33m01/28 09:49:41 Starting from save/fncnn6_reset_importance_lazy_qhamming_limited2_tsent03_rcider_tsent05_a05_topdown/model-best.pth[0m
[36m01/28 09:49:45 Setting CNN weigths from save/fncnn6_reset_importance_lazy_qhamming_limited2_tsent03_rcider_tsent05_a05_topdown/model-cnn-best.pth[0m
[33m01/28 09:49:49 Finetuning up from 6 modules in the cnn[33m01/28 09:49:55 Starting from the best saved model[0m
[33m01/28 09:49:55 Starting from save/strat_lazy_rhamming_pool0_tsent01_a04_showtell152/model-best.pth[0m
= 0.5)
)), ('att_embed', Sequential (
  (0): Linear (2048 -> 512)
  (1): ReLU ()
  (2): Dropout (p = 0.0)
)), ('logit', Linear (512 -> 9488)), ('ctx2att', Linear (512 -> 512)), ('fc_embed', Sequential (
  (0): Linear (2048 -> 512)
  (1): ReLU ()
  (2): Dropout (p = 0.0)
)), ('core', TopDownCore (
  (att_lstm): LSTMCell(1536, 512)
  (lang_lstm): LSTMCell(1024, 512)
  (attention): Attention (
    (h2att): Linear (512 -> 512)
    (alpha_net): Linear (512 -> 1)
  )
))])[0m
[33m01/28 09:49:50 Loading the model dict (last checkpoint) ['embed_1.weight', 'embed.0.weight', 'att_embed.0.weight', 'att_embed.0.bias', 'logit.weight', 'logit.bias', 'ctx2att.weight', 'ctx2att.bias', 'fc_embed.0.weight', 'fc_embed.0.bias', 'core.att_lstm.weight_ih', 'core.att_lstm.weight_hh', 'core.att_lstm.bias_ih', 'core.att_lstm.bias_hh', 'core.lang_lstm.weight_ih', 'core.lang_lstm.weight_hh', 'core.lang_lstm.bias_ih', 'core.lang_lstm.bias_hh', 'core.attention.h2att.weight', 'core.attention.h2att.bias', 'core.attention.alpha_net.weight', 'core.attention.alpha_net.bias'][0m
[33m01/28 09:49:50 DataLoader loading json file: data/coco/cocotalk.json[0m
[33m01/28 09:49:50 vocab size is 9487 [0m
[33m01/28 09:49:50 DataLoader loading h5 file: data/coco/cocotalk.h5[0m
[33m01/28 09:49:50 read 123287 images of size 3x256x256[0m
[33m01/28 09:49:50 max sequence length in data is 16[0m
[33m01/28 09:49:50 assigned 82783 images to split train[0m
[33m01/28 09:49:50 assigned 5000 images to split val[0m
[33m01/28 09:49:50 assigned 5000 images to split test[0m
[32m01/28 09:49:52 using importance sampling r=CIDEr (tau=0.50) and q=Hamming (Vpool=2, tau=0.30)[0m
[33m01/28 09:49:52 Evaluating the test split (-1)[0m
[36m01/28 09:50:00 Setting CNN weigths from save/strat_lazy_rhamming_pool0_tsent01_a04_showtell152/model-cnn-best.pth[0m
[33m01/28 09:50:07 Finetuning up from 0 modules in the cnn[0m
[33m01/28 09:50:08 Show & Tell : OrderedDict([('img_embed', Linear (2048 -> 512)), ('core', LSTM(512, 512, bias=False, dropout=0.5)), ('embed', Embedding(9488, 512)), ('drop_x_lm', Dropout (p = 0.5)), ('logit', Linear (512 -> 9488))])[0m
[33m01/28 09:50:09 Loading the model dict (last checkpoint) ['img_embed.weight', 'img_embed.bias', 'core.weight_ih_l0', 'core.weight_hh_l0', 'embed.weight', 'logit.weight', 'logit.bias'][0m
[33m01/28 09:50:10 DataLoader loading json file: data/coco/cocotalk.json[0m
[33m01/28 09:50:10 vocab size is 9487 [0m
[33m01/28 09:50:10 DataLoader loading h5 file: data/coco/cocotalk.h5[0m
[33m01/28 09:50:10 read 123287 images of size 3x256x256[0m
[33m01/28 09:50:10 max sequence length in data is 16[0m
[33m01/28 09:50:10 assigned 82783 images to split train[0m
[33m01/28 09:50:10 assigned 5000 images to split val[0m
[33m01/28 09:50:10 assigned 50[33m01/28 09:50:15 Starting from the best saved model[0m
[33m01/28 09:50:15 Starting from save/fncnn6_reset_importance_lazy_qhamming_limited2_tsent03_rcider_tsent05_a05_sh[33m01/28 09:50:24 Finetuning up from 0 modules in the cnn[0m
[33m01/28 09:50:25 Top Down : OrderedDict([('embed_1', Embedding(9488, 512)), ('embed', Sequential (
  (0): Embedding(9488, 512)
  (1): ReLU ()
  (2): Dropout (p = 0.5)
)), ('att_embed', Sequential (
  (0): Linear (2048 -> 512)
  (1): ReLU ()
  (2): Dropout (p = 0.0)
)), ('logit', Linear (512 -> 9488)), ('ctx2att', Linear (512 -> 512)), ('fc_embed', Sequential (
  (0): Linear (2048 -> 512)
  (1): ReLU ()
  (2): Dropout (p = 0.0)
)), ('core', TopDownCore (
  (att_lstm): LSTMCell(1536, 512)
  (lang_lstm): LSTMCell(1024, 512)
  (attention): Attention (
    (h2att): Linear (512 -> 512)
    (alpha_net): Linear (512 -> 1)
  )
))])[0m
[33m01/28 09:50:34 Loading the model dict (last checkpoint) ['embed_1.weight', 'embed.0.weight', 'att_embed.0.weight', 'att_embed.0.bias', 'logit.weight', 'logit.bias', 'ctx2att.weight', 'ctx2att.bias', 'fc_embed.0.weight', 'fc_embed.0.bias', 'core.att_lstm.weight_ih', 'core.att_lstm.weight_hh', 'core.att_lstm.bias_ih', 'core.att_lstm.bias_hh', 'core.lang_lstm.weight_ih', 'core.lang_lstm.weight_hh', 'core.lang_lstm.bias_ih', 'core.lang_lstm.bias_hh', 'core.attention.h2att.weight', 'core.attention.h2att.bias', 'core.attention.alpha_net.weight', 'core.attention.alpha_net.bias'][0m
[33m01/28 09:50:34 DataLoader loading json file: data/coco/cocotalk.json[0m
[33m01/28 09:50:35 vocab size is 9487 [0m
[33m01/28 09:50:35 DataLoader loading h5 file: data/coco/cocotalk.h5[0m
[33m01/28 09:50:35 read 123287 images of size 3x256x256[0m
[33m01/28 09:50:35 max sequence length in data is 16[0m
[33m01/28 09:50:35 assigned 82783 images to split train[0m
[33m01/28 09:50:35 assigned 5000 images to split val[0m
[33m01/28 09:50:35 assigned 5000 images to split test[0m
[32m01/28 09:50:35 RewardSampler (stratified sampling), r=Hamming (Vpool=0, tau=0.10)[0m
[33m01/28 09:50:35 Evaluating the test split (-1)[0m
[33m01/28 09:51:10 Finetuning up from 6 modules in the cnn[0m
[33m01/28 09:51:10 Show & Tell : OrderedDict([('img_embed', Linear (2048 -> 512)), ('core', LSTM(512, 512, bias=False, dropout=0.5)), ('embed', Embedding(9488, 512)), ('drop_x_lm', Dropout (p = 0.5)), ('logit', Linear (512 -> 9488))])[0m
[33m01/28 09:51:19 Loading the model dict (last checkpoint) ['img_embed.weight', 'img_embed.bias', 'core.weight_ih_l0', 'core.weight_hh_l0', 'embed.weight', 'logit.weight', 'logit.bias'][0m
[33m01/28 09:51:28 DataLoader loading json file: data/coco/cocotalk.json[0m
[33m01/28 09:51:28 vocab size is 9487 [0m
[33m01/28 09:51:28 DataLoader loading h5 file: data/coco/cocotalk.h5[0m
[33m01/28 09:51:28 read 123287 images of size 3x256x256[0m
[33m01/28 09:51:28 max sequence length in data is 16[0m
[33m01/28 09:51:28 assigned 82783 images to split train[0m
[33m01/28 09:51:28 assigned 5000 images to split val[0m
[33m01/28 09:51:28 assigned 5000 images to split test[0m
[32m01/28 09:51:39 using importance sampling r=CIDEr (tau=0.50) and q=Hamming (Vpool=2, tau=0.30)[0m
[33m01/28 09:51:39 Evaluating the test split (-1)[0m
[33m01/28 09:55:48 Loading reference captions..[0m
[33m01/28 09:55:49 Reference captions loaded![0m
[33m01/28 09:55:52 using 5000/5000 predictions[0m
[33m01/28 09:55:52 Loading model captions[0m
[33m01/28 09:55:52 Model captions loaded[0m
[33m01/28 09:55:52 Starting evaluation...[0m
PTBTokenizer tokenized 307085 tokens at 1071287.91 tokens per second.
PTBTokenizer tokenized 51773 tokens at 282563.37 tokens per second.
[33m01/28 09:56:08 Loading reference captions..[0m
[33m01/28 09:56:04 Reference captions loaded![0m
[33m01/28 09:56:06 using 5000/5000 predictions[0m
[33m01/28 09:56:06 Loading model captions[0m
[33m01/28 09:56:06 Model captions loaded[0m
[33m01/28 09:56:06 Starting evaluation...[0m
PTBTokenizer tokenized 307085 tokens at 1364041.29 tokens per second.
PTBTokenizer tokenized 51238 tokens at 398802.29 tokens per second.
[33m01/28 09:56:12 Reference captions loaded![0m
[33m01/28 09:56:14 using 5000/5000 predictions[0m
[33m01/28 09:56:14 Loading model captions[0m
[33m01/28 09:56:14 Model captions loaded[0m
[33m01/28 09:56:14 Starting evaluation...[0m
PTBTokenizer tokenized 307085 tokens at 1007165.34 tokens per second.
ni.Database.get(Database.java:193)
	at org.fusesource.lmdbjni.Database.get(Database.java:186)
	at org.fusesource.lmdbjni.Database.get(Database.java:161)
	at edu.anu.spice.LmdbTupleDB.getTransaction(LmdbTupleDB.java:96)
	at edu.anu.spice.SpiceParser.loadTuplesFromDB(SpiceParser.java:195)
	at edu.anu.spice.SpiceParser.loadTuples(SpiceParser.java:245)
	at edu.anu.spice.SpiceParser.parseCaptions(SpiceParser.java:251)
	at edu.anu.spice.SpiceScorer.scoreBatch(SpiceScorer.java:109)
	at edu.anu.spice.SpiceScorer.main(SpiceScorer.java:60)
[33m01/28 09:56:14 GPU ID: 0 | available memory: 11172M[0m
econd.
[33m01/28 09:56:18 Starting from the best saved model[0m
[33m01/28 09:56:19 Starting from save/fncnn6_reset_strat_rhamming_pool0_tsent01_a04_showtell152/model-best.pth[0m
[36m01/28 09:56:23 Setting CNN weigths from save/fncnn6_reset_strat_rhamming_pool0_tsent01_a04_showtell152/model-cnn-best.pth[0m
e.lmdbjni.Database.get(Database.java:193)
	at org.fusesource.lmdbjni.Database.get(Database.java:186)
	at org.fusesource.lmdbjni.Database.get(Database.java:161)
	at edu.anu.spice.LmdbTupleDB.getTransaction(LmdbTupleDB.java:96)
	at edu.anu.spice.SpiceParser.loadTuplesFromDB(SpiceParser.java:195)
	at edu.anu.spice.SpiceParser.loadTuples(SpiceParser.java:245)
	at edu.anu.spice.SpiceParser.parseCaptions(SpiceParser.java:251)
	at edu.anu.spice.SpiceScorer.scoreBatch(SpiceScorer.java:109)
	at edu.anu.spice.SpiceScorer.main(SpiceScorer.java:60)
[33m01/28 09:56:25 Finetuning up from 6 modules in the cnn[0m
[33m01/28 09:56:26 Show & Tell : OrderedDict([('img_embed', Linear (2048 -> 512)), ('core', LSTM(512, 512, bias=False, dropout=0.5)), ('embed', Embedding(9488, 512)), ('drop_x_lm', Dropout (p = 0.5)), ('logit', Linear (512 -> 9488))])[0m
[33m01/28 09:56:26 Loading the model dict (last checkpoint) ['img_embed.weight', 'img_embed.bias', 'core.weight_ih_l0', 'core.weight_hh_l0', 'embed.weight', 'logit.weight', 'logit.bias'][0m
[33m01/28 09:56:27 DataLoader loading json file: data/coco/cocotalk.json[0m
[33m01/28 09:56:27 vocab size is 9487 [0m
[33m01/28 09:56:27 DataLoader loading h5 file: data/coco/cocotalk.h5[0m
[33m01/28 09:56:27 read 123287 images of size 3x256x256[0m
[33m01/28 09:56:27 max sequence length in data is 16[0m
[33m01/28 09:56:27 assigned 82783 images to split train[0m
[33m01/28 09:56:27 assigned 5000 images to split val[0m
[33m01/28 09:56:27 assigned 5000 images to split test[0m
[32m01/28 09:56:27 RewardSampler (stratified sampling), r=Hamming (Vpool=0, tau=0.10)[0m
[33m01/28 09:56:27 Evaluating the test split (-1)[0m
Error: Could not score batched file input:
org.fusesource.lmdbjni.LMDBException: MDB_CURSOR_FULL: Internal error - cursor stack limit reached
	at org.fusesource.lmdbjni.Util.checkErrorCode(Util.java:44)
	at org.fusesource.lmdbjni.Database.get(Database.java:202)
	at org.fusesource.lmdbjni.Database.get(Database.java:193)
	at org.fusesource.lmdbjni.Database.get(Database.java:186)
	at org.fusesource.lmdbjni.Database.get(Database.java:161)
	at edu.anu.spice.LmdbTupleDB.getTransaction(LmdbTupleDB.java:96)
	at edu.anu.spice.SpiceParser.loadTuplesFromDB(SpiceParser.java:195)
	at edu.anu.spice.SpiceParser.loadTuples(SpiceParser.java:245)
	at edu.anu.spice.SpiceParser.parseCaptions(SpiceParser.java:251)
	at edu.anu.spice.SpiceScorer.scoreBatch(SpiceScorer.java:109)
	at edu.anu.spice.SpiceScorer.main(SpiceScorer.java:60)
[33m01/28 09:58:14 Loading reference captions..[0m
[33m01/28 09:58:18 Reference captions loaded![0m
[33m01/28 09:58:20 using 5000/5000 predictions[0m
[33m01/28 09:58:20 Loading model captions[0m
[33m01/28 09:58:20 Model captions loaded[0m
[33m01/28 09:58:20 Starting evaluation...[0m
PTBTokenizer tokenized 307085 tokens at 714313.53 tokens per second.
PTBTokenizer tokenized 50854 tokens at 289756.07 tokens per second.
Parsing reference captions
Error: Could not score batched file input:
org.fusesource.lmdbjni.LMDBException: MDB_CURSOR_FULL: Internal error - cursor stack limit reached
	at org.fusesource.lmdbjni.Util.checkErrorCode(Util.java:44)
	at org.fusesource.lmdbjni.Database.get(Database.java:202)
	at org.fusesource.lmdbjni.Database.get(Database.java:193)
	at org.fusesource.lmdbjni.Database.get(Database.java:186)
	at org.fusesource.lmdbjni.Database.get(Database.java:161)
	at edu.anu.spice.LmdbTupleDB.getTransaction(LmdbTupleDB.java:96)
	at edu.anu.spice.SpiceParser.loadTuplesFromDB(SpiceParser.java:195)
	at edu.anu.spice.SpiceParser.loadTuples(SpiceParser.java:245)
	at edu.anu.spice.SpiceParser.parseCaptions(SpiceParser.java:251)
	at edu.anu.spice.SpiceScorer.scoreBatch(SpiceScorer.java:109)
	at edu.anu.spice.SpiceScorer.main(SpiceScorer.java:60)
[33m01/28 10:00:00 Loading reference captions..[0m
[33m01/28 10:00:05 Reference captions loaded![0m
[33m01/28 10:00:06 using 5000/5000 predictions[0m
[33m01/28 10:00:06 Loading model captions[0m
[33m01/28 10:00:07 Model captions loaded[0m
[33m01/28 10:00:07 Starting evaluation...[0m
PTBTokenizer tokenized 307085 tokens at 781339.19 tokens per second.
PTBTokenizer tokenized 51619 tokens at 354848.38 tokens per second.
Parsing reference captions
[33m01/28 10:00:23 Loading reference captions..[0m
[33m01/28 10:00:24 Reference captions loaded![0m
[33m01/28 10:00:26 using 5000/5000 predictions[0m
[33m01/28 10:00:26 Loading model captions[0m
[33m01/28 10:00:26 Model captions loaded[0m
[33m01/28 10:00:26 Starting evaluation...[0m
PTBTokenizer tokenized 307085 tokens at 1316423,93 tokens per second.
PTBTokenizer tokenized 51790 tokens at 399883,79 tokens per second.
Parsing reference captions
Error: Could not score batched file input:
org.fusesource.lmdbjni.LMDBException: MDB_CURSOR_FULL: Internal error - cursor stack limit reached
	at org.fusesource.lmdbjni.Util.checkErrorCode(Util.java:44)
	at org.fusesource.lmdbjni.Database.get(Database.java:202)
	at org.fusesource.lmdbjni.Database.get(Database.java:193)
	at org.fusesource.lmdbjni.Database.get(Database.java:186)
	at org.fusesource.lmdbjni.Database.get(Database.java:161)
	at edu.anu.spice.LmdbTupleDB.getTransaction(LmdbTupleDB.java:96)
	at edu.anu.spice.SpiceParser.loadTuplesFromDB(SpiceParser.java:195)
	at edu.anu.spice.SpiceParser.loadTuples(SpiceParser.java:245)
	at edu.anu.spice.SpiceParser.parseCaptions(SpiceParser.java:251)
	at edu.anu.spice.SpiceScorer.scoreBatch(SpiceScorer.java:109)
	at edu.anu.spice.SpiceScorer.main(SpiceScorer.java:60)
Error: Could not score batched file input:
org.fusesource.lmdbjni.LMDBException: MDB_CURSOR_FULL: Internal error - cursor stack limit reached
	at org.fusesource.lmdbjni.Util.checkErrorCode(Util.java:44)
	at org.fusesource.lmdbjni.Database.get(Database.java:202)
	at org.fusesource.lmdbjni.Database.get(Database.java:193)
	at org.fusesource.lmdbjni.Database.get(Database.java:186)
	at org.fusesource.lmdbjni.Database.get(Database.java:161)
	at edu.anu.spice.LmdbTupleDB.getTransaction(LmdbTupleDB.java:96)
	at edu.anu.spice.SpiceParser.loadTuplesFromDB(SpiceParser.java:195)
	at edu.anu.spice.SpiceParser.loadTuples(SpiceParser.java:245)
	at edu.anu.spice.SpiceParser.parseCaptions(SpiceParser.java:251)
	at edu.anu.spice.SpiceScorer.scoreBatch(SpiceScorer.java:109)
	at edu.anu.spice.SpiceScorer.main(SpiceScorer.java:60)
[33m01/28 13:20:14 GPU ID: 0 | available memory: 12207M[0m
[33m01/28 13:20:20 Starting from the best saved model[0m
[33m01/28 13:20:20 Starting from save/combine_strat_rhamming_pool2_tsent03_a05_word_coco_tword009_idf10_a07_showtell152/model-best.pth[0m
[36m01/28 13:20:23 Setting CNN weigths from save/combine_strat_rhamming_pool2_tsent03_a05_word_coco_tword009_idf10_a07_showtell152/model-cnn-best.pth[0m
[33m01/28 13:20:26 Finetuning up from 0 modules in the cnn[0m
[33m01/28 13:20:26 Show & Tell : OrderedDict([('img_embed', Linear (2048 -> 512)), ('core', LSTM(512, 512, bias=False, dropout=0.5)), ('embed', Embedding(9488, 512)), ('drop_x_lm', Dropout (p = 0.5)), ('logit', Linear (512 -> 9488))])[0m
[33m01/28 13:20:26 Loading the model dict (last checkpoint) ['img_embed.weight', 'img_embed.bias', 'core.weight_ih_l0', 'core.weight_hh_l0', 'embed.weight', 'logit.weight', 'logit.bias'][0m
[33m01/28 13:20:27 DataLoader loading json file: data/coco/cocotalk.json[0m
[33m01/28 13:20:27 vocab size is 9487 [0m
[33m01/28 13:20:27 DataLoader loading h5 file: data/coco/cocotalk.h5[0m
[33m01/28 13:20:27 read 123287 images of size 3x256x256[0m
[33m01/28 13:20:27 max sequence length in data is 16[0m
[33m01/28 13:20:27 assigned 82783 images to split train[0m
[33m01/28 13:20:27 assigned 5000 images to split val[0m
[33m01/28 13:20:27 assigned 5000 images to split test[0m
[32m01/28 13:20:43 RewardSampler (stratified sampling), r=Hamming (Vpool=2, tau=0.30)[0m
[32m01/28 13:20:43 GT loss:[0m
[32m01/28 13:20:43 Initialized Word2 loss tau=0.090, alpha=0.7[0m
[32m01/28 13:20:43 Sampled loss:[0m
[32m01/28 13:20:43 Initialized Word2 loss tau=0.090, alpha=0.7[0m
[33m01/28 13:20:43 Evaluating the test split (-1)[0m
[33m01/28 13:27:55 Loading reference captions..[0m
[33m01/28 13:27:55 Reference captions loaded![0m
[33m01/28 13:27:57 using 5000/5000 predictions[0m
[33m01/28 13:27:57 Loading model captions[0m
[33m01/28 13:27:57 Model captions loaded[0m
[33m01/28 13:27:57 Starting evaluation...[0m
PTBTokenizer tokenized 307085 tokens at 782907.63 tokens per second.
PTBTokenizer tokenized 49707 tokens at 236485.29 tokens per second.
Parsing reference captions
Error: Could not score batched file input:
org.fusesource.lmdbjni.LMDBException: MDB_CURSOR_FULL: Internal error - cursor stack limit reached
	at org.fusesource.lmdbjni.Util.checkErrorCode(Util.java:44)
	at org.fusesource.lmdbjni.Database.get(Database.java:202)
	at org.fusesource.lmdbjni.Database.get(Database.java:193)
	at org.fusesource.lmdbjni.Database.get(Database.java:186)
	at org.fusesource.lmdbjni.Database.get(Database.java:161)
	at edu.anu.spice.LmdbTupleDB.getTransaction(LmdbTupleDB.java:96)
	at edu.anu.spice.SpiceParser.loadTuplesFromDB(SpiceParser.java:195)
	at edu.anu.spice.SpiceParser.loadTuples(SpiceParser.java:245)
	at edu.anu.spice.SpiceParser.parseCaptions(SpiceParser.java:251)
	at edu.anu.spice.SpiceScorer.scoreBatch(SpiceScorer.java:109)
	at edu.anu.spice.SpiceScorer.main(SpiceScorer.java:60)
[33m01/28 13:29:12 GPU ID: 1 | available memory: 12207M[0m
[33m01/28 13:29:22 Starting from the best saved model[0m
[33m01/28 13:29:22 Starting from save/importance_qhamming_pool0_tsent01_rcider_tsent05_a04_topdown/model-best.pth[0m
[36m01/28 13:29:26 Setting CNN weigths from save/importance_qhamming_pool0_tsent01_rcider_tsent05_a04_topdown/model-cnn-best.pth[0m
[33m01/28 13:29:32 Finetuning up from 0 modules in the cnn[0m
[33m01/28 13:29:33 Top Down : OrderedDict([('embed_1', Embedding(9488, 512)), ('embed', Sequential (
  (0): Embedding(9488, 512)
  (1): ReLU ()
  (2): Dropout (p = 0.5)
)), ('att_embed', Sequential (
  (0): Linear (2048 -> 512)
  (1): ReLU ()
  (2): Dropout (p = 0.0)
)), ('logit', Linear (512 -> 9488)), ('ctx2att', Linear (512 -> 512)), ('fc_embed', Sequential (
  (0): Linear (2048 -> 512)
  (1): ReLU ()
  (2): Dropout (p = 0.0)
)), ('core', TopDownCore (
  (att_lstm): LSTMCell(1536, 512)
  (lang_lstm): LSTMCell(1024, 512)
  (attention): Attention (
    (h2att): Linear (512 -> 512)
    (alpha_net): Linear (512 -> 1)
  )
))])[0m
[33m01/28 13:29:34 Loading the model dict (last checkpoint) ['embed_1.weight', 'embed.0.weight', 'att_embed.0.weight', 'att_embed.0.bias', 'logit.weight', 'logit.bias', 'ctx2att.weight', 'ctx2att.bias', 'fc_embed.0.weight', 'fc_embed.0.bias', 'core.att_lstm.weight_ih', 'core.att_lstm.weight_hh', 'core.att_lstm.bias_ih', 'core.att_lstm.bias_hh', 'core.lang_lstm.weight_ih', 'core.lang_lstm.weight_hh', 'core.lang_lstm.bias_ih', 'core.lang_lstm.bias_hh', 'core.attention.h2att.weight', 'core.attention.h2att.bias', 'core.attention.alpha_net.weight', 'core.attention.alpha_net.bias'][0m
[33m01/28 13:29:34 DataLoader loading json file: data/coco/cocotalk.json[0m
[33m01/28 13:29:34 vocab size is 9487 [0m
[33m01/28 13:29:34 DataLoader loading h5 file: data/coco/cocotalk.h5[0m
[33m01/28 13:29:34 read 123287 images of size 3x256x256[0m
[33m01/28 13:29:34 max sequence length in data is 16[0m
[33m01/28 13:29:34 assigned 82783 images to split train[0m
[33m01/28 13:29:34 assigned 5000 images to split val[0m
[33m01/28 13:29:34 assigned 5000 images to split test[0m
[32m01/28 13:29:42 using importance sampling r=CIDEr (tau=0.50) and q=Hamming (Vpool=0, tau=0.10)[0m
[33m01/28 13:29:42 Evaluating the test split (-1)[0m
[33m01/28 13:35:18 Loading reference captions..[0m
[33m01/28 13:35:20 Reference captions loaded![0m
[33m01/28 13:35:21 using 5000/5000 predictions[0m
[33m01/28 13:35:21 Loading model captions[0m
[33m01/28 13:35:21 Model captions loaded[0m
[33m01/28 13:35:21 Starting evaluation...[0m
PTBTokenizer tokenized 307085 tokens at 1283720.79 tokens per second.
PTBTokenizer tokenized 51780 tokens at 251842.51 tokens per second.
Parsing reference captions
Error: Could not score batched file input:
org.fusesource.lmdbjni.LMDBException: MDB_CURSOR_FULL: Internal error - cursor stack limit reached
	at org.fusesource.lmdbjni.Util.checkErrorCode(Util.java:44)
	at org.fusesource.lmdbjni.Database.get(Database.java:202)
	at org.fusesource.lmdbjni.Database.get(Database.java:193)
	at org.fusesource.lmdbjni.Database.get(Database.java:186)
	at org.fusesource.lmdbjni.Database.get(Database.java:161)
	at edu.anu.spice.LmdbTupleDB.getTransaction(LmdbTupleDB.java:96)
	at edu.anu.spice.SpiceParser.loadTuplesFromDB(SpiceParser.java:195)
	at edu.anu.spice.SpiceParser.loadTuples(SpiceParser.java:245)
	at edu.anu.spice.SpiceParser.parseCaptions(SpiceParser.java:251)
	at edu.anu.spice.SpiceScorer.scoreBatch(SpiceScorer.java:109)
	at edu.anu.spice.SpiceScorer.main(SpiceScorer.java:60)
[33m01/28 21:20:03 GPU ID: 1 | available memory: 8114M[0m
