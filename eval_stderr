[33m01/23 18:28:22 GPU ID: 1 | available memory: 12207M[0m
[33m01/23 18:28:48 Starting from the best saved model[0m
[33m01/23 18:28:48 Starting from save/word_coco_tword015_a07_showtell152/model-best.pth[0m
[36m01/23 18:28:51 Setting CNN weigths from save/word_coco_tword015_a07_showtell152/model-cnn-best.pth[0m
[33m01/23 18:29:09 Finetuning up from 0 modules in the cnn[0m
[33m01/23 18:29:10 Show & Tell : OrderedDict([('img_embed', Linear (2048 -> 512)), ('core', LSTM(512, 512, bias=False, dropout=0.5)), ('embed', Embedding(9488, 512)), ('drop_x_lm', Dropout (p = 0.5)), ('logit', Linear (512 -> 9488))])[0m
[33m01/23 18:29:10 Loading the model dict (last checkpoint) ['img_embed.weight', 'img_embed.bias', 'core.weight_ih_l0', 'core.weight_hh_l0', 'embed.weight', 'logit.weight', 'logit.bias'][0m
[33m01/23 18:29:14 DataLoader loading json file: data/coco/cocotalk.json[0m
[33m01/23 18:29:15 vocab size is 9487 [0m
[33m01/23 18:29:15 DataLoader loading h5 file: data/coco/cocotalk.h5[0m
[33m01/23 18:29:15 read 123287 images of size 3x256x256[0m
[33m01/23 18:29:15 max sequence length in data is 16[0m
[33m01/23 18:29:15 assigned 82783 images to split train[0m
[33m01/23 18:29:15 assigned 5000 images to split val[0m
[33m01/23 18:29:15 assigned 5000 images to split test[0m
## OAR [2018-01-23 18:29:27] Job 137964 KILLED ##
[33m01/23 18:29:43 GPU ID: 1 | available memory: 12189M[0m
[33m01/23 18:29:48 Starting from the best saved model[0m
[33m01/23 18:29:48 Starting from save/word_coco_tword015_a07_showtell152/model-best.pth[0m
[36m01/23 18:29:52 Setting CNN weigths from save/word_coco_tword015_a07_showtell152/model-cnn-best.pth[0m
odel-cnn-best.pth[0m
[33m01/23 18:30:00 GPU ID: 5 | available memory: 16276M[0m
[33m01/23 18:30:13 Finetuning up from 0 modules in the cnn[0m
[33m01/23 18:30:14 Show & Tell : OrderedDict([('img_embed', Linear (2048 -> 512)), ('core', LSTM(512, 512, bias=False, dropout=0.5)), ('embed', Embedding(9488, 512)), ('drop_x_lm', Dropout (p = 0.5)), ('logit', Linear (512 -> 9488))])[0m
[33m01/23 18:30:17 Loading the model dict (last checkpoint) ['img_embed.weight', 'img_embed.bias', 'core.weight_ih_l0', 'core.weight_hh_l0', 'embed.weight', 'logit.weight', 'logit.bias'][0m
[33m01/23 18:30:18 DataLoader loading json file: data/coco/cocotalk.json[0m
[33m01/23 18:30:18 vocab size is 9487 [0m
[33m01/23 18:30:18 DataLoader loading h5 file: data/coco/cocotalk.h5[0m
[33m01/23 18:30:18 read 123287 images of size 3x256x256[0m
[33m01/23 18:30:18 max sequence length in data is 16[0m
[33m01/23 18:30:18 assigned 82783 images to split train[0m
[33m01/23 18:30:18 assigned 5000 images to split val[0m
[33m01/23 18:30:18 assigned 5000 images to split test[0m
[32m01/23 18:30:18 RewardSampler (stratified sampling), r=Hamming (Vpool=1, tau=0.17)[0m
[33m01/23 18:30:18 Evaluating the test split (-1)[0m
assigned 82783 images to split train[0m
[33m01/23 18:30:20 assigned 5000 images to split val[0m
[33m01/23 18:30:20 assigned 5000 images to split test[0m
[32m01/23 18:30:20 RewardSampler (stratified sampling), r=Hamming (Vpool=2, tau=0.30)[0m
[33m01/23 18:30:20 Evaluating the test split (-1)[0m
[32m01/23 18:30:51 Initialized Word2 loss tau=0.150, alpha=0.7[0m
[33m01/23 18:30:51 Evaluating the test split (-1)[0m
[33m01/23 18:35:58 Loading reference captions..[0m
[33m01/23 18:36:00 Reference captions loaded![0m
[33m01/23 18:36:02 using 5000/5000 predictions[0m
[33m01/23 18:36:02 Loading model captions[0m
[33m01/23 18:36:02 Model captions loaded[0m
[33m01/23 18:36:02 Starting evaluation...[0m
PTBTokenizer tokenized 307085 tokens at 1150313.53 tokens per second.
PTBTokenizer tokenized 51263 tokens at 360003.18 tokens per second.
[33m01/23 18:36:20 Loading reference captions..[0m
[33m01/23 18:36:22 Reference captions loaded![0m
[33m01/23 18:36:24 using 5000/5000 predictions[0m
[33m01/23 18:36:24 Loading model captions[0m
[33m01/23 18:36:24 Model captions loaded[0m
[33m01/23 18:36:24 Starting evaluation...[0m
PTBTokenizer tokenized 307085 tokens at 923650.88 tokens per second.

PTBTokenizer tokenized 49687 tokens at 425840.69 tokens per second.
PTBTokenizer tokenized 51758 tokens at 432839.72 tokens per second.
[33m01/23 18:37:26 GPU ID: 1 | available memory: 12207M[0m
[33m01/23 18:37:30 Starting from the best saved model[0m
[33m01/23 18:37:30 Starting from save/importance_lazy_qhamming_limited1_tsent017_rcider_tsent05_a04_showtell152/model-best.pth[0m
[36m01/23 18:37:34 Setting CNN weigths from save/importance_lazy_qhamming_limited1_tsent017_rcider_tsent05_a04_showtell152/model-cnn-best.pth[0m
[33m01/23 18:37:45 Finetuning up from 0 modules in the cnn[0m
[33m01/23 18:37:46 Show & Tell : OrderedDict([('img_embed', Linear (2048 -> 512)), ('core', LSTM(512, 512, bias=False, dropout=0.5)), ('embed', Embedding(9488, 512)), ('drop_x_lm', Dropout (p = 0.5)), ('logit', Linear (512 -> 9488))])[0m
[33m01/23 18:37:47 Loading the model dict (last checkpoint) ['img_embed.weight', 'img_embed.bias', 'core.weight_ih_l0', 'core.weight_hh_l0', 'embed.weight', 'logit.weight', 'logit.bias'][0m
[33m01/23 18:37:47 DataLoader loading json file: data/coco/cocotalk.json[0m
[33m01/23 18:37:47 vocab size is 9487 [0m
[33m01/23 18:37:47 DataLoader loading h5 file: data/coco/cocotalk.h5[0m
[33m01/23 18:37:47 read 123287 images of size 3x256x256[0m
[33m01/23 18:37:47 max sequence length in data is 16[0m
[33m01/23 18:37:47 assigned 82783 images to split train[0m
[33m01/23 18:37:47 assigned 5000 images to split val[0m
[33m01/23 18:37:47 assigned 5000 images to split test[0m
[32m01/23 18:37:53 using importance sampling r=CIDEr (tau=0.50) and q=Hamming (Vpool=1, tau=0.17)[0m
[33m01/23 18:37:53 Evaluating the test split (-1)[0m
[33m01/23 18:37:59 GPU ID: 0 | available memory: 12205M[0m
[33m01/23 18:38:05 Starting from the best saved model[0m
[33m01/23 18:38:05 Starting from save/importance_qhamming_limited1_tsent017_rcider_tsent05_a04_showtell152/model-best.pth[0m
[36m01/23 18:38:09 Setting CNN weigths from save/importance_qhamming_limited1_tsent017_rcider_tsent05_a04_showtell152/model-cnn-best.pth[0m
[33m01/23 18:38:16 Finetuning up from 0 modules in the cnn[0m
[33m01/23 18:38:17 Show & Tell : OrderedDict([('img_embed', Linear (2048 -> 512)), ('core', LSTM(512, 512, bias=False, dropout=0.5)), ('embed', Embedding(9488, 512)), ('drop_x_lm', Dropout (p = 0.5)), ('logit', Linear (512 -> 9488))])[0m
[33m01/23 18:38:18 Loading the model dict (last checkpoint) ['img_embed.weight', 'img_embed.bias', 'core.weight_ih_l0', 'core.weight_hh_l0', 'embed.weight', 'logit.weight', 'logit.bias'][0m
[33m01/23 18:38:19 DataLoader loading json file: data/coco/cocotalk.json[0m
[33m01/23 18:38:19 vocab size is 9487 [0m
[33m01/23 18:38:19 DataLoader loading h5 file: data/coco/cocotalk.h5[0m
[33m01/23 18:38:19 read 123287 images of size 3x256x256[0m
[33m01/23 18:38:19 max sequence length in data is 16[0m
[33m01/23 18:38:19 assigned 82783 images to split train[0m
[33m01/23 18:38:19 assigned 5000 images to split val[0m
[33m01/23 18:38:19 assigned 5000 images to split test[0m
[32m01/23 18:38:22 using importance sampling r=CIDEr (tau=0.50) and q=Hamming (Vpool=1, tau=0.17)[0m
[33m01/23 18:38:22 Evaluating the test split (-1)[0m
[33m01/23 18:42:32 Loading reference captions..[0m
[33m01/23 18:42:33 Reference captions loaded![0m
[33m01/23 18:42:34 using 5000/5000 predictions[0m
[33m01/23 18:42:35 Loading model captions[0m
[33m01/23 18:42:35 Model captions loaded[0m
[33m01/23 18:42:35 Starting evaluation...[0m
PTBTokenizer tokenized 307085 tokens at 1392237.86 tokens per second.
PTBTokenizer tokenized 51403 tokens at 443307.31 tokens per second.
[33m01/23 18:43:57 Loading reference captions..[0m
[33m01/23 18:43:58 Reference captions loaded![0m
[33m01/23 18:44:00 using 5000/5000 predictions[0m
[33m01/23 18:44:00 Loading model captions[0m
[33m01/23 18:44:00 Model captions loaded[0m
[33m01/23 18:44:00 Starting evaluation...[0m
PTBTokenizer tokenized 307085 tokens at 806774.17 tokens per second.
PTBTokenizer tokenized 51784 tokens at 153366.24 tokens per second.
