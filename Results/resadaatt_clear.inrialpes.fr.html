<table>
    <tr>
        <th>Model</th>
        <th>CNN</th>
        <th>params</th>
        <th>loss</th>
        <th>weights</th>
        <th>Beam</th>
        <th>CIDEr</th>
        <th>Bleu4</th>
        <th>Perplexity</th>
        <th>best/last</th>
    </tr>
    <tr>
        <td>adaptive_attention</td>
        <td>resnet152</td>
        <td> base lr: 1.0e-04 decay: 5, Adam(0.8,0.999), batch: 10, seq: 16</td>
        <td> Word Level, Sim=Coco, $\tau=0.17$, $\alpha=0.4$</td>
        <td>RNN + 1.0 x cnn(6:)</td>
        <td>3</td>
        <td>100.99</td>
        <td>32.32</td>
        <td>15.08</td>
        <td>204k / 256k</td>
    </tr>
    <tr>
        <td>adaptive_attention</td>
        <td>resnet152</td>
        <td> base lr: 1.0e-04 decay: 5, Adam(0.8,0.999), batch: 10, seq: 16 Augment</td>
        <td> ML</td>
        <td>RNN + 1.0 x cnn(6:)</td>
        <td>3</td>
        <td>100.75</td>
        <td>32.90</td>
        <td>11.47</td>
        <td>200k / 256k</td>
    </tr>
    <tr>
        <td>adaptive_attention</td>
        <td>resnet152</td>
        <td> base lr: 1.0e-04 decay: 5, Adam(0.8,0.999), batch: 10, seq: 16</td>
        <td> SampleR, r=hamming limited=1, $\tau=0.17$, $\alpha=0.4$</td>
        <td>RNN + 1.0 x cnn(6:)</td>
        <td>3</td>
        <td>99.75</td>
        <td>31.52</td>
        <td>10.25</td>
        <td>224k / 256k</td>
    </tr>
    <tr>
        <td>adaptive_attention</td>
        <td>resnet152</td>
        <td> base lr: 1.0e-04 decay: 5, Adam(0.8,0.999), batch: 10, seq: 16</td>
        <td> SampleR, r=hamming limited=1, $\tau=0.17$, $\alpha=0.3$</td>
        <td>RNN + 1.0 x cnn(6:)</td>
        <td>3</td>
        <td>99.44</td>
        <td>31.20</td>
        <td>10.11</td>
        <td>256k / 256k</td>
    </tr>
    <tr>
        <td>adaptive_attention</td>
        <td>resnet152</td>
        <td> base lr: 1.0e-04 decay: 5, Adam(0.8,0.999), batch: 10, seq: 16</td>
        <td> Combining losses,  Word Level, Sim=Coco, $\tau=0.17$, $\alpha=0.3$ w/ SampleR, r=hamming limited=1 $\tau=0.17$, $\alpha=0.2$</td>
        <td>RNN + 1.0 x cnn(6:)</td>
        <td>3</td>
        <td>99.12</td>
        <td>31.71</td>
        <td>133.97</td>
        <td>176k / 248k</td>
    </tr>
    <tr>
        <td>adaptive_attention</td>
        <td>resnet152</td>
        <td> base lr: 1.0e-04 decay: 5, Adam(0.8,0.999), batch: 10, seq: 16</td>
        <td> ML</td>
        <td>RNN + 1.0 x cnn(6:)</td>
        <td>3</td>
        <td>98.77</td>
        <td>32.12</td>
        <td>10.16</td>
        <td>176k / 248k</td>
    </tr>
    <tr>
        <td>adaptive_attention</td>
        <td>resnet152</td>
        <td> base lr: 1.0e-04 decay: 5, Adam(0.8,0.999), batch: 10, seq: 16 Maxout</td>
        <td> ML</td>
        <td>RNN + 1.0 x cnn(6:)</td>
        <td>3</td>
        <td>98.61</td>
        <td>32.03</td>
        <td>9.87</td>
        <td>168k / 256k</td>
    </tr>
    <tr>
        <td>adaptive_attention</td>
        <td>resnet152</td>
        <td> base lr: 1.0e-04 decay: 5, Adam(0.8,0.999), batch: 10, seq: 16</td>
        <td> SampleR, r=hamming limited=1, $\tau=0.17$, $\alpha=0.2$</td>
        <td>RNN + 1.0 x cnn(6:)</td>
        <td>3</td>
        <td>98.61</td>
        <td>32.01</td>
        <td>10.07</td>
        <td>160k / 196k</td>
    </tr>
    <tr>
        <td>adaptive_attention</td>
        <td>resnet152</td>
        <td> base lr: 1.0e-04 decay: 5, Adam(0.8,0.999), batch: 10, seq: 16</td>
        <td> SampleR, r=hamming limited=1, $\tau=0.17$, $\alpha=0.5$</td>
        <td>RNN + 1.0 x cnn(6:)</td>
        <td>3</td>
        <td>98.56</td>
        <td>31.97</td>
        <td>10.57</td>
        <td>200k / 256k</td>
    </tr>
    <tr>
        <td>adaptive_attention</td>
        <td>resnet152</td>
        <td> base lr: 1.0e-04 decay: 5, Adam(0.8,0.999), batch: 10, seq: 16</td>
        <td> ML</td>
        <td>RNN + 1.0 x cnn(3:)</td>
        <td>3</td>
        <td>98.35</td>
        <td>31.99</td>
        <td>10.19</td>
        <td>212k / 240k</td>
    </tr>
    <tr>
        <td>adaptive_attention</td>
        <td>resnet152</td>
        <td> base lr: 1.0e-04 decay: 5, Adam(0.8,0.999), batch: 10, seq: 16</td>
        <td> Word Level, Sim=Coco, $\tau=0.17$, $\alpha=0.3$</td>
        <td>RNN + 1.0 x cnn(6:)</td>
        <td>3</td>
        <td>97.46</td>
        <td>31.76</td>
        <td>13.29</td>
        <td>172k / 232k</td>
    </tr>
    <tr>
        <td>adaptive_attention</td>
        <td>resnet152</td>
        <td> base lr: 1.0e-04 decay: 5, Adam(0.8,0.999), batch: 10, seq: 16</td>
        <td> Word Level, Sim=Coco, $\tau=0.15$, $\alpha=0.5$</td>
        <td>RNN + 1.0 x cnn(6:)</td>
        <td>3</td>
        <td>97.07</td>
        <td>31.34</td>
        <td>18.80</td>
        <td>200k / 256k</td>
    </tr>
    <tr>
        <td>adaptive_attention</td>
        <td>resnet152</td>
        <td> base lr: 1.0e-05 decay: -1, Adam(0.8,0.999), batch: 20, seq: 16</td>
        <td> ML</td>
        <td>RNN + 1.0 x cnn(6:)</td>
        <td>3</td>
        <td>95.77</td>
        <td>30.59</td>
        <td>10.50</td>
        <td>120k / 168k</td>
    </tr>
    <tr>
        <td>adaptive_attention</td>
        <td>resnet152</td>
        <td> base lr: 1.0e-05 decay: -1, Adam(0.8,0.999), batch: 20, seq: 16</td>
        <td> ML</td>
        <td>RNN + 0.1 x cnn(6:)</td>
        <td>3</td>
        <td>95.75</td>
        <td>30.44</td>
        <td>10.39</td>
        <td>132k / 168k</td>
    </tr>
    <tr>
        <td>adaptive_attention</td>
        <td>resnet152</td>
        <td> base lr: 4.0e-04 decay: 5, Adam(0.8,0.999), batch: 10, seq: 16 Augment</td>
        <td> ML</td>
        <td>RNN</td>
        <td>3</td>
        <td>95.48</td>
        <td>31.05</td>
        <td>11.69</td>
        <td>140k / 144k</td>
    </tr>
    <tr>
        <td>adaptive_attention</td>
        <td>resnet152</td>
        <td> base lr: 4.0e-04 decay: 5, Adam(0.8,0.999), batch: 10, seq: 16</td>
        <td> SampleR, r=hamming limited=1, $\tau=0.17$, $\alpha=0.5$</td>
        <td>RNN</td>
        <td>3</td>
        <td>95.03</td>
        <td>30.76</td>
        <td>10.89</td>
        <td>108k / 172k</td>
    </tr>
    <tr>
        <td>adaptive_attention</td>
        <td>resnet152</td>
        <td> base lr: 4.0e-04 decay: 5, Adam(0.8,0.999), batch: 10, seq: 16</td>
        <td> Word Level, Sim=Coco, $\tau=0.17$, $\alpha=0.5$</td>
        <td>RNN</td>
        <td>3</td>
        <td>94.85</td>
        <td>30.62</td>
        <td>18.94</td>
        <td>92k / 172k</td>
    </tr>
    <tr>
        <td>adaptive_attention</td>
        <td>resnet152</td>
        <td> base lr: 4.0e-04 decay: 5, Adam(0.8,0.999), batch: 10, seq: 16</td>
        <td> Word Level, Sim=Coco, $\tau=0.17$, $\alpha=0.3$</td>
        <td>RNN</td>
        <td>3</td>
        <td>94.83</td>
        <td>30.77</td>
        <td>13.58</td>
        <td>92k / 168k</td>
    </tr>
    <tr>
        <td>adaptive_attention</td>
        <td>resnet152</td>
        <td> base lr: 4.0e-04 decay: 5, Adam(0.8,0.999), batch: 10, seq: 16 Maxout</td>
        <td> ML</td>
        <td>RNN</td>
        <td>3</td>
        <td>94.61</td>
        <td>30.46</td>
        <td>10.15</td>
        <td>100k / 172k</td>
    </tr>
    <tr>
        <td>adaptive_attention</td>
        <td>resnet152</td>
        <td> base lr: 4.0e-04 decay: 5, Adam(0.8,0.999), batch: 10, seq: 16</td>
        <td> SampleR, r=hamming limited=1, $\tau=0.17$, $\alpha=0.4$</td>
        <td>RNN</td>
        <td>3</td>
        <td>94.37</td>
        <td>29.81</td>
        <td>10.59</td>
        <td>132k / 172k</td>
    </tr>
    <tr>
        <td>adaptive_attention</td>
        <td>resnet152</td>
        <td> base lr: 4.0e-04 decay: 5, Adam(0.8,0.999), batch: 10, seq: 16</td>
        <td> Word Level, Sim=Coco, $\tau=0.17$, $\alpha=0.4$</td>
        <td>RNN</td>
        <td>3</td>
        <td>94.30</td>
        <td>30.40</td>
        <td>15.71</td>
        <td>112k / 172k</td>
    </tr>
    <tr>
        <td>adaptive_attention</td>
        <td>resnet152</td>
        <td> base lr: 4.0e-04 decay: 5, Adam(0.8,0.999), batch: 10, seq: 16</td>
        <td> SampleR, r=hamming limited=1, $\tau=0.17$, $\alpha=0.2$</td>
        <td>RNN</td>
        <td>3</td>
        <td>94.11</td>
        <td>30.31</td>
        <td>10.31</td>
        <td>112k / 172k</td>
    </tr>
    <tr>
        <td>adaptive_attention</td>
        <td>resnet152</td>
        <td> base lr: 5.0e-04 decay: 5, Adam(0.8,0.999), batch: 10, seq: 16</td>
        <td> Combining losses,  Word Level, Sim=Coco, $\tau=0.17$, $\alpha=0.4$ w/ SampleR, r=hamming limited=1 $\tau=0.17$, $\alpha=0.5$</td>
        <td>RNN</td>
        <td>3</td>
        <td>94.05</td>
        <td>30.18</td>
        <td>294.94</td>
        <td>104k / 172k</td>
    </tr>
    <tr>
        <td>adaptive_attention</td>
        <td>resnet152</td>
        <td> base lr: 4.0e-04 decay: 5, Adam(0.8,0.999), batch: 10, seq: 16</td>
        <td> Word Level, Sim=Coco, $\tau=0.13$, $\alpha=0.5$</td>
        <td>RNN</td>
        <td>3</td>
        <td>94.05</td>
        <td>30.72</td>
        <td>19.20</td>
        <td>100k / 172k</td>
    </tr>
    <tr>
        <td>adaptive_attention</td>
        <td>resnet152</td>
        <td> base lr: 4.0e-04 decay: 5, Adam(0.8,0.999), batch: 10, seq: 16</td>
        <td> Word Level, Sim=Coco, $\tau=0.15$, $\alpha=0.5$</td>
        <td>RNN</td>
        <td>3</td>
        <td>93.99</td>
        <td>30.42</td>
        <td>19.34</td>
        <td>96k / 172k</td>
    </tr>
    <tr>
        <td>adaptive_attention</td>
        <td>resnet152</td>
        <td> base lr: 4.0e-04 decay: 5, Adam(0.8,0.999), batch: 10, seq: 18 xt = wt</td>
        <td> ML</td>
        <td>RNN</td>
        <td>3</td>
        <td>93.97</td>
        <td>28.99</td>
        <td>10.25</td>
        <td>116k / 172k</td>
    </tr>
    <tr>
        <td>adaptive_attention</td>
        <td>resnet152</td>
        <td> base lr: 4.0e-04 decay: 5, Adam(0.8,0.999), batch: 10, seq: 16</td>
        <td> SampleR, r=hamming limited=1, $\tau=0.17$, $\alpha=0.3$</td>
        <td>RNN</td>
        <td>3</td>
        <td>93.97</td>
        <td>30.21</td>
        <td>10.46</td>
        <td>96k / 172k</td>
    </tr>
    <tr>
        <td>adaptive_attention</td>
        <td>resnet152</td>
        <td> base lr: 4.0e-04 decay: 5, Adam(0.8,0.999), batch: 10, seq: 16</td>
        <td> Word Level, Sim=Coco, $\tau=0.14$, $\alpha=0.5$</td>
        <td>RNN</td>
        <td>3</td>
        <td>93.88</td>
        <td>30.46</td>
        <td>19.26</td>
        <td>100k / 172k</td>
    </tr>
    <tr>
        <td>adaptive_attention</td>
        <td>resnet152</td>
        <td> base lr: 4.0e-04 decay: 5, Adam(0.8,0.999), batch: 10, seq: 16</td>
        <td> Word Level, Sim=Coco, $\tau=0.17$, $\alpha=0.2$</td>
        <td>RNN</td>
        <td>3</td>
        <td>93.85</td>
        <td>30.48</td>
        <td>11.94</td>
        <td>100k / 172k</td>
    </tr>
    <tr>
        <td>adaptive_attention</td>
        <td>resnet152</td>
        <td> base lr: 4.0e-04 decay: 5, Adam(0.8,0.999), batch: 10, seq: 16</td>
        <td> Word Level, Sim=Coco, $\tau=0.15$, $\alpha=0.4$</td>
        <td>RNN</td>
        <td>3</td>
        <td>93.79</td>
        <td>30.30</td>
        <td>16.11</td>
        <td>100k / 172k</td>
    </tr>
    <tr>
        <td>adaptive_attention</td>
        <td>resnet152</td>
        <td> base lr: 4.0e-04 decay: 5, Adam(0.8,0.999), batch: 10, seq: 16</td>
        <td> ML</td>
        <td>RNN</td>
        <td>3</td>
        <td>93.72</td>
        <td>30.41</td>
        <td>10.42</td>
        <td>104k / 172k</td>
    </tr>
    <tr>
        <td>adaptive_attention</td>
        <td>resnet152</td>
        <td> base lr: 4.0e-04 decay: 5, Adam(0.8,0.999), batch: 10, seq: 16</td>
        <td> Word Level, Sim=Coco, $\tau=0.16$, $\alpha=0.5$</td>
        <td>RNN</td>
        <td>3</td>
        <td>93.59</td>
        <td>30.28</td>
        <td>19.37</td>
        <td>96k / 172k</td>
    </tr>
    <tr>
        <td>adaptive_attention</td>
        <td>resnet152</td>
        <td> base lr: 5.0e-04 decay: 5, Adam(0.8,0.999), batch: 10, seq: 16</td>
        <td> Combining losses,  Word Level, Sim=Coco, $\tau=0.17$, $\alpha=0.3$ w/ SampleR, r=hamming limited=1 $\tau=0.17$, $\alpha=0.5$</td>
        <td>RNN</td>
        <td>3</td>
        <td>93.58</td>
        <td>29.98</td>
        <td>142.61</td>
        <td>100k / 172k</td>
    </tr>
    <tr>
        <td>adaptive_attention</td>
        <td>resnet152</td>
        <td> base lr: 5.0e-04 decay: 5, Adam(0.8,0.999), batch: 10, seq: 16</td>
        <td> Combining losses,  Word Level, Sim=Coco, $\tau=0.17$, $\alpha=0.5$ w/ SampleR, r=hamming limited=1 $\tau=0.17$, $\alpha=0.5$</td>
        <td>RNN</td>
        <td>3</td>
        <td>92.74</td>
        <td>29.64</td>
        <td>585.28</td>
        <td>108k / 172k</td>
    </tr>
    <tr>
        <td>adaptive_attention</td>
        <td>resnet152</td>
        <td> base lr: 4.0e-04 decay: 5, Adam(0.8,0.999), batch: 10, seq: 16</td>
        <td> Word Level, Sim=Coco, $\tau=0.15$, $\alpha=0.6$</td>
        <td>RNN</td>
        <td>3</td>
        <td>92.67</td>
        <td>29.83</td>
        <td>24.03</td>
        <td>124k / 172k</td>
    </tr>
    <tr>
        <td>adaptive_attention</td>
        <td>resnet152</td>
        <td> base lr: 5.0e-04 decay: 5, Adam(0.8,0.999), batch: 10, seq: 16</td>
        <td> Combining losses,  Word Level, Sim=Coco, $\tau=0.17$, $\alpha=0.3$ w/ SampleR, r=hamming limited=1 $\tau=0.17$, $\alpha=0.2$</td>
        <td>RNN</td>
        <td>3</td>
        <td>92.56</td>
        <td>30.04</td>
        <td>137.66</td>
        <td>124k / 172k</td>
    </tr>
    <tr>
        <td>adaptive_attention</td>
        <td>resnet152</td>
        <td> base lr: 4.0e-04 decay: 5, Adam(0.8,0.999), batch: 10, seq: 16 xt = wt</td>
        <td> ML</td>
        <td>RNN</td>
        <td>3</td>
        <td>92.51</td>
        <td>28.55</td>
        <td>10.23</td>
        <td>120k / 172k</td>
    </tr>
    <tr>
        <td>adaptive_attention</td>
        <td>resnet152</td>
        <td> base lr: 4.0e-04 decay: 5, Adam(0.8,0.999), batch: 10, seq: 18 Maxout xt = wt</td>
        <td> ML</td>
        <td>RNN</td>
        <td>3</td>
        <td>92.06</td>
        <td>27.79</td>
        <td>10.14</td>
        <td>112k / 172k</td>
    </tr>
    <tr>
        <td>adaptive_attention</td>
        <td>resnet152</td>
        <td> base lr: 4.0e-04 decay: -1, Adam(0.8,0.999), batch: 20, seq: 16</td>
        <td> ML</td>
        <td>RNN</td>
        <td>3</td>
        <td>91.97</td>
        <td>29.41</td>
        <td>10.67</td>
        <td>76k / 96k</td>
    </tr>
    <tr>
        <td>adaptive_attention</td>
        <td>resnet152</td>
        <td> base lr: 4.0e-04 decay: 5, Adam(0.8,0.999), batch: 10, seq: 16</td>
        <td> Word Level, Sim=Coco, $\tau=0.15$, $\alpha=0.7$</td>
        <td>RNN</td>
        <td>3</td>
        <td>91.97</td>
        <td>30.01</td>
        <td>32.06</td>
        <td>104k / 172k</td>
    </tr>
    <tr>
        <td>adaptive_attention</td>
        <td>resnet152</td>
        <td> base lr: 5.0e-04 decay: 5, Adam(0.8,0.999), batch: 10, seq: 16</td>
        <td> Combining losses,  Word Level, Sim=Coco xIDF, $\tau=0.15$, $\alpha=0.7$ w/ SampleR, r=hamming limited=1 $\tau=0.17$, $\alpha=0.4$</td>
        <td>RNN</td>
        <td>3</td>
        <td>90.97</td>
        <td>29.58</td>
        <td>2028.49</td>
        <td>100k / 172k</td>
    </tr>
    <tr>
        <td>adaptive_attention</td>
        <td>resnet152</td>
        <td> base lr: 4.0e-04 decay: 5, Adam(0.8,0.999), batch: 10, seq: 16</td>
        <td> Word Level, Sim=Coco, $\tau=0.15$, $\alpha=0.8$</td>
        <td>RNN</td>
        <td>3</td>
        <td>90.87</td>
        <td>29.37</td>
        <td>48.06</td>
        <td>104k / 172k</td>
    </tr>
    <tr>
        <td>adaptive_attention</td>
        <td>resnet152</td>
        <td> base lr: 4.0e-04 decay: -1, Adam(0.8,0.999), batch: 10, seq: 16</td>
        <td> ML</td>
        <td>RNN</td>
        <td>3</td>
        <td>90.64</td>
        <td>29.20</td>
        <td>10.70</td>
        <td>160k / 172k</td>
    </tr>
    <tr>
        <td>adaptive_attention</td>
        <td>resnet50</td>
        <td> base lr: 1.0e-04 decay: 5, Adam(0.8,0.999), batch: 10, seq: 16</td>
        <td> ML</td>
        <td>RNN</td>
        <td>3</td>
        <td>89.84</td>
        <td>29.53</td>
        <td>12.19</td>
        <td>112k / 172k</td>
    </tr>
    <tr>
        <td>adaptive_attention</td>
        <td>resnet50</td>
        <td> base lr: 1.0e-04 decay: 5, Adam(0.9,0.999), batch: 10, seq: 16</td>
        <td> ML</td>
        <td>RNN</td>
        <td>3</td>
        <td>89.54</td>
        <td>29.19</td>
        <td>12.22</td>
        <td>96k / 172k</td>
    </tr>
    <tr>
        <td>adaptive_attention</td>
        <td>resnet152</td>
        <td> base lr: 4.0e-04 decay: 5, Adam(0.8,0.999), batch: 20, seq: 16</td>
        <td> Word Level, Sim=Coco xIDF, $\tau=0.15$, $\alpha=0.9$</td>
        <td>RNN</td>
        <td>3</td>
        <td>89.30</td>
        <td>29.08</td>
        <td>93.64</td>
        <td>60k / 84k</td>
    </tr>
    <tr>
        <td>adaptive_attention</td>
        <td>resnet152</td>
        <td> base lr: 5.0e-04 decay: -1, Adam(0.8,0.999), batch: 10, seq: 18</td>
        <td> ML</td>
        <td>RNN + 0.0 x cnn(6:)</td>
        <td>3</td>
        <td>88.77</td>
        <td>28.28</td>
        <td>10.70</td>
        <td>148k / 288k</td>
    </tr>
    <tr>
        <td>adaptive_attention</td>
        <td>resnet152</td>
        <td> base lr: 4.0e-04 decay: 5, Adam(0.8,0.999), batch: 10, seq: 16</td>
        <td> Word Level, Sim=Coco xIDF, $\tau=0.15$, $\alpha=0.9$</td>
        <td>RNN</td>
        <td>3</td>
        <td>88.25</td>
        <td>28.56</td>
        <td>93.94</td>
        <td>108k / 156k</td>
    </tr>
    <tr>
        <td>adaptive_attention</td>
        <td>resnet50</td>
        <td> base lr: 4.0e-04 decay: -1, Adam(0.8,0.999), batch: 10, seq: 16</td>
        <td> ML</td>
        <td>RNN</td>
        <td>3</td>
        <td>88.09</td>
        <td>28.64</td>
        <td>10.91</td>
        <td>152k / 168k</td>
    </tr>
    <tr>
        <td>adaptive_attention</td>
        <td>resnet50</td>
        <td> base lr: 1.0e-04 decay: 5, Adam(0.9,0.999), batch: 10, seq: 16</td>
        <td> ML</td>
        <td>RNN</td>
        <td>3</td>
        <td>87.68</td>
        <td>28.79</td>
        <td>12.23</td>
        <td>100k / 172k</td>
    </tr>
    <tr>
        <td>adaptive_attention</td>
        <td>resnet152</td>
        <td> base lr: 4.0e-04 decay: 5, Adam(0.8,0.999), batch: 10, seq: 16</td>
        <td> Word Level, Sim=Coco, $\tau=0.15$, $\alpha=0.9$</td>
        <td>RNN</td>
        <td>3</td>
        <td>87.63</td>
        <td>28.12</td>
        <td>93.96</td>
        <td>128k / 168k</td>
    </tr>
</table>