{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bs2 = json.load(open('bootstrap_bs2.json', 'r'))\n",
    "bs5 = json.load(open('bootstrap_bs5.json', 'r'))\n",
    "bs1 = json.load(open('bootstrap_bs1.json', 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82783"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82783"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bs5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "Caps = {}\n",
    "for item in bs1:\n",
    "    if item['image_id'] not in Caps:\n",
    "        Caps[item['image_id']] = [[item['caption'], item['score']]]\n",
    "    else:\n",
    "        Caps[item['image_id']].append([item['caption'], item['score']])\n",
    "        \n",
    "for item in bs2:\n",
    "    if item['image_id'] not in Caps:\n",
    "        Caps[item['image_id']] = [[item['caption'], item['score']]]\n",
    "    else:\n",
    "        Caps[item['image_id']].append([item['caption'], item['score']])\n",
    "        \n",
    "for item in bs5:\n",
    "    if item['image_id'] not in Caps:\n",
    "        Caps[item['image_id']] = [[item['caption'], item['score']]]\n",
    "    else:\n",
    "        Caps[item['image_id']].append([item['caption'], item['score']])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82783"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Caps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[57870,\n",
       " 384029,\n",
       " 222016,\n",
       " 520950,\n",
       " 69675,\n",
       " 547471,\n",
       " 122688,\n",
       " 392136,\n",
       " 398494,\n",
       " 90570,\n",
       " 504616,\n",
       " 161919]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(Caps)[:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['a man in a kitchen preparing food in a kitchen', '-12.8587'],\n",
       " ['a man is standing in front of an open refrigerator', '-12.4298'],\n",
       " ['a man standing in front of an open refrigerator', '-10.6457']]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Caps[90570]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "coco = json.load(open('data/coco/dataset_coco.json', 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(coco)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "coco = coco['images']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cocoid': 391895,\n",
       " 'filename': 'COCO_val2014_000000391895.jpg',\n",
       " 'filepath': 'val2014',\n",
       " 'imgid': 0,\n",
       " 'sentences': [{'imgid': 0,\n",
       "   'raw': 'A man with a red helmet on a small moped on a dirt road. ',\n",
       "   'sentid': 770337,\n",
       "   'tokens': ['a',\n",
       "    'man',\n",
       "    'with',\n",
       "    'a',\n",
       "    'red',\n",
       "    'helmet',\n",
       "    'on',\n",
       "    'a',\n",
       "    'small',\n",
       "    'moped',\n",
       "    'on',\n",
       "    'a',\n",
       "    'dirt',\n",
       "    'road']},\n",
       "  {'imgid': 0,\n",
       "   'raw': 'Man riding a motor bike on a dirt road on the countryside.',\n",
       "   'sentid': 771687,\n",
       "   'tokens': ['man',\n",
       "    'riding',\n",
       "    'a',\n",
       "    'motor',\n",
       "    'bike',\n",
       "    'on',\n",
       "    'a',\n",
       "    'dirt',\n",
       "    'road',\n",
       "    'on',\n",
       "    'the',\n",
       "    'countryside']},\n",
       "  {'imgid': 0,\n",
       "   'raw': 'A man riding on the back of a motorcycle.',\n",
       "   'sentid': 772707,\n",
       "   'tokens': ['a',\n",
       "    'man',\n",
       "    'riding',\n",
       "    'on',\n",
       "    'the',\n",
       "    'back',\n",
       "    'of',\n",
       "    'a',\n",
       "    'motorcycle']},\n",
       "  {'imgid': 0,\n",
       "   'raw': 'A dirt path with a young person on a motor bike rests to the foreground of a verdant area with a bridge and a background of cloud-wreathed mountains. ',\n",
       "   'sentid': 776154,\n",
       "   'tokens': ['a',\n",
       "    'dirt',\n",
       "    'path',\n",
       "    'with',\n",
       "    'a',\n",
       "    'young',\n",
       "    'person',\n",
       "    'on',\n",
       "    'a',\n",
       "    'motor',\n",
       "    'bike',\n",
       "    'rests',\n",
       "    'to',\n",
       "    'the',\n",
       "    'foreground',\n",
       "    'of',\n",
       "    'a',\n",
       "    'verdant',\n",
       "    'area',\n",
       "    'with',\n",
       "    'a',\n",
       "    'bridge',\n",
       "    'and',\n",
       "    'a',\n",
       "    'background',\n",
       "    'of',\n",
       "    'cloud',\n",
       "    'wreathed',\n",
       "    'mountains']},\n",
       "  {'imgid': 0,\n",
       "   'raw': 'A man in a red shirt and a red hat is on a motorcycle on a hill side.',\n",
       "   'sentid': 781998,\n",
       "   'tokens': ['a',\n",
       "    'man',\n",
       "    'in',\n",
       "    'a',\n",
       "    'red',\n",
       "    'shirt',\n",
       "    'and',\n",
       "    'a',\n",
       "    'red',\n",
       "    'hat',\n",
       "    'is',\n",
       "    'on',\n",
       "    'a',\n",
       "    'motorcycle',\n",
       "    'on',\n",
       "    'a',\n",
       "    'hill',\n",
       "    'side']}],\n",
       " 'sentids': [770337, 771687, 772707, 776154, 781998],\n",
       " 'split': 'test'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coco[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = json.load(open('data/coco/generated_captions_confusion_scored_distrib.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "123287"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import exp\n",
    "Caps_gen = []\n",
    "for k in Caps:\n",
    "    entry = {'captions': [c[0] for c in Caps[k]],\n",
    "             'scores': [exp(float(c[1])) for c in Caps[k]],\n",
    "             'id': k}\n",
    "    Caps_gen.append(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'captions': ['a man standing in a doorway in a kitchen',\n",
       "  'a man standing in front of a door'],\n",
       " 'id': 504616,\n",
       " 'scores': [2.077386385082233e-06, 2.347170494843264e-06]}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Caps_gen[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(Caps_gen, open('data/coco/bootstrap_raml_exp_gen15_beam25.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
